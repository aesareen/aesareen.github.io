<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://aesareen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aesareen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-17T17:28:34+00:00</updated><id>https://aesareen.github.io/feed.xml</id><title type="html">Arnav Sareen - Portfolio</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Project Four: Clustering the Future - Leveraging Collegiate and Athletic Data to Find NFL Comparisons for College Players</title><link href="https://aesareen.github.io/blog/2025/project4/" rel="alternate" type="text/html" title="Project Four: Clustering the Future - Leveraging Collegiate and Athletic Data to Find NFL Comparisons for College Players"/><published>2025-04-14T15:30:00+00:00</published><updated>2025-04-14T15:30:00+00:00</updated><id>https://aesareen.github.io/blog/2025/project4</id><content type="html" xml:base="https://aesareen.github.io/blog/2025/project4/"><![CDATA[<h1 id="i-introduction">I. Introduction</h1> <p>As I am writing this blog post (which, very sadly, marks the last project for <code class="language-plaintext highlighter-rouge">ITCS 3162</code>—really enjoyed doing this), the NFL Draft is just a few days away. Across the NFL, thousands of scouts, analysts, general managers, and certainly fans are making predictions, countless mock drafts, and trying to see which incoming NFL player is going to take the league by storm.</p> <p>One of the most common discussions amidst this frenzy is the notion of <em>player comparisons</em>, where an incoming college player is evaluated in the backdrop of an already-established NFL receiver based upon a variety of characteristics, typically physical. However, while the “eye test” for this sort of comparison might be really helpful for general ideas and comparisons, it is not nearly as robust and quantifiable as it could be. Additionally, while these comparisons tend to gravitate certain archetypes of players towards a single NFL-prototype, that tends to embody the entire subgenre of wide receiver instead of looking at a more holistic viewpoint that includes other viewpoints. For example, incoming Ohio State prospect Emeka Egbuka has gotten countless comparisons to current Seahawks wide receiver Jaxon Smith-Njigba, and rightfully so, as the two possess incredibly similar route tendencies and also attributes. However, Smith-Njigba himself is part of a larger group of wide receivers who prioritize their game on flawless route running fundamentals, tempo, and QB-connection as compared to the “muscle-first” approach of someone like D.K. Metcalf or Mike Evans. More players outside Smith-Njigba certainly exist for these types of comparisons, and it feels like finding them is a vital step in creating a more comprehensive draft procedure.</p> <p>With all of this mind, this project attempts to answer <strong>What are the major wide receiver archetypes in the NFL? What are their characteristics?</strong> and <strong>How do the incoming crop of wide receiver prospects compare to those already in the league?</strong>. As the previous projects had their main topics of regression and classification, this project tackles yet another major pillar of the machine learning space: clustering. This problem is especially helpful for a clustering project because there are no labels or ground truths on what these clusters even are and what players belong to what, making it an inherently <strong>unsupervised</strong> learning problem. Though we won’t have an explicit metric to analyze like previous projects, our analysis can be a lot more humane and qualitative, using our judgement and knowledge of these receivers to judge our model’s performance.</p> <h2 id="iii-where-did-this-data-even-come-from">I.II: Where did this data even come from?</h2> <p>Like last project, this kind of NFL analysis does not really have a super readily available dataset all cleaned and ready to leverage (or even created at all). Thus, it was up to me again to create it and standardize it into a usable format to ensure that this project could even be made. This project demanded data sources not just from the NFL but also college, thus I enlisted the help of 4 discrete data sources to help me out:</p> <ul> <li><code class="language-plaintext highlighter-rouge">nfl_data_py</code>: Leveraged this in Project 3, but this project is awesome as it includes a bunch of NFL data, but most importantly for my project, a slew of roster data about every player in the NFL, such as their draft position, age, and headshot (which is vital for my tables!)</li> <li><code class="language-plaintext highlighter-rouge">cfbd-python</code>: The college football API that provided all of the statistics and advanced analytics for the college side of things. This package is very well-documented and though it requires an API key, the free tier is very generous at 1,500 requests per month and I got an extra 1,500 requests as a student. I only ended up needing around 30-40 requests amid all of my mess ups and such, but still nice to have!</li> <li><a href="nflcombineresults.com">nflcombineresults.com</a> and <a href="https://www.reddit.com/r/NFL_Draft/comments/1j5g0pt/2025_nfl_combine_data_all_stats_in_one_place/">Reddit</a>: Just like NFL salary cap data, getting combine data is surprisingly really difficult and annoying to retrieve. <code class="language-plaintext highlighter-rouge">nfl_data_py</code> does have a method for this, but the data is kinda outdated and not very well filled-in, so I was forced to end up finding a website I can quickly scrape. I only issued 3-4 requests and then saved the data to a CSV to prevent spam, and the data here is much higher-quality than the <code class="language-plaintext highlighter-rouge">nfl_data_py</code> equivalent. For 2025 data, that wasn’t yet available yet on nflcombineresults, so I found an awesome spreadsheet that someone had made and I could download off Google Sheets.</li> <li><a href="https://www.ras.football">RAS</a>: This statsitic is kind of the heart of my entire project. RAS stands for “Relative Athletic Score” and it essentially is a single metric to quantify a player’s entire performance at the combine, which means one metric to measure how fast, strong, agile, and overall physically fit a player is. The website is super easy to use and thank the stars for the owners as they introduced a simple button to save a query on the website to a CSV file, absolute legends.</li> </ul> <p>There is a brief discussion below about how the data from these sources is specifically gathered and combined, but that is the overview!</p> <p><strong>If you’d prefer to download this notebook, just press <a href="https://github.com/aesareen/3162-portfolio/blob/main/assets/jupyter/project_4.ipynb">here</a>.</strong></p> <h2 id="iiii-what-is-clustering">I.III: What is clustering?</h2> <p>Before we get too immersed in the project, it’s worth taking a step back to define clustering and one if it’s main types: <strong>k-means</strong>. As mentioned earlier, clustering in an unsupervised machine learning model that tries to deduce patterns from our data by comparing how similar each point is to other data points. While classification does a similar task, clustering does not have labels and our algorithm must essentially determine or create these labels from scratch. To find this similarity, clustering algorithms use distance metrics like Eucledian or Manhattan to and then can make decisions based upon how close a point is to another.</p> <p><strong>K-Means</strong> clustering leverages a pre-defined number of clusters (or classes) as a hyperparameter and then classifies each data point by computing the distance between that point and the distance to each one of our cluster’s centers (called a centroid). To classify a point, we simply take find the closest distance of our point to a cluster’s centroid and voilà, our data point now belongs to that cluster! We then recalculate the group’s center with this new data point, and then repeat this process until our points tend to converge to a single center centroid or a max number of epochs is reached.</p> <h1 id="ii-pre-processing-and-visualizing-the-data">II. Pre-Processing and Visualizing the Data</h1> <p>I won’t go into the specifics of the data gathering because it was kind of a pain in the ass, but I will quickly summarize how it went below:</p> <h2 id="1-loading-the-ras-data">1. Loading the RAS Data</h2> <p>The RAS data was already in some nice spreadsheets, so I simply just loaded it in and then parsed it out to only have wide receivers. You might be asking: Wide receivers again, why? Well, it was honestly because that gave me the highest pool of data to work with for the upcoming draft picks. This draft has a great deal of receivers to be drafted, and though that is likely true most years, this fact combined with the number of analytics that exist for receivers from the <code class="language-plaintext highlighter-rouge">cfbd_api</code> made it an easy choice.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RAS_parsed: pd.DataFrame = all_RAS.loc[(all_RAS['Year'] &gt;= 2021) &amp; (all_RAS['Pos'] == 'WR')].dropna(axis = 0, subset = ['RAS', 'Name'])

RAS_2025_receivers: pd.DataFrame = RAS_2025.loc[RAS_2025['Pos'] == 'WR'].dropna(axis = 0, subset = ['RAS', 'Name'])
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/RAS_scores_by_position-480.webp 480w,/assets/img/RAS_scores_by_position-800.webp 800w,/assets/img/RAS_scores_by_position-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/RAS_scores_by_position.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="2-getting-the-college-football-data">2. Getting the College Football Data</h2> <p>Given I had to call an API for this, I decided to just create a function that took in a specific metric I wanted and the years I wanted it for. I only used it twice, but it will make getting metrics super easy if I so wish.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_cfbd_data</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">cfbd</span><span class="p">.</span><span class="n">Configuration</span><span class="p">,</span> <span class="n">years</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">api_instance</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">api_call</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">display_usage</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">load</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">:</span>
  <span class="sh">"""</span><span class="s">call the CFBD api multiple times, conglomerate the data, save it to a CSV and dataframe, and then return the dataframe

  Args:
      config (cfbd.Configuration): configuration object to authenticate the CFBD api
      years (list[int]): years to collect the data for 
      api_instance (str): the specific unauthenticated API type from CFBD where the specific API call is housed
      api_call (str): specific API call that you</span><span class="sh">'</span><span class="s">d like to make for each year
      filepath (str): filepath to either write the file to (if load = True) or read from (if false)
      display_usage (bool, optional): display the number of API calls remaining for a given API key. Defaults to False.
      load (bool, optional): whether to load a dataset from an existing filepath. Defaults to True.

  Returns:
      pd.DataFrame: dataframe of requested statistics, either loaded or collected from the API
  </span><span class="sh">"""</span>
  <span class="n">data</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">load</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">cfbd</span><span class="p">.</span><span class="nc">ApiClient</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">as</span> <span class="n">api</span><span class="p">:</span>
          <span class="c1"># Get the desired class from the cfbd module
</span>          <span class="n">api_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">cfbd</span><span class="p">,</span> <span class="n">api_instance</span><span class="p">)</span>
          <span class="n">authenticated_api_instance</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="nf">api_class</span><span class="p">(</span><span class="n">api</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">year</span> <span class="ow">in</span> <span class="n">years</span><span class="p">:</span>
              <span class="c1"># Get the method from the class
</span>              <span class="n">retrieved_api_call</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[...,</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">authenticated_api_instance</span><span class="p">,</span> <span class="n">api_call</span><span class="p">)</span>

              <span class="c1"># Make the API call (finally...)
</span>              <span class="n">response</span><span class="p">:</span> <span class="n">ApiResponse</span> <span class="o">=</span> <span class="nf">retrieved_api_call</span><span class="p">(</span><span class="n">year</span> <span class="o">=</span> <span class="n">year</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

              <span class="c1"># Add this data to our list
</span>              <span class="n">data</span><span class="p">.</span><span class="nf">extend</span><span class="p">([</span><span class="nf">dict</span><span class="p">(</span><span class="n">player</span><span class="p">)</span> <span class="k">for</span> <span class="n">player</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">data</span><span class="p">])</span>

              <span class="k">if</span> <span class="n">display_usage</span><span class="p">:</span>
                  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The amount of API calls left is: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">headers</span><span class="p">[</span><span class="sh">'</span><span class="s">X-Calllimit-Remaining</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
      
      <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
      
      <span class="c1"># To avoid having to run these API calls every time, we save the file
</span>      <span class="n">df</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
      <span class="c1"># In case we already have the file saved, just read it
</span>      <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div> <p>I then had to parse some of the data down to de-duplicate it, removed team-level statistics and only have player-level values, and also pivot some of the values as some of them were in a weird format where all of the metics were in one column instead of being in their own column.</p> <h2 id="3-combine-data">3. Combine Data</h2> <p>This data was the most annoying to get because as I said, it didn’t really exist in a nice format with enough robustness. For the nflcombinedata.com, I leveraged BeautifulSoup again and that worked perfectly. I tried to keep this scraping to a minimum as the website throws a 404 error technically when you request it, but the HTML data is still received, which is my guess of them saying “please don’t spam us.”</p> <p>For 2025 data, I used the spreadsheet above but did have to do some pre-processing as the rest of the combine data had the heights of each player in inches while this spreadsheet had it in FIIE values (Feet, .Inches-Inches, Eights-of-an-Inch). I created a small helper function that did this and then overrode the values in the height column.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">convert_fiie_to_in</span><span class="p">(</span><span class="n">fiie</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="sh">"""</span><span class="s">convert a fiie height to inches

Args:
    fiie (int): height in fiie

Returns:
    float: height in inches, rounded to closest hundredth of an inch
</span><span class="sh">"""</span>
<span class="n">fiie_str</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nf">str</span><span class="p">(</span><span class="n">fiie</span><span class="p">)</span>
<span class="n">feet</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">fiie_str</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">inches</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">fiie_str</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">eighth_of_inch</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">fiie_str</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

<span class="n">height</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">feet</span><span class="p">)</span> <span class="o">*</span> <span class="mi">12</span><span class="p">)</span> <span class="o">+</span> <span class="nf">int</span><span class="p">(</span><span class="n">inches</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">eighth_of_inch</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">8</span><span class="p">))</span>

<span class="k">return</span> <span class="nf">round</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert FIIE heights to inches to match the rest of the data 
</span><span class="n">combine_data_2025</span><span class="p">[</span><span class="sh">'</span><span class="s">height</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">combine_data_2025</span><span class="p">[</span><span class="sh">'</span><span class="s">height</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="n">func</span> <span class="o">=</span> <span class="n">convert_fiie_to_in</span><span class="p">)</span>
</code></pre></div></div> <h2 id="4-putting-it-all-together">4. Putting it all Together</h2> <p>With all of this data now loaded but in different dataframes, I had to consolidate the metrics I wanted into one place. I focused on both physical and a player’s prior season statistics for a reason I’ll discuss in the next section, but both are key components of my clustering.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">col_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">year</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">year_name_col</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">,</span> <span class="n">player_name_col</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
  <span class="sh">"""</span><span class="s">Filter a dataframe to find the specific statistic for a player in a particular year
  Args:
      player_name (str): name of the player
      col_name (str): name of the column that holds the desired statistic
      dataframe (pd.DataFrame): dataframe to filter with
      year (int): year where statistic was captured
      year_name_col (str, optional): name of the year column for the dataframe. Defaults to </span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="s">.
      player_name_col (str, optional): name of the player_name column for the dataframe. Defaults to </span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="s">.
      verbose (bool, optional): display instances where no data was found for a particular player. Defaults to False.

  Returns:
      float: _description_
  </span><span class="sh">"""</span>
  <span class="n">player_stats</span> <span class="o">=</span> <span class="n">dataframe</span><span class="p">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">dataframe</span><span class="p">[</span><span class="n">player_name_col</span><span class="p">]</span> <span class="o">==</span> <span class="n">player_name</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">dataframe</span><span class="p">[</span><span class="n">year_name_col</span><span class="p">]</span> <span class="o">==</span> <span class="n">year</span><span class="p">)]</span>

  <span class="c1"># the pandemic messed up some of the players due to COVID opt-outs or some players had injuries, if that is the case, we can try looking at a year before the provided date
</span>  <span class="k">if</span> <span class="n">player_stats</span><span class="p">.</span><span class="n">empty</span><span class="p">:</span>
      <span class="n">player_stats</span> <span class="o">=</span> <span class="n">dataframe</span><span class="p">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">dataframe</span><span class="p">[</span><span class="n">player_name_col</span><span class="p">]</span> <span class="o">==</span> <span class="n">player_name</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">dataframe</span><span class="p">[</span><span class="n">year_name_col</span><span class="p">]</span> <span class="o">==</span> <span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

  
  <span class="c1"># Filter by additional parameters if desired
</span>  <span class="c1"># for col, val in args:
</span>  <span class="c1">#     player_stats = player_stats.loc[player_stats[col] == val]
</span>  
  <span class="c1"># Convert to array and grab the value if possible
</span>  <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">player_stats</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">player_stats</span><span class="p">[</span><span class="n">col_name</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
          <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">No data for </span><span class="si">{</span><span class="n">player_name</span><span class="si">}</span><span class="s"> for </span><span class="si">{</span><span class="n">col_name</span><span class="si">}</span><span class="s"> in year </span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">None</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># In order to make the most high-quality predictions possible, I am only going to focus on players that I have RAS scores as that is my smallest dataset for and then get college data from their previous data from along with their more advanced metrics
</span>
<span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span>

<span class="k">for</span> <span class="n">player</span><span class="p">,</span> <span class="n">year</span><span class="p">,</span> <span class="n">RAS</span> <span class="ow">in</span> <span class="n">RAS_parsed</span><span class="p">[[</span><span class="sh">'</span><span class="s">Name</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">RAS</span><span class="sh">'</span><span class="p">]].</span><span class="nf">to_numpy</span><span class="p">():</span>
    <span class="c1"># print(player, year)
</span>    <span class="n">row</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">player_name</span><span class="sh">'</span><span class="p">:</span> <span class="n">player</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">headshot_url</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">headshot_url</span><span class="sh">'</span><span class="p">,</span> <span class="n">nfl_data</span><span class="p">,</span> <span class="n">year</span><span class="p">,</span> <span class="sh">'</span><span class="s">season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">player_name</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">receptions</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">REC</span><span class="sh">'</span><span class="p">,</span> <span class="n">receiving_stats</span><span class="p">,</span> <span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">player</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">yards</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">YDS</span><span class="sh">'</span><span class="p">,</span> <span class="n">receiving_stats</span><span class="p">,</span> <span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">player</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">touchdowns</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">TD</span><span class="sh">'</span><span class="p">,</span> <span class="n">receiving_stats</span><span class="p">,</span> <span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">player</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">yards_per_reception</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">YPR</span><span class="sh">'</span><span class="p">,</span> <span class="n">receiving_stats</span><span class="p">,</span> <span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">player</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">average_passing_downs_ppa</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">average_passing_downs_ppa</span><span class="sh">'</span><span class="p">,</span> <span class="n">player_predicted_points_combined</span><span class="p">,</span> <span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">average_standard_downs_ppa</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">average_standard_downs_ppa</span><span class="sh">'</span><span class="p">,</span> <span class="n">player_predicted_points_combined</span><span class="p">,</span> <span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">average_third_down_ppa</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">average_third_down_ppa</span><span class="sh">'</span><span class="p">,</span> <span class="n">player_predicted_points_combined</span><span class="p">,</span> <span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">season</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">height</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">height</span><span class="sh">'</span><span class="p">,</span> <span class="n">combine_data</span><span class="p">,</span> <span class="n">year</span><span class="p">,</span> <span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">weight</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">weight</span><span class="sh">'</span><span class="p">,</span> <span class="n">combine_data</span><span class="p">,</span> <span class="n">year</span><span class="p">,</span> <span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">forty</span><span class="sh">'</span><span class="p">:</span> <span class="nf">get_val</span><span class="p">(</span><span class="n">player</span><span class="p">,</span> <span class="sh">'</span><span class="s">forty</span><span class="sh">'</span><span class="p">,</span> <span class="n">combine_data</span><span class="p">,</span> <span class="n">year</span><span class="p">,</span> <span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">RAS</span><span class="sh">'</span><span class="p">:</span> <span class="n">RAS</span><span class="p">,</span>
    <span class="p">}</span>
<span class="n">rows</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>

<span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
</code></pre></div></div> <p>I had a little bit of pre-processing after this due to some null values, and to ensure that I was going to be able to make actual clusters, I eliminated values that were less than 70% full.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Drop columns where more than 70% of values are null
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="p">.</span><span class="mi">3</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Very few values are missing, so we can just impute them with the median for the column
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">average_passing_downs_ppa</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">average_passing_downs_ppa</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">average_standard_downs_ppa</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">average_standard_downs_ppa</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">average_third_down_ppa</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">average_third_down_ppa</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>This same process was essentially repeated for the college data as well.</p> <h1 id="iii-data-understanding--visualization">III. Data Understanding / Visualization</h1> <p>Some of this was already done in the previous section, but I mentioned that physical data cannot be the sole factor in our clustering. Observe the following:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/puka_vs_mingo_combine_table-480.webp 480w,/assets/img/puka_vs_mingo_combine_table-800.webp 800w,/assets/img/puka_vs_mingo_combine_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/puka_vs_mingo_combine_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Both Jonathan Mingo and Puka Nacua were drafted in the 2023 NFL Draft. Mingo was one of the draft’s most touted prospects, being drafted in the second round, while Nacua was unknown to most NFL fans, drafted in the fifth round. 2 years later, the lives of these two young men are entirely different: Mingo is already on his second NFL team after being traded for a fourth-round pick and has never posted more than 70 receiving yards in an NFL game and has yet to catch a touchdown. Nacua was a second-team All-Pro selection and a Pro Bowler in his rookie year, and likely would have repeated such a feat in the most recent season if he didn’t miss 6 games (he still had a very good season, nevertheless). The discrepancies in these two players may be obvious in hindsight, but the physical comparison above is just as diverging as these two players’ career paths: Mingo absolutely wins over Nacua and that is indisputable. How Mingo outweights Nacua by 10 pounds yet is still noticeably faster than him and considerably stronger illustrates the two outlooks on the players prior to the draft.</p> <p>However, as I realized, physical characteristics don’t tell the full story and when we look at how each player performed, we get to get a much more holistic picture of the players’ performance.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/puka_vs_mingo_stats_table-480.webp 480w,/assets/img/puka_vs_mingo_stats_table-800.webp 800w,/assets/img/puka_vs_mingo_stats_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/puka_vs_mingo_stats_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We begin to see that Nacua was a far better receiver, particularly on the ever-important 3rd-down that teams need, and while Mingo might have more visible “big-play” potential, the fundamentals of Nacua game is what have made him such a dominant force at the next level. There of course caveats to this comparison (Mingo’s competition at Ole Miss was far better than Nacua’s at BYU), but including statistics to me is a valuable feature to have in our analysis.</p> <h1 id="iv-clustering-and-modeling-our-data">IV. Clustering and Modeling our Data</h1> <p>With all of our data retrieved, pre-processed, and scrutinized, the modeling step has finally arrived! To kick off things, we first need to standardize our data because we working with distances and having vastly different scales for our data can create a very weird scenario where our data may be very close together in one dimension but thousands of units apart in another.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># First, we likely need to scale our data 
# Keep player_name and headshot_url separate, scale all numeric features
</span><span class="n">scaler</span><span class="p">:</span> <span class="n">StandardScaler</span> <span class="o">=</span>  <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">features_scaled</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:])</span>

<span class="c1"># Create a DataFrame with the scaled features
</span><span class="n">df_scaled</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">features_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
</code></pre></div></div> <p>With that accomplished, we can start by clustering our data for NFL players and figuring out the overall clusters present across the league. I decided to leverage a K-Means algorithim here because will be making a lot of new predictions, which <a href="https://datascience.stackexchange.com/a/92252">K-Means tends to excel at</a> as compared to hierarchial clustering, and because I wanted to control the number of clusters that were created. While having the variable number with hierarchial is great, it doesn’t really help me if there are three random receivers grouped together and I have no idea why. With a pre-defined number, I can get a general sense of the receivers within that cluster and analyze the commonalities between them without being super time-intensive.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">4</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:])</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">cluster</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span>
</code></pre></div></div> <p>While we can certainly just walk through the clusters for all the players and analyze them that way, an amazing way to really visualize our clusters, even with a large number of features, is <strong>Principal Component Analysis (PCA)</strong>. This helps transform our high-dimensional data into a lower-dimensional data while preserving as much of the essential components of the features as possible. As always, <code class="language-plaintext highlighter-rouge">plotly</code> makes making these types of visualizations super easy and my theme allows for interactive plotly diagrams, so I made some 3-D scatterplots to explore our clusters!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca_curr</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:])</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">pca1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_curr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">pca2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_curr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">pca3</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_curr</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>

<span class="n">clusters</span><span class="p">:</span> <span class="n">px</span><span class="p">.</span><span class="n">scatter_3d</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">scatter_3d</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> 
    <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">pca1</span><span class="sh">'</span><span class="p">,</span> 
    <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">pca2</span><span class="sh">'</span><span class="p">,</span> 
    <span class="n">z</span> <span class="o">=</span> <span class="sh">'</span><span class="s">pca3</span><span class="sh">'</span><span class="p">,</span> 
    <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">cluster</span><span class="sh">'</span><span class="p">,</span> 
    <span class="n">hover_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">player_name</span><span class="sh">'</span><span class="p">,</span> 
<span class="p">)</span>
</code></pre></div></div> <div class="l-page"> <iframe src="/assets/plotly/NFL_WRs_clusters.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed white;"></iframe> </div> <p>This is so cool, and I think really allows us to see the clusters “come to life.” We can also see some of the outliers that I might want to keep in mind when reaching our final conclusions (cough, <em>Jaxon Smith-Njigba</em> and <em>Xaviet Worthy</em> and <em>Tank Dell</em>).</p> <p>I then repeated the entire process thus far for the new draft prospects, and created a similar PCA visualization for them based upon the clusters we defined for NFL receivers.</p> <div class="l-page"> <iframe src="/assets/plotly/NFL_2025_WRs_clusters.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed white;"></iframe> </div> <p>This was brief, but this really concludes the bulk of clustering work. Now it’s time to delve into the clusters and see what they tell us about the current landscape of receivers and the next generation!</p> <h1 id="v-analysis">V. Analysis</h1> <p>To begin answering our question about the various archetypes that currently exist in the NFL, it helps to get a physical profile of each of them and how those profiles are currently valued by NFL personnel. I used the nfl_data that we imported earlier and the feature data we have been working with to find the means of each cluster’s height, weight, forty time (speed), and draft position.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wide_receiver_cluster_overview-480.webp 480w,/assets/img/wide_receiver_cluster_overview-800.webp 800w,/assets/img/wide_receiver_cluster_overview-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/wide_receiver_cluster_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This table was really helpful for me to begin breaking down our clusters and figuring out how K-Means organized them. On my understanding, they are roughly as follows:</p> <h2 id="cluster-one-wr-1-upside">Cluster One: WR-1 Upside</h2> <p>This cluster featured the likes of Ja’Marr Chase, Malik Nabers, Brian Thomas Jr., Quentin Johnston, Treylon Burks, Xavier Legette, and Rome Odunze—all prospects highly touted coming out of college and were thought of being the “alphas” within their respective offenses. Not all of those guys reached those potential, but the combination of their physical attributes, elite college production, and overall lofty expectations + high upside make sense about why they were clustered together.</p> <p>Some players, such as Ja’lynn Polk and Troy Franklin weirdly got grouped togetehr in this cluster, which I think is a bit odd given that those profiles likely fit much better in the “grit” category and “speedster” categories respectatively, but nevertheless, the prestige of this cluster is apparent.</p> <h2 id="cluster-two-first-guy-in-last-guy-out">Cluster Two: First Guy in, Last Guy Out</h2> <p>This cluster is really interesting, and is a conglomeration of some really incredible names. Spearheaded by Amon-Ra St. Brown and Puka Nacua, it also includes Derius Davis, Jayden Reed, Amari Rodgers, and Kayshon Boutte. All of these receivers might seem terriblely “average” on paper, and that is perhaps reflected in their lowest average RAS and draft position. However, as Nacua and St. Brown have showed, it’s the fundamentals in route running and work ethic that are really difficulty to quanitfy that have led to those two being such dominant forces in the NFL. These guys have a chip on their shoulder coming into the league, and it’s great to see them represented in our data.</p> <h2 id="cluster-three-physical-specimens">Cluster Three: Physical Specimens</h2> <p>These players really were given incredible gifts: Tall, strong, and fast, they have all the tools to really perform well at the next level. We are talking about players like George Pickens, Keon Coleman, Jonathan Mingo, Cedric Tillman, and DeVaughn Vele—players that on paper check all the boxes but are perhaps less refined in other parts of their game like route running and catching. These players often had solid college careers, though not always spectacular, and are in need of heavy refinement to fully unlock their potential.</p> <p>There are some odd players clustered here, like Smith-Njigba and Rashod Bateman, but I am guessing those guys are kind of in a tier betwen this cluster and the cluster above: not physically dominate but not inept either, and can likely fall in any of the clusters given the particular features.</p> <h2 id="cluster-four-speedsters">Cluster Four: Speedsters</h2> <p>These receivers are known primarily for one thing: their ability to vertically stretch the field. Xavier Worthy, Chirs Olave, Zay Flowers, Garrett Wilson, and Calvin Austin III. These guys typically are on the lighter-side and are right behind the WR-1 upside in average draft position given how important speed has become in the modern game.</p> <p>There are few weird outliers here as well, with Tank Dell (as mentioned earlier) and Jordan Addison particularly, but these two are likely here because the weight of them really factored into their eventual clustering decision.</p> <p>With that knowledge, that helps answer our question about the major wide receiver archetypes and can really inform how our algorithim classified the receivers in the upcoming draft class:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wide_receiver_25_clusters-480.webp 480w,/assets/img/wide_receiver_25_clusters-800.webp 800w,/assets/img/wide_receiver_25_clusters-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/wide_receiver_25_clusters.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can see some of the best receivers in the class, Emeka Egbuka, Jack Bech, and Jayden Higgins in that first category, while some of the guys in the second cluster are perhaps just waiting to become the next Puka Nacua or Amon-Ra St. Brown. The freakish athletes return in cluster 3, which includes 6’5 Da’Quan Felton and a bunch of receivers who are at least 6’3. The final category features the blazing fast Jaylin Noel and Tai Felton, though Xavier Restrepo’s real forty time of 4.8 was not included in the data for some reason (luckily for him because that was <em>terrible</em>).</p> <h1 id="vi-impact--conclusion">VI. Impact &amp; Conclusion</h1> <p>While there are quite a few outliers and head-scratchers throughout this project, I think this project really illustrates how valuable a holistic view of players can be and how marrying past-production with a player’s physical features can be very powerful in finding players like them. This model tends to sometimes priortize the combine metrics over the seasonal stats, which can lead to players like Smith-Njigba not really having a clear-cut cluster for them, but with a bit more tweaking and fine-tuning, I bet that can be alleviated. Finally, with the Draft being as much of an art as it is a science, this at least allows us to standardize part of the winding process of NFL drafting.</p> <h1 id="vii-references">VII. References</h1> <ul> <li><a href="https://dataknowsall.com/blog/pcavisualized.html">2 Beautiful Ways to Visualize PCA</a></li> <li><a href="https://api.collegefootballdata.com">CFBD API Documentation</a></li> <li>I leveraged ChatGPT to essentially sanity-check my clusters and bounce ideas off about the overall characteristics for each one</li> <li>as always, scikit-learn documentation and associated packages is always <em>super</em> helpful</li> </ul>]]></content><author><name></name></author><category term="dtsc-3162"/><category term="clustering"/><category term="nfl"/><category term="draft"/><summary type="html"><![CDATA[The NFL Draft is on the horizon and all 32 teams are wondering the same thing: How will these college players translate to the NFL? How can we group college players together to reveal their NFL counterparts?]]></summary></entry><entry><title type="html">Project Three - Catching the Bag: Forecasting the NFL Wide Receiver Contracts Based Upon Historical Statistics</title><link href="https://aesareen.github.io/blog/2025/project3/" rel="alternate" type="text/html" title="Project Three - Catching the Bag: Forecasting the NFL Wide Receiver Contracts Based Upon Historical Statistics"/><published>2025-03-25T15:30:00+00:00</published><updated>2025-03-25T15:30:00+00:00</updated><id>https://aesareen.github.io/blog/2025/project3</id><content type="html" xml:base="https://aesareen.github.io/blog/2025/project3/"><![CDATA[<h1 id="i-introduction">I. Introduction</h1> <p>Amidst the donut-loving, chilly streets of New England persists one of the region’s most beloved and historic pastimes: sports. Having been born and spent the first twelve years of my life living in Massachusetts, I was practically indoctrinated in a fierce and persistent love for Boston sports teams—the Bruins, Red Sox, and Celtics all remain casual fandoms in my current life, but my absolute adoration for the New England Patriots and football has only blossomed in my years since departing the Bay State (perhaps due to the lack of a similarly successful equivalent in the Carolinas…).</p> <p>With this very crucial backstory out of the way (trust me), we can finally launch into the main theme of this project: predicting the contracts of NFL players. In case you’re unfamiliar, most sports operate on a contractual basis, where players are signed to a team for a specific number of years for up to a specified amount of money. In the NFL, when these players’ contracts expire and are not renewed by the original team (a rationale that ranges from the player simply not being good enough anymore to financial constraints of the team), the player enters a period known as <strong>free agency</strong>. Now, any team (with some occasional, nuanced exceptions that are beyond the scope of this project), are able to offer a contract to the player in hopes that they bring their talents to a new franchise. As you can imagine, free agency period—which typically coincides with the start of the NFL New Year in mid-March—is a deeply exciting and turbulent time as generations of lives change overnight through the formulation and signing of unfathomably lucrative contracts. For instance, in this most recent 2025 free agency period alone, just looking at players who are <em>wide receivers</em>, we can observe the following contracts:</p> <ul> <li>Ja’Marr Chase (<em>Cincinnati Bengals</em>): $<strong>161,000,000</strong> over 4 years, $<strong>40,250,000</strong> average per year</li> <li>D.K. Metcalf (<em>Pittsburgh Steelers</em>): $<strong>131,999,529</strong> over 4 years, $<strong>32,999,882</strong> average per year</li> <li>Tee Higgins (<em>Cincinnati Bengals</em>): $<strong>115,000,000</strong> over 4 years, $<strong>28,750,000</strong> average per year</li> <li>Chris Godwin (<em>Tampa Bay Buccaneers</em>): $<strong>66,000,000</strong> over 3 years, $<strong>22,000,000</strong> average per year</li> <li>Devante Adams (<em>Los Angeles Rams</em>): $<strong>44,000,000</strong> over 2 years, $<strong>22,000,000</strong> average per year</li> </ul> <p>Literally while working on this project, my beloved New England Patriots made a long-overdue splash for wide receiver Stefon Diggs, signing him to a massive 3 year, <strong>$63,500,000</strong> deal for an average of <strong>$21,166,667</strong> per year. With this obscene amount of money being thrown around, I think it is normal for both invested fans and even casual observers to ask: <strong>How much money should these players be made? What about their production on the field translates to their pay off the field?</strong>. To do this, we must launch in the world of regression and delve into the various models and techniques out there to predict continuous values. For simplicity, my project focuses on a very specific position in the NFL, the aforementioned wide receivers. However, the techniques and strategies explained below are equally applicable to any other positions (with different features and inputs of course).</p> <h2 id="iii-where-did-this-data-even-come-from">I.II: Where did this data even come from?</h2> <p>Now, despite how popular the NFL is and how big of a deal NFL free agency has become, there is actually <em>no easily accessible API or package for NFL salary data</em>. That means I had to utilize every site administrator’s worst nightmare: <strong>web scraping</strong>. Luckily, there is an awesome site called <a href="https://www.overthecap.com">overthecap.com</a> that provides an immense amount of financial data and information about NFL players, updated daily. There is a <a href="https://overthecap.com/contract-history/">very specific page</a> that focuses upon contract history and has data spanning all the way back to 1985 and robust tracking since the early 2010s. Hence, some basic using <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> parsing, I was able to quickly scrape OverTheCap and organize my data into a DataFrame that contains information like:</p> <ul> <li>Player Name</li> <li>Contract Sign Date</li> <li>Contract Value, Average Per Year (APY), and Length</li> <li>Whether the contract is still active</li> </ul> <p>This financial data is coupled with statistical data courtesy of the <code class="language-plaintext highlighter-rouge">nfl_data_py</code> package, which is the Python Wrapper of the amazing <a href="https://nflverse.nflverse.com/">nflverse</a> project that has made a huge amount of NFL player data and advanced statistics available for free. For our use case where we are focusing on receivers, this includes vital stats like:</p> <ul> <li>A Player’s Height, Weight, and Age</li> <li>A Player’s Receiving Yards, Touchdowns, Receptions and Targets Per Season</li> <li>A number of advanced stats like <a href="https://www.the33rdteam.com/epa-explained/">Receiving EPA</a>, <a href="https://www.addmorefunds.com/articles/nfl/air-yards/">Weighted Opportunity Rating</a>, and <a href="https://www.pff.com/news/fantasy-football-predicting-breakout-rookie-wide-receivers-using-pff-grades-and-dominator-rating">Dominator Rating</a></li> </ul> <p>These metrics will prove incredibly helpful in aiding our prediction, though as we will soon see, sometimes the most simple of calculations is all we need to make largely accurate predictions.</p> <p><strong>If you’d prefer to download this notebook, just press <a href="https://github.com/aesareen/3162-portfolio/blob/main/assets/jupyter/project_3.ipynb">here</a>.</strong></p> <h1 id="ii-how-does-regression-work">II. How Does Regression Work?</h1> <p>I use three main regression models throughout this notebook, but they all stem from the grandfather of all regression models: <strong>Linear Regression</strong>.</p> <p>Statistically, calculating linear regression for a single variable (a <strong>univariate</strong> function) relies upon a relatively simple equation to find the slope of our line, $\beta$, and the intercept, $\alpha$.</p> \[\beta = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{(x_i - \bar{x})^2}\] \[\alpha = \hat{y} - \beta\hat{x}\] <p>This <strong>analytical method</strong> works great for single variable functions, but doesn’t address the fact that many real-world problems involves a single variable depending on a variety of predictor variables, along with this approach doesn’t quite scale super-well computationally with the massive amounts of data that the modern world interacts with. Hence, the need for the <strong>closed-form</strong> solution, also known as <strong>Ordinary Least Squares</strong>. This utilizes an equation with a data matrix that stores all of our features and inputs for us, and uses some nifty matrix operations and matrix calculus to calculate the optimal weights for us with the following equation:</p> \[\hat{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\] <p>$X$ here is our <strong>data matrix</strong>, which is simply a matrix-representation of our data, for instance, if I had data that looked like this:</p> <head> <meta charset="UTF-8"/> <title>Centered Table</title> <style>table{margin:50px auto;border-collapse:collapse;text-align:center}th,td{border:1px solid #333;padding:8px 12px}</style> </head> <body> <table> <thead> <tr> <th>Receiving Yards</th> <th>Touchdowns</th> <th>Age</th> </tr> </thead> <tbody> <tr> <td>1300</td> <td>7</td> <td>22</td> </tr> <tr> <td>386</td> <td>2</td> <td>29</td> </tr> </tbody> </table> </body> <p>In a data matrix, that would look like:</p> \[\begin{pmatrix}1 &amp; 1300 &amp; 7 &amp; 22\\\ 1 &amp; 386 &amp; 2 &amp; 29\end{pmatrix}\] <p>You can see we add an additional column of 1’s to represent our intercept or <strong>bias</strong> (of course, this can be any constant value, but 1 is traditional to start out with).</p> <p>This OLS solution is awesome, and is in fact the method that scikit-learn uses underneath the hood with the <code class="language-plaintext highlighter-rouge">LinearRegression</code> class <a href="https://stackoverflow.com/a/34470001">[1]</a> but faces one main drawback: calculating $(\mathbf{X}^\top \mathbf{X})^{-1}$ can be a very expensive operation with very large datasets. Hence, there is one last approach, <strong>Least Mean Squares</strong> or also known as <strong>Linear Regression with Gradient Descent</strong> that adopts a more iterative optimization approach to deal with larger datasets that may take too long to run with OLS or work with data matrices that might not even fit into the memory of our machine to do the inversion.</p> \[\mathbf{w}_{new} = \mathbf{w}_{old} - \alpha \nabla J(\mathbf{w}_{old})\] <p>With this method, we are initializing our weights to random values and leveraging a hyperparameter called <strong>learning rate</strong>, $\alpha$ or $\eta$ (depending on where you look), to iteratively tweak our weights. The key behind any gradient descent process is a <strong>loss function</strong>, which in this case is $J(\mathbf{w}_{old})$. We can have a variety of loss functions, but the idea is that they represent the difference between the our true labels and the values we are currently predicting. We want to minimize this of course, hence why we take the <strong>gradient</strong> of it and subtract it from our current weights. In the case of Linear Regression, our loss function is really nice, <strong>the mean squared error</strong>:</p> \[J(\mathbf{w}_{old}) = \frac{1}{n}\sum{(x_{i}^\top \mathbf{w}_{old} - y_{i})^{2}} = \frac{1}{n}\sum{(\hat{y} - y_{i})^{2}}\] <p>It looks really complex, but it is really just finding the difference our our predicted value and true value and averaging that difference over the entire dataset. I utilize a univariate Linear Regression Model below and it performs not great, but given how simple it is, much better than I anticipated!</p> <p>The other models I use are summarized below, with a brief description for each:</p> <ul> <li><strong>Decision Trees</strong>: While decision trees are awesome for classification, we can also utilize them for regression while working with numerical values. They also work to minimize a statistic like MSE and split based on that instead of Gini Index or entropy; the leaf nodes are predicting numerical values instead of the predicted class.</li> <li><strong>Random Forests</strong>: Super similar idea to Decision Trees and their versatility, but they take it further by being an ensemble model that combines many weak decision trees together to usually create more powerful, robust predictions that are less prone to overfitting compared to a normal decision tree.</li> <li><strong>XGBoost</strong>: XGBoost is yet another ensemble model that is very efficient and effective, and among one of the most popular models for both classification and regression. To be honest, I don’t quite fully understand the nuances of gradient boosting quite yet but I know that it involves the “multiple-models” approach of random forests with the minimizing loss function through gradients that we discussed earlier with least mean squares. There is a really easy to use Python package that is super compatible with scikit-learn, so I decided to give it a go!</li> </ul> <h1 id="iii-data-understanding-and-pre-processing">III. Data Understanding and Pre-Processing</h1> <p>Given that I created the financial dataset myself, much of the pre-processing and formatting was already done when I collected the data. To get a better idea of why I decided to just focus on the wide receivers instead of creating a more general model to all predictions, we can quickly visualize the average salary values by position:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nfl_average_pay_per_year-480.webp 480w,/assets/img/nfl_average_pay_per_year-800.webp 800w,/assets/img/nfl_average_pay_per_year-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nfl_average_pay_per_year.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This chart can provide some really helpful insight into why a general model wouldn’t perform great: there is so much variation in the pay per position. Yes, we could obviously use pay as a feature and that would help, but in my mind, creating a tailored model on a per-position basis can allow so much more fine-tuning and adapting that you can only get on a granular scale. Receivers to me met the middle ground of being a valued position while not being so far down that their contracts are not particularly interesting (see other positions not even listed here like center, guard, or punter).</p> <p>With that solidified, we can move onto cleaning and processing the analytical data. To do that, I first had to use the pandas <code class="language-plaintext highlighter-rouge">.join()</code> methods to combine multiple stats datasets together and then had to do some name alterations to ensure that the names in this dataframe were consistent with the ones in the salary one:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a dictionary to hold the player name corrections
</span><span class="n">name_corrections</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">DK Metcalf</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">D.K. Metcalf</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Devonta Smith</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">DeVonta Smith</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Michael Pittman</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Michael Pittman, Jr.</span><span class="sh">"</span>
<span class="p">}</span>

<span class="c1"># Apply the corrections to the nfl_data DataFrame
</span><span class="k">for</span> <span class="n">old_name</span><span class="p">,</span> <span class="n">new_name</span> <span class="ow">in</span> <span class="n">name_corrections</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">nfl_data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">nfl_data</span><span class="p">[</span><span class="sh">'</span><span class="s">player_name</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="n">old_name</span><span class="p">,</span> <span class="sh">'</span><span class="s">player_name</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_name</span>
</code></pre></div></div> <p>With that out of the way, I decided to take a look at the top salaries overall in the NFL and because I had access to headshots via <code class="language-plaintext highlighter-rouge">nfl_data_py</code>, I decided to have some fun and make some nice tables:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/highest_paid_players_all-480.webp 480w,/assets/img/highest_paid_players_all-800.webp 800w,/assets/img/highest_paid_players_all-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/highest_paid_players_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/highest_paid_wrs-480.webp 480w,/assets/img/highest_paid_wrs-800.webp 800w,/assets/img/highest_paid_wrs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/highest_paid_wrs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This illustrates yet another reason why I think going with wide receivers is much better than focusing on the most flashy position, quarterback. If you look at the highest paid players list (which is exclusively QBs), some of the players there are <em>really</em> good and are among the best in their position. But some of the players there <em>kinda suck</em> and don’t really have the statistical feats of their peers to make an algorithmic approach predicated historical stats really feasible (tldr; Trevor Lawrence and Tua Tagovailoa are over-paid and would likely mess up my model). On the other hand, all of the wide receivers paid within the top ten are essentially the best-of-the-best only and all of them have strong cases to be in that list (some cases are stronger than others though).</p> <p>The <code class="language-plaintext highlighter-rouge">nfl_data_py</code> dataframe gives a great deal of information, and most of it isn’t particularly relevant, so I parsed it down to nine relevant features that I believe are most critical to this prediction (all of these are per season): player’s age, receptions, targets, receiving touchdowns, receiving yards, receiving air yards, receiving yards culminated after a catch, receiving EPA, target share, and weighted dominator score. To tackle our first experiment with using a simple linear regression model, I plotted each against the our target, which I decided to be average pay per year instead of contract value due to it being a bit more normalized against long, potentially misleading contracts.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nfl_feat_scatter-480.webp 480w,/assets/img/nfl_feat_scatter-800.webp 800w,/assets/img/nfl_feat_scatter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nfl_feat_scatter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="iv-experiment-one-univariate-linear-regression">IV. Experiment One: Univariate Linear Regression</h1> <p>Having already gone in-depth about Linear Regression and now identified the trends of our relevant features, we can finally begin some actual modeling. There is a slight problem, none of our features really look strictly linear, and in fact, most of them have a pretty similar trend line if we plotted a single one for each feature individually. To make it simple, I picked the most most straightforward feature, receiving yards, for my univariate linear regression model. As with any model, we first begin by splitting our data into relevant datasets:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">basic_X_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">basic_X_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">basic_y_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> 
<span class="n">basic_y_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> 

<span class="c1"># We have to use reshape here to turn our row vectors into column vectors
</span><span class="n">basic_X_trn</span><span class="p">,</span> <span class="n">basic_X_tst</span><span class="p">,</span> <span class="n">basic_y_trn</span><span class="p">,</span> <span class="n">basic_y_tst</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">combined_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">receiving_yards</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">combined_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">APY ($)</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">test_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">33</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <p>As with any scikit-learn model, creating and fitting our model is literally just two lines, which is awesome:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">basic_model</span><span class="p">:</span> <span class="n">LinearRegression</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">basic_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">basic_X_trn</span><span class="p">,</span> <span class="n">basic_y_trn</span><span class="p">)</span>
</code></pre></div></div> <p>Given that we are doing quite a bit of repeated evaluation throughout this project that is essentially the same sequence over and over again, I decided to create a helper function <code class="language-plaintext highlighter-rouge">evaluate_model</code> that helps speed up this process a bit. It simply takes in the model to evaluate, the relevant training &amp; test datasets, and then also a list of metrics that we want to evaluate the model on, calculating each metrics on both the training &amp; test datasets:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Callable</span>

<span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">X_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">X_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">metrics</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="nb">float</span><span class="p">]],</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Calculate the metric scores for a model

    Args:
        model (BaseEstimator): scikit learn model to use for evaluation
        X_trn (np.ndarray): training input for the model
        X_tst (np.ndarray): testing input 
        y_trn (np.ndarray): training output
        y_tst (np.ndarray): testing output
        metrics (list[Callable[[np.ndarray, np.ndarray], float]]): metrics to evaluate the model on; from sklearn.metrics
        model_name (str, optional): name of the model to use in print statements. Defaults to </span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="s">.

    Returns:
        tuple[dict[str, float], np.ndarray, np.ndarray]: metrics of the model, train predictions of model, and test predictions
    </span><span class="sh">"""</span>
    <span class="n">metric_scores</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="c1"># Calculate the training metrics
</span>    <span class="n">train_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_trn</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
        <span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">metric</span><span class="p">.</span><span class="n">__name__</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">).</span><span class="nf">title</span><span class="p">()</span>
        <span class="n">metric_score</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nf">metric</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">train_preds</span><span class="p">)</span>
        <span class="n">metric_scores</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s">_train</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric_score</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The </span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s"> on the training data of our </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s"> is </span><span class="si">{</span><span class="n">metric_score</span><span class="si">:</span><span class="p">,.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Calculate the test metrics
</span>    <span class="n">test_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_tst</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
        <span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">metric</span><span class="p">.</span><span class="n">__name__</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">).</span><span class="nf">title</span><span class="p">()</span>
        <span class="n">metric_score</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nf">metric</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">test_preds</span><span class="p">)</span>
        <span class="n">metric_scores</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s">_test</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric_score</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The </span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s"> on the testing data of our </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s"> is </span><span class="si">{</span><span class="n">metric_score</span><span class="si">:</span><span class="p">,.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">metric_scores</span><span class="p">,</span> <span class="n">train_preds</span><span class="p">,</span> <span class="n">test_preds</span>
</code></pre></div></div> <p>We can then use this function to do much of the evaluation for us:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">basic_eval</span><span class="p">,</span> <span class="n">basic_train_preds</span><span class="p">,</span> <span class="n">basic_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">basic_model</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">basic_X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">basic_X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">basic_y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">basic_y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Basic Linear Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Output
The Root Mean Squared Error on the training data of our Basic Linear Regression Model is 2,983,494.0606
The R2 Score on the training data of our Basic Linear Regression Model is 0.6813
The Root Mean Squared Error on the testing data of our Basic Linear Regression Model is 2,798,006.3796
The R2 Score on the testing data of our Basic Linear Regression Model is 0.4157
</code></pre></div></div> <p>That’s not particularly great, let’s quickly visualize our line to see how close it is to the actual points (I created another helper function called <code class="language-plaintext highlighter-rouge">make_plot</code> but forgot that my future models will use more than one feature, making it pretty difficult to plot.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">basic_plot</span> <span class="o">=</span> <span class="nf">make_plot</span><span class="p">(</span><span class="n">basic_X_tst</span><span class="p">,</span> <span class="n">basic_y_tst</span><span class="p">,</span> <span class="n">basic_test_preds</span><span class="p">,</span> <span class="sh">"</span><span class="s">Basic Linear Regression: Just Using Receiving Yards</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/basic_lin_reg-480.webp 480w,/assets/img/basic_lin_reg-800.webp 800w,/assets/img/basic_lin_reg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/basic_lin_reg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So based on this woeful model performance and the previous scatterplot we made, we can clearly see that relying upon a model that uses just a single feature is not going to be nearly enough to succeed. Thus, let’s move on to the next part: multivariate regression.</p> <h1 id="v-experiment-two-multivariate-regression-models">V. Experiment Two: Multivariate Regression Models</h1> <p>Before we delve into using more sophisticated models, it’s clear that we are going to need to buff up our training and test data to include the relevant features that we identified earlier.</p> <p>Wide Receivers have a big of flexibility in the NFL, and some players who are actually listed as a Wide Receiver never play the position at all but instead serve as integral pieces of a team’s special teams (see Patriots legend <a href="https://en.wikipedia.org/wiki/Matthew_Slater">Matthew Slater</a>). Players that fill into this bucket typically have their weighted dominator score be 0, so we will do a little bit of data pre-processing here and simply drop those rows (it’s only a small handful of players affected).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's create a new training set that relies on a few more features
</span><span class="n">X_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">X_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">y_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> 
<span class="n">y_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> 

<span class="c1"># Some receivers just play special teams and don't necessarily really play as a traditional Wide Receiver, so if there w8dom value is NaN here, we can assume they fall into that category
</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">combined_stats</span><span class="p">[</span><span class="n">rel_features</span><span class="p">].</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">w8dom</span><span class="sh">'</span><span class="p">).</span><span class="n">values</span><span class="p">,</span> <span class="n">combined_stats</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">w8dom</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">APY ($)</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">test_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">33</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <p>With these dataset, we can follow a very simple sequence of method and steps to our linear regression model for our next model: A Decision Tree.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing out a Decision Tree
</span><span class="n">tree_model</span><span class="p">:</span> <span class="n">DecisionTreeRegressor</span> <span class="o">=</span> <span class="nc">DecisionTreeRegressor</span><span class="p">()</span>
<span class="n">tree_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">)</span>

<span class="n">tree_eval</span><span class="p">:</span> <span class="nb">dict</span>
<span class="n">tree_train_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">tree_test_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>

<span class="n">tree_eval</span><span class="p">,</span> <span class="n">tree_train_preds</span><span class="p">,</span> <span class="n">tree_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">tree_model</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Decision Tree Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Root Mean Squared Error on the training data of our Decision Tree Regression Model is 291,430.6363
The R2 Score on the training data of our Decision Tree Regression Model is 0.9970
The Root Mean Squared Error on the testing data of our Decision Tree Regression Model is 2,288,790.0169
The R2 Score on the testing data of our Decision Tree Regression Model is 0.6443
</code></pre></div></div> <p>We are already seeing much better performance! But, look at the discrepancy in the training metrics and the testing metrics; that is a massive difference and a clear sign of <strong>overfitting</strong>. Seeing this, I realized that I needed to figure out why this was happening and potential steps to mitigate it. First off, was there a particular feature that the Decision Tree was overfitting to especially?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># What feature is our model overfitting to?
</span><span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">tree_model</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">combined_stats</span><span class="p">[</span><span class="n">rel_features</span><span class="p">].</span><span class="n">columns</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>receiving_yards                0.806376
tgt_sh                         0.060913
targets                        0.038668
w8dom                          0.029500
receiving_epa                  0.017403
receiving_air_yards            0.016052
age                            0.013541
receptions                     0.007803
receiving_yards_after_catch    0.005176
receiving_tds                  0.004569
dtype: float64
</code></pre></div></div> <p>Ah! So we see a huge part of our model’s predictions is just looking at the player’s receiving yards (which we saw above isn’t necessarily the best idea) and then using the other features as essentially afterthoughts to tune its predictions. Now, we have to ask ourselves <strong>is this a problem?</strong>. The model overfitting certainly is, but is just dropping receiving yards the real solution? Because, NFL teams certainly do look at receiving yards as a major indicator of how to structure a player’s contracts, and in many regards, <strong>receiving yards is the most important statistic for a WR</strong>. I did some experimentation, specifically some <strong>ablation tests</strong> where I dropped receiving yards and evaluate the performance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's see if we can remove receiving_yards to get similar accuracy
</span><span class="n">rel_features</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="sh">'</span><span class="s">receiving_yards</span><span class="sh">'</span><span class="p">)</span>
<span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">combined_stats</span><span class="p">[</span><span class="n">rel_features</span><span class="p">].</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">w8dom</span><span class="sh">'</span><span class="p">).</span><span class="n">values</span><span class="p">,</span> <span class="n">combined_stats</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">w8dom</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">APY ($)</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">test_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">33</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">tree_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">)</span>
<span class="n">tree_eval</span><span class="p">,</span> <span class="n">tree_train_preds</span><span class="p">,</span> <span class="n">tree_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">tree_model</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Decision Tree Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Root Mean Squared Error on the training data of our Decision Tree Regression Model is 291,430.6363
The R2 Score on the training data of our Decision Tree Regression Model is 0.9970
The Root Mean Squared Error on the testing data of our Decision Tree Regression Model is 2,176,075.7494
The R2 Score on the testing data of our Decision Tree Regression Model is 0.6785
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># View new feature importances
</span><span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">tree_model</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">combined_stats</span><span class="p">[</span><span class="n">rel_features</span><span class="p">].</span><span class="n">columns</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tgt_sh                         0.696595
targets                        0.114698
w8dom                          0.055644
receiving_air_yards            0.052225
receiving_epa                  0.035021
age                            0.016034
receiving_yards_after_catch    0.012006
receiving_tds                  0.009475
receptions                     0.008303
dtype: float64
</code></pre></div></div> <p>We find ourselves getting nearly identical performance with the model having a more balanced feature importance index. Hence, I decided to remove <code class="language-plaintext highlighter-rouge">receiving_yards</code> as a way to hopefully make our model more generalizable, interpretable, and less prone to overfitting. I utilize this new updated feature list for my next model, which also aims to combat overfitting by combine multiple decision trees together: random forests.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's try to over-combat the overfitting with an ensemble model
</span><span class="n">rf</span><span class="p">:</span> <span class="n">RandomForestRegressor</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">()</span>
<span class="n">rf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>

<span class="n">rf_eval</span><span class="p">:</span> <span class="nb">dict</span> 
<span class="n">rf_train_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">rf_test_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>

<span class="n">rf_eval</span><span class="p">,</span> <span class="n">rf_train_preds</span><span class="p">,</span> <span class="n">rf_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">rf</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Random Forest Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Root Mean Squared Error on the training data of our Random Forest Regression Model is 797,022.3446
The R2 Score on the training data of our Random Forest Regression Model is 0.9779
The Root Mean Squared Error on the testing data of our Random Forest Regression Model is 1,852,110.1425
The R2 Score on the testing data of our Random Forest Regression Model is 0.7671
</code></pre></div></div> <p>We can see that we get an instant boost in out testing accuracy at minimal decrease in our trainining accuracy, and if we inspect the feature importances for the random forest model, we get a much more balanced picture:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tgt_sh                         0.385418
receiving_air_yards            0.301290
w8dom                          0.064857
targets                        0.062970
receiving_yards_after_catch    0.055652
receiving_epa                  0.051901
receptions                     0.051064
age                            0.015059
receiving_tds                  0.011789
dtype: float64
</code></pre></div></div> <p>We are striking a good medium of using target share and receiving air yards to predict now, however the weighted dominator factor, target, and receptions all have notable impacts on the final predicted score.</p> <p>Finally, we can move onto our last model: XGBoost. Even though this not directly from the scikit-learn library, it’s objects and methods are directly compatiable with the library and the code is essentially identical</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Another ensemble method, but maybe even more generalizable and powerful: XGBoost!
</span><span class="n">xgboost</span><span class="p">:</span> <span class="n">XGBRegressor</span> <span class="o">=</span> <span class="nc">XGBRegressor</span><span class="p">()</span>
<span class="n">xgboost</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">)</span>


<span class="n">xgboost_eval</span><span class="p">:</span> <span class="nb">dict</span> 
<span class="n">xgboost_train_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">xgboost_test_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>

<span class="n">xgboost_eval</span><span class="p">,</span> <span class="n">xgboost_train_preds</span><span class="p">,</span> <span class="n">xgboost_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">XGBoost Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Root Mean Squared Error on the training data of our XGBoost Regression Model is 293,540.1383
The R2 Score on the training data of our XGBoost Regression Model is 0.9970
The Root Mean Squared Error on the testing data of our XGBoost Regression Model is 1,902,217.4789
The R2 Score on the testing data of our XGBoost Regression Model is 0.7543
</code></pre></div></div> <p>We can see that the latter two models do very similarly in performance, and all of them can be helpful in trying to acocomplish our ultimate goal: predicting these contracts. With these enhanced models out of the way, we can move onto the culminating experiement: <strong>hyperparameter tuning</strong>.</p> <h1 id="vi-experiment-three-hyperparameter-tuning">VI. Experiment Three: Hyperparameter Tuning</h1> <p>I utilized hyperparameter tuning last project, and none of my models gave me even useable results until I did. My accuracy for this project is a bit better this time around, so I am not expecting so drastic of an improvement, but at least noticable growth is RMSE and $R^2$ would be great.</p> <p>The process for all three models—decision trees, random forests, and XGBoost—is essentially identical, with the only difference being the actual parameters we are tuning of course. For reference, I show the code for the XGBoost tuning below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgboost_param_grid</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.10</span><span class="p">,</span><span class="mf">0.15</span><span class="p">,</span><span class="mf">0.20</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.30</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">min_child_weight</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span> <span class="p">],</span>
    <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span> <span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.4</span> <span class="p">],</span>
    <span class="sh">"</span><span class="s">colsample_bytree</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.5</span> <span class="p">,</span><span class="mf">0.7</span> <span class="p">]</span>
<span class="p">}</span>

<span class="n">xgboost_random_search</span><span class="p">:</span> <span class="n">RandomizedSearchCV</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="n">param_distributions</span> <span class="o">=</span> <span class="n">xgboost_param_grid</span><span class="p">,</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">neg_root_mean_squared_error</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r2</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">refit</span> <span class="o">=</span> <span class="sh">"</span><span class="s">neg_root_mean_squared_error</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">11</span>
<span class="p">)</span>

<span class="n">xgboost_random_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>
<span class="n">tuned_xgboost</span><span class="p">:</span> <span class="n">XGBRegressor</span> <span class="o">=</span> <span class="n">xgboost_random_search</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The best estimator parameters were: </span><span class="si">{</span><span class="n">xgboost_random_search</span><span class="p">.</span><span class="n">best_params_</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">tuned_forest_eval</span><span class="p">,</span> <span class="n">tuned_forest_train_preds</span><span class="p">,</span> <span class="n">tuned_forest_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">tuned_xgboost</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Tuned XGBoost Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The results for all of the models, including the hyparameter tuned ones, can be found below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nfl_results_table-480.webp 480w,/assets/img/nfl_results_table-800.webp 800w,/assets/img/nfl_results_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nfl_results_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This table shows us that while hyperparameter tuning definietely helped our models here, it wasn’t the parameters themselves this time that needed to be radically changed to improve performance. Instead, our models might benefit from other measures like regularization to further improve performance and generalizability. Oh well, that’s for another project.</p> <h1 id="vii-conclusion-and-impact">VII. Conclusion and Impact</h1> <p>One of the biggest impacts of a project like this is that currently contract negotations demand an intimate knowledge of the current market for a player and how a player’s stats translate into tangible financial value. This can help provide players that are representing themselves or athletes that are a bit earlier into their career that cannot quite afford an expensive agent to get a rough estimate about how their production thus far translates to NFL dollar amounts. However, one of the drawbacks is that there are so many aspects that go into contract agreements that is deeply unquantifable, at least not readily; how much a player likes or dislikes a particular geogrpahic location, the increased tax rates in one state compared to another that can inflate the final APY, or the famous “hometown discount” that players like <a href="https://www.cbssports.com/nfl/news/agents-take-what-type-of-discount-will-tom-brady-give-the-patriots-on-his-contract-this-time/">Tom Brady took for years</a> so that the team could sign players that put them in a better position to win. These things are beyond the scope of nearly any machine learning model, and certainly mine.</p> <p>However, what we did see is that what matters in terms of differentiating the pay day for various NFL wide receivers is relatively simple, the stats that have been tracked since the conception of the game of football: receiving yards and how prominent a player is within their offense (the latter has not been numerically calculated for as long as the former, but both have historically been very easy to spot). Additionally, I learned that creating datasets is a lot more time intensive and laborious than it seems and I am inifintely grateful for the incredible work by millions of people on Kaggle, scientific labortories, and countless other entities that take the time to collect, normalize, and parse much of this data for us. Finally, while my previous project showed the immense power of hyperparameter tuning, this one showed that it certainly can’t be the only avenue to improve performance and should, as with many things in machine learning, be one of the many tools that we utilize in order to make our models more robust and usable in the everday world.</p> <h1 id="viii-references">VIII. References</h1> <p><a href="www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup Documentation</a> <br/> <a href="https://machinelearningmastery.com/xgboost-for-regression/">XGBoost for Regression</a> <br/> <a href="https://jayant017.medium.com/hyperparameter-tuning-in-xgboost-using-randomizedsearchcv-88fcb5b58a73">Hyperparameter Tuning for XGBoost</a> <br/> Special shotout to the scikit-learn, great_tables, and seaborn documentation :)</p>]]></content><author><name></name></author><category term="dtsc-3162"/><summary type="html"><![CDATA[Amid the massive spending in NFL free agency, can we truly answer whether teams are doling out good contracts? Is there a way to qualitatively predict what a player should be paid based upon their output and production?]]></summary></entry><entry><title type="html">Project Two - Educationally Influenced: Predicting the Academic Success Rates of Students in Cyprus</title><link href="https://aesareen.github.io/blog/2025/project2/" rel="alternate" type="text/html" title="Project Two - Educationally Influenced: Predicting the Academic Success Rates of Students in Cyprus"/><published>2025-02-24T14:30:00+00:00</published><updated>2025-02-24T14:30:00+00:00</updated><id>https://aesareen.github.io/blog/2025/project2</id><content type="html" xml:base="https://aesareen.github.io/blog/2025/project2/"><![CDATA[<h1 id="i-introduction">I. Introduction</h1> <p>As someone who always wanted to go into teaching, I have always been fascinated by the diverse makeups that classrooms bring together, there is perhaps not such a more heterogenous space in society that is so commonly available. The magic of teaching for me is that every single student carries their own dreams, aspirations, and motivations—and crucially from a pedagogical perspective, their own background knowledge. It is miraculously up the lecturer at hand to convey knowledge at a carefully-sculpted rate, depth, and breadth that sufficiently engages all students without leaving those behind that are clearly struggling with the ideas or stultifying those that clearly have sufficiently grasped the material and are ready for a greater challenge (or perhaps, most frustratingly for a teacher, students who simply do not care). In an ideal scenario, teachers could analyze the background and study habits of each student, develop a personalized plan that either curbs potential barricades to academic success or encourages characteristics that underpin in, and reap the success of a spirited, confident, and well-educated classroom.</p> <p>Where such a ideality was previously limited to the imaginations of educators, like many things in the modern-day, it can increasingly become a reality with the rise of machine learning and artificial intelligence systems. Ethical considerations at bay (because they certainly are quite a few), the ability to input an entire student’s demographics, personality, and interests into an algorithm and immediately how to best get that student to learn and critically, enjoy that learning, is maybe one of the most altruistic and revolutionary innovations brought upon this digital revolution.</p> <p>Thus, the following project delves into a fundamental question: <strong>Which features of a student are most correlated with academic success and how can we utilize these features to predict which grades they will achieve in their studies?</strong>. Forecasting such an outcome takes more than just analyzing a student’s study hours and class attendance; it demands at through look at their socioeconomic status, their familial circumstances, and their actions before, during, and after class. Only with such a holistic view can we even begin giving justice to this socially-essential inquiry.</p> <h3 id="where-did-this-data-even-come-from">Where did this data even come from?</h3> <p>Courtesy of the amazing <a href="https://www.archive.ics.uci.edu/">UC Irvine Machine Learning Repository</a> is a <a href="https://archive.ics.uci.edu/dataset/856/higher+education+students+performance+evaluation">dataset</a> released in 2023 about Engineering and Educational Science students attending Near East University in Cyprus. The best part of this dataset is that was utilized to support findings in a paper named <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-35249-3.pdf"><em>Student Performance Classification Using Artificial Intelligence Techniques</em></a>, so while doing my analysis, I can compare the results to actual researchers that completed a very similar task (and see if I can beat them, probably not though).</p> <p>There are not an incredible amount of samples in the dataset (only 145), but quite a few features for each student that include:</p> <ul> <li>Parental Education &amp; Other Familial Information</li> <li>Study Habits (amount of hours, amount of scientific &amp; non-scientific literature read, how often a student took notes in class)</li> <li>Attendance to various academic-related events and also classes</li> <li>Preparation before particular exams</li> <li>The student’s academic performance (both in GPA and grade)</li> </ul> <p><strong>If you’d prefer to download this notebook, just press <a href="https://github.com/aesareen/3162-portfolio/blob/main/assets/jupyter/project_2.ipynb">here</a>.</strong></p> <h1 id="ii-pre-processing-and-visualizing-the-data">II. Pre-Processing and Visualizing the Data</h1> <p>We can load in our dataframe like always do to get started:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">edu_df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./datasets/cyprus_education_dataset.csv</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Let’s take a peek at our first ten rows:</p> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>STUDENT ID</th> <th>1</th> <th>2</th> <th>3</th> <th>...</th> <th>29</th> <th>30</th> <th>COURSE ID</th> <th>GRADE</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>STUDENT1</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <th>1</th> <td>STUDENT2</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>2</td> <td>3</td> <td>1</td> <td>1</td> </tr> <tr> <th>2</th> <td>STUDENT3</td> <td>2</td> <td>2</td> <td>2</td> <td>...</td> <td>2</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>3</th> <td>STUDENT4</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>3</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>...</th> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <th>141</th> <td>STUDENT142</td> <td>1</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>5</td> </tr> <tr> <th>142</th> <td>STUDENT143</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>4</td> <td>3</td> <td>9</td> <td>1</td> </tr> <tr> <th>143</th> <td>STUDENT144</td> <td>2</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>4</td> </tr> <tr> <th>144</th> <td>STUDENT145</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>5</td> <td>4</td> <td>9</td> <td>3</td> </tr> </tbody> </table> <p>145 rows × 33 columns (total)</p> </div> <p>We can see that all of the column names and values are numerical, which is super helpful perhaps for a machine learning model but not so helpful for us mere human non-models. So, my first pre-processing step was simply renaming all the columns to the values they actually are so you (and I) can figure out what they are a little bit more.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The columns are just numbers, so I am replacing them with their actual values
</span><span class="n">col_names</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Sex</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">School Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Scholarship Percentage</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Additional Work</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Regular Art/Sports</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Has Partner</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Total Salary</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Transportation Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Accommodation Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">Mother</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Father</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Number of Sisters / Brothers</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Parental Status</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Mother</span><span class="sh">'</span><span class="s">s Occupation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Father</span><span class="sh">'</span><span class="s">s Occupation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Weekly Study Hours</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Reading frequency (non-scientific books/journals)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Reading frequency (scientific books/journals)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Attendance to department seminars / conferences</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Impact of projects / activities on your success</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Attendance to Classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Preparation to midterm exam 1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Preparation to midterm exams 2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Taking notes in classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Listening in classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Discussion improves my interest and success in the course</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Flip-classroom</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Cumulative GPA last semester</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Expected GPA at graduation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Course ID</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Output Grade</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Student ID</span><span class="sh">"</span><span class="p">]</span>
<span class="n">edu_df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">col_names</span>
</code></pre></div></div> <p>Now we have:</p> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>Age</th> <th>Sex</th> <th>School Type</th> <th>Scholarship Percentage</th> <th>...</th> <th>Expected GPA at graduation</th> <th>Course ID</th> <th>Output Grade</th> <th>Student ID</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>STUDENT1</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <th>1</th> <td>STUDENT2</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>2</td> <td>3</td> <td>1</td> <td>1</td> </tr> <tr> <th>2</th> <td>STUDENT3</td> <td>2</td> <td>2</td> <td>2</td> <td>...</td> <td>2</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>3</th> <td>STUDENT4</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>3</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>...</th> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <th>141</th> <td>STUDENT142</td> <td>1</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>5</td> </tr> <tr> <th>142</th> <td>STUDENT143</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>4</td> <td>3</td> <td>9</td> <td>1</td> </tr> <tr> <th>143</th> <td>STUDENT144</td> <td>2</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>4</td> </tr> <tr> <th>144</th> <td>STUDENT145</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>5</td> <td>4</td> <td>9</td> <td>3</td> </tr> </tbody> </table> <p>145 rows × 33 columns (total)</p> </div> <p>Much better! Our values are still a bit vague (what is a 3 Scholarship Percentage), but we can modify each column as necessary for our visualization step. Now we can move on to seeing the first step actually pre-processing our data: verifying if there any null values.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The number of NaN values per column in our dataset: </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">edu_df</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The number of NaN values per column in our dataset: 
 Age                       0
Sex                       0
School Type               0
Scholarship Percentage    0
Additional Work           0
Regular Art/Sports        0
Has Partner               0
Total Salary              0
Transportation Medium     0
Accommodation Type        0
dtype: int64
</code></pre></div></div> <p>Ah! Perfect! Super unrealistic but UCI archive did actually tell us there are no missing values within this dataset, making it super easy to work with. That is likely the result of this being a very small and manually-collected collection process, so many missing values were likely handled long ago (also the fact that the original researchers would deal with them as part of their own ML work before releasing it to the public!).</p> <p>However, we do not have zero pre-processing to do (ah, I wish). There is still quite a bit work we need to do if you want to make sense of all of this data and ensure that we are identifying the most pertinent features and the best model to identify trends within those features. If we are are trying to predict student outcomes, I think a good first step is to see what outcomes we are dealing with; in other words, model the distribution of grades and GPA that this particular dataset contains.</p> <p>However, in order to do that, we must do some interesting conversions as the grading system is currently numerically encoded and those numbers convert to the specific grading system used at NEU University in Cyprus, which must be further converted to ECTS grades and finally US-scale grades. To accomplish this, I used a resource provided by NEU to convert their grading system to ECTS and then utilized the most logical grade from the ECTS system based upon the equivalent US grade # Let’s change the grades so they are a lit bit more interpretable by us <a href="https://muhendislik.neu.edu.tr/wp-content/uploads/sites/146/2022/07/27/PGE_Exam-Regulations-Assessment-and-Grading.pdf?ver=01278343a7a9d5d38fc21f2487da1610">[1]</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grades_mapping</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="sh">'</span><span class="s">C-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="sh">'</span><span class="s">C+</span><span class="sh">'</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span> <span class="sh">'</span><span class="s">B-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">6</span><span class="p">:</span> <span class="sh">'</span><span class="s">B+</span><span class="sh">'</span><span class="p">,</span> <span class="mi">7</span><span class="p">:</span> <span class="sh">'</span><span class="s">A-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="mi">9</span><span class="p">:</span> <span class="sh">'</span><span class="s">A+</span><span class="sh">'</span><span class="p">}</span>

<span class="c1"># Convert the numerical Cyprus grading system grades to American Letter Grades
</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span><span class="n">grades_mapping</span><span class="p">)</span>

<span class="c1"># Make these letter grades ordinal
</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span>
    <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">],</span> 
    <span class="n">categories</span><span class="o">=</span><span class="n">grades_mapping</span><span class="p">.</span><span class="nf">values</span><span class="p">(),</span> 
    <span class="n">ordered</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <p>From there, I used my favorite visualization package, <code class="language-plaintext highlighter-rouge">plotly</code> to make two graphs: One of the grade distribution of all the students within our training dataset and one of the expected GPA distribution of all the students.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grade_dist_of_students-480.webp 480w,/assets/img/grade_dist_of_students-800.webp 800w,/assets/img/grade_dist_of_students-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/grade_dist_of_students.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpa_dist_of_students-480.webp 480w,/assets/img/gpa_dist_of_students-800.webp 800w,/assets/img/gpa_dist_of_students-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpa_dist_of_students.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, that might look a bit concerning given that most students do not have a very good grade, however, I view it as a potential benefit. It shows that there are a very select amount of students that are high-performing in this given survey and analyzing their patterns, situation, and demographics especially can provide a lot of insight into the background of successful students.</p> <p>Now, we need to get an idea of which features are particularly pertinent to student success. I have a few initial ideas (such as parent education, how often they attend classes, and how often they listen in classes), but I have no idea which factors high-performing students are doing the most. So I created a series of seaborn <em>swarmplots</em> to help me with this; swarmplots are the best pick here over a traditional scatterplot as much of the data we have overlaps with one another and we need to add a bit of jitter to fully see the data (which is especially helpful given we don’t have an overwhelming number of observations here, which swarmplots struggle with). We can apply some more pre-processing before using the swarmplot to make the axis titles make a bit more sense (previously, they were solely numerical values and I have no idea what a 3 in Mother’s Education means), and then make a subplot of each relevant feature against our target—a student’s grade—to visualize which features might be worth really analyzing!</p> <div class="row mt-3"> <div class="col-lg mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/education_feats_swarmplot-480.webp 480w,/assets/img/education_feats_swarmplot-800.webp 800w,/assets/img/education_feats_swarmplot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/education_feats_swarmplot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="iii-model-selection">III. Model Selection</h1> <p>Given that our data is not very linear at all (we can see that with the massive amount of overlap between different data points), utilizing a model that does well this type of data is key. With that in mind, I identified the following models as likely the candidates to perform well for a classification task:</p> <ul> <li><strong>Support Vector Machine with a non-linear kernel function</strong>: Support Vector Machines are best when we are not working with linearly separable data, as we are here; it is able to project our data using a <em>kernel function</em> and find a hyperplane that separates our classes in feature space. They are not the most interpretable machine learning model out there, but they are powerful and versatile to a wide domain tasks (hopefully including ours!)</li> <li><strong>Random Forest</strong>: Random Forests are an incredibly popular and powerful ensemble model that conglomerates multiple weak decision trees together to create a model that is overall much more accurate and robust. These slew of decision trees mean they can usually provide high accuracy and also provide some awesome insight into the most vital features of our dataset, but unlike their little siblings the Decision Tree, they are not nearly are interpretable or computationally inexpensive, and can also be suspectable to bias to overrepresented classes.</li> <li><strong>Gradient Boosting Machine</strong>: Gradient Boosting is yet another ensemble algorithm like Random Forest, but it takes a step further by building upon previous models sequentially and correcting the errors of each predecessor. It shares many of the same benefits of Random Forests but with the added benefit of really handling weird datasets well, such as data that is missing or needs to be robustly pre-processed before using it. However, it can overfit and also takes quite a bit of resources to hyperparameter tune (I also don’t understand it as well as the others).</li> </ul> <p>With those models identified, let’s go to implementing them!</p> <h1 id="iv-model-implementation">IV. Model Implementation</h1> <h2 id="ivi-splitting-our-data">IV.I: Splitting our Data</h2> <p>Before implementing any models, we have to first divide our dataset into a series of training and test datasets. To do this, we can utilize the nifty <code class="language-plaintext highlighter-rouge">train_test_split</code> function from Scikit-Learn’s <code class="language-plaintext highlighter-rouge">model_selection</code> module. Before I split my data, I also dropped features that I didn’t feel like the model should use for classification (such as gender and age) and those that overlap with our main criteria for student success (output grades), such as expected GPA at graduation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Sex</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">School Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Student ID</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Parental Status</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Has Partner</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Expected GPA at graduation</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cumulative GPA last semester</span><span class="sh">'</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">()</span>
</code></pre></div></div> <p>Also, given the huge skew we saw in the grade distribution during our data visualization, I opted to utilize the stratify parameter with <code class="language-plaintext highlighter-rouge">train_test_split</code> to ensure that classes were equally represented in both our training and testing dataset, hopefully giving our models the best chance to not overfit and make good predictions on the test dataset!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Opting for a slightly bigger test size here given our smaller dataset along with stratifying given the heavy skew in grades that we saw with the previous visualization
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <h2 id="ivii-evaluation-metrics">IV.II: Evaluation Metrics</h2> <p>To see our model performed, we need a way to quantitatively score it. For this, I utilize a series of built-in functions from Scikit-Learn to calculate accuracy, F1-Score, and Confusion Matrices. Accuracy is the default calculation when doing something like this, and is always a helpful metric to have, but in an imbalanced dataset like this, can be highly misleading. If our model does overfit and assigns almost every student a D grade (<em>foreshadowing</em>), then our accuracy might be OK but the few students that do have higher grades will be completely overshadowed! For that reason, I also decided to include a weighted F1-Score to ensure that both Precision and Recall are included within our evaluation, and we aren’t applauding a model that in reality is pretty terrible. Finally, our confusion matrix can provide some great insight into how the model is classifying various how it should be classifying by showing us the predicted class versus the actual class, so if it does make a mistake, we can see which class it is from. Again, many of the metrics are already provided by Scikit-Learn, but I did make wrappers for each so I could just generalize them a bit easier:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Any</span>

<span class="k">def</span> <span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates the accuracy for a given model on a test set

    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model

    Returns:
        float: Model accuracy
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates the f1-score for a given model on a test set

    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model

    Returns:
        float: Model f1-score
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]),</span>  <span class="n">average</span> <span class="o">=</span> <span class="sh">'</span><span class="s">weighted</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GT</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates and visualizes the confusion matrix for a given model on a test set
    
    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model
        
    Returns:
        
    </span><span class="sh">"""</span>
    <span class="c1"># Get predictions and compute confusion matrix
</span>    <span class="n">cm</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="nf">from_estimator</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">cm</span>
</code></pre></div></div> <h2 id="iviii-naive-implementation">IV.III: Naive Implementation</h2> <p>Finally onto making and implementing our models! As any good Machine Learning Engineer would do (right?), after splitting my data and creating my evaluation metrics, I just throw my data to my model with essentially no other tuning or customization. The results, were um, not <em>great</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">svc</span> <span class="o">=</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">kernel</span> <span class="o">=</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="mi">11</span><span class="p">)</span>

        <span class="n">svc</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">svc_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">rf</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">7</span><span class="p">)</span>
        <span class="n">rf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">rf_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">gb</span> <span class="o">=</span> <span class="nc">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">150</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="p">.</span><span class="mi">15</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">)</span>
        <span class="n">gb</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">gb</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">gb_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.32756132756132755</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.32756132756132755</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.42273533204384267</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_svc_cm-480.webp 480w,/assets/img/edu_svc_cm-800.webp 800w,/assets/img/edu_svc_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_svc_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_rf_cm-480.webp 480w,/assets/img/edu_rf_cm-800.webp 800w,/assets/img/edu_rf_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_rf_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_gb_cm-480.webp 480w,/assets/img/edu_gb_cm-800.webp 800w,/assets/img/edu_gb_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_gb_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>Taking a look at the confusion matrices, we can see that all three models do what I feared: categorizing almost every student as having a D grade, regardless of what the features indicated. They need some serious refinement if they want to actually become usable for anything practical. We can start by reducing the number of features that each utilizes.</p> <h2 id="iviv-feature-engineering">IV.IV: Feature Engineering</h2> <p>While I love this dataset and all of the information it provides about students, it’s clear that not all of it is pertinent to accurately predicting student performance and in many cases, might be hurting it. Thus, we need to see which features models are utilizing and focus on those instead of muddying the water with unnecessary noise. Luckily, random forests have a class attribute that can provide us that exact information:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_importances</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">rf</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">most_important_features</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="n">feature_importances</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">head</span><span class="p">(</span><span class="mi">7</span><span class="p">).</span><span class="n">index</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The most important features are: </span><span class="si">{</span><span class="n">most_important_features</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">The</span> <span class="n">most</span> <span class="n">important</span> <span class="n">features</span> <span class="n">are</span><span class="p">:</span> <span class="nc">Index</span><span class="p">([</span><span class="sh">'</span><span class="s">Impact of projects / activities on your success</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Additional Work</span><span class="sh">'</span><span class="p">,</span>
       <span class="sh">'</span><span class="s">Number of Sisters / Brothers</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Mother</span><span class="sh">'</span><span class="n">s</span> <span class="n">Education</span><span class="sh">'</span><span class="s">,
       </span><span class="sh">'</span><span class="n">Reading</span> <span class="nf">frequency </span><span class="p">(</span><span class="n">scientific</span> <span class="n">books</span><span class="o">/</span><span class="n">journals</span><span class="p">)</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="n">Weekly</span> <span class="n">Study</span> <span class="n">Hours</span><span class="sh">'</span><span class="s">,
       </span><span class="sh">'</span><span class="n">Father</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">'</span><span class="p">],</span>
      <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Just looking at these features, a lot of this actually makes a ton of sense. These are engineering students that were surveyed, and that means things like projects and outside activities can provide so much valuable insight and information on the academic development of the participant. The amount of additional work they put outside of classes is vital, and we do see parent’s education does seem to play a role (Mother more than father indicates again the vital role of maternal focus on education in adolescence). The number of sisters and brothers may seem odd, but this is perhaps attributed to having an adequate support system that can aid a student throughout their studies or even having siblings in the same major that provide excellent study aids and peers. Nevertheless, let’s see if focusing on these features can aid our model’s performance. We modify our training and test datasets, and run essentially the exact same code above to get the following:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># SVM
</span>      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4594594594594595</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.29474757776644567</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="c1"># Random Forest
</span>        <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.5675675675675675</span>
        <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.5183982683982684</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># Gradient Boosting
</span>      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4864864864864865</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4370368150855956</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_svc_rev_cm-480.webp 480w,/assets/img/edu_svc_rev_cm-800.webp 800w,/assets/img/edu_svc_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_svc_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_rf_rev_cm-480.webp 480w,/assets/img/edu_rf_rev_cm-800.webp 800w,/assets/img/edu_rf_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_rf_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_gb_rev_cm-480.webp 480w,/assets/img/edu_gb_rev_cm-800.webp 800w,/assets/img/edu_gb_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_gb_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model with re-engineering features, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>We can see for Random Forest, just focusing on those features provided quite a noticeable accuracy bump, while for Gradient Boosting it minimally improved performance and actually reduced performance for the SVM (oops). Regardless, all of our models are still doing quite poor, meaning we need to try one more thing: Hyperparameter Tuning.</p> <h2 id="ivv-hyperparameter-tuning">IV.V: Hyperparameter Tuning</h2> <p>Hyperparameters can be incredibly crucial to the performance of every single ML model, so ensuring that they are properly tuned and have the right values is a must. Luckily, Sci-Kit Learn provides a <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> class that can conduct this entire process for us; all we have to do is define bounds for each hyperparameter and allow Scikit-Learn to take care of the rest. This <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> is better than the <code class="language-plaintext highlighter-rouge">GridSearchCV</code> as it does not search the entirety of hyperspace but randomly queries and tests values for a specified number of iterations, reducing the overall time needed to tune our models. Below is the code for conducting a hyperparameter search for a SVM, but the process is largely the same for Random Forests and Gradient Boosting, just with different hyperparameters and values of course.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hyperparameter Tuning for Support Vector 
</span>
<span class="n">svc_param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">kernel</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">degree</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">scale</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">svc_grid_search</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="nc">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">11</span><span class="p">),</span>
    <span class="n">param_distributions</span><span class="o">=</span><span class="n">svc_param_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">11</span>
<span class="p">)</span>


<span class="n">svc_grid_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span>
<span class="n">svc_rev_hyp</span> <span class="o">=</span> <span class="n">svc_grid_search</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The best estimator parameters were: </span><span class="si">{</span><span class="n">svc_grid_search</span><span class="p">.</span><span class="n">best_params_</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">svc_rev_hyp_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span>
</code></pre></div></div> <p>And finally, we were able to get some solid results with this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># SVM
</span>      <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">kernel</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">degree</span><span class="sh">'</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">}</span>
      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8648648648648649</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8517859965228386</span>  
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="c1"># Random Forest
</span>        <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span> <span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">log_loss</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">class_weight</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">balanced_subsample</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bootstrap</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.972972972972973</span>
        <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.9752661752661753</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># Gradient Boosting
</span>      <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">subsample</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span> <span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">friedman_mse</span><span class="sh">'</span><span class="p">}</span>
      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.43243243243243246</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.37537537537537535</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_svc_rev_hyp_cm-480.webp 480w,/assets/img/edu_svc_rev_hyp_cm-800.webp 800w,/assets/img/edu_svc_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_svc_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_rf_rev_hyp_cm-480.webp 480w,/assets/img/edu_rf_rev_hyp_cm-800.webp 800w,/assets/img/edu_rf_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_rf_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_gb_rev_hyp_cm-480.webp 480w,/assets/img/edu_gb_rev_hyp_cm-800.webp 800w,/assets/img/edu_gb_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_gb_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model with re-engineering features and hyperparameter tuning, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>Now, let’s address the elephant in the room before proceeding: yes, I still can’t figure out the Gradient Boosting Trees. Somehow the accuracy got worst throughout the refinement process, and I chalk that up to my own ignorance and novice to the concept, not the model itself (I’ve heard wonderful things about that). However, the accuracy jump for SVMs and Random Forest is incredible, with the latter really doing exceptional across all of our metrics, with only misclassifying one student that got a D as a B- (we just have a nice, overly optimistic model). I knew hypermeters were important, but I’d be lying if I told you I thought the performance would increase this much. We have actually have something that is usable for a potential school setting!</p> <p>Just to summarize, here all of the results of our models and implementations in a nice table (shout out again to <code class="language-plaintext highlighter-rouge">great_tables</code>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_results_table-480.webp 480w,/assets/img/edu_results_table-800.webp 800w,/assets/img/edu_results_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/edu_results_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="v-conclusion-and-impact">V. Conclusion and Impact</h1> <p>Just to remind you, we set out to answer two main questions: <strong>Which features of a student are most correlated with academic success and how can we utilize these features to predict which grades they will achieve in their studies?</strong>. We definitely answered both, with our process highlighting the importance of practical hands-on experience outside the classroom, parental education level, and also the need for emotional factors like a support system that can provide comfort and aid. We also found that by focusing on these features, we can accurately predict student grades with an accuracy upwards of <strong>97%</strong>, albeit also sometimes as low as 43%. As educators begin to try to implement holistic educational curriculums, focusing on factors like these can help identify inequities before they become systemic an ensure that students are receiving the help needed based upon their own personal situation and lifestyle. Even if these models will forever always be inherently flawed and not a comprehensive predictor of academic success, they can provide a good baseline to identify where students are expected to perform and how they either exceed those expectations——indicating effective instruction and collegiate support—or faltered below them—indicating a need for more robust interventions and overhaul of existing systems.</p> <h1 id="vi-references">VI. References</h1> <p><a href="https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/#model-evaluation-metrics">GeeksForGeeks: Comprehensive Guide to Classification Models in Scikit-Learn</a></p> <p><a href="https://www.geeksforgeeks.org/how-to-tune-hyperparameters-in-gradient-boosting-algorithm/#">GeeksForGeeks: How to Tune Hyperparameters in Gradient Boosting Algorithm </a></p> <p>Literally Every Page in the Scikit-Learn Documentation for every model or function I used</p>]]></content><author><name></name></author><category term="dtsc-3162"/><category term="viz"/><category term="pandas"/><category term="linear-regression"/><category term="svm"/><category term="logistic-regression"/><summary type="html"><![CDATA[Trying to answer the old age question for teachers: What are the things that determine high-performing students and how can we see which students are more likely to be academically successful?]]></summary></entry><entry><title type="html">Project One - Cities Rediscovering Themselves: The Aftermath of Local Law 18 in New York City’s Airbnb Market Across the Boroughs</title><link href="https://aesareen.github.io/blog/2025/project1/" rel="alternate" type="text/html" title="Project One - Cities Rediscovering Themselves: The Aftermath of Local Law 18 in New York City’s Airbnb Market Across the Boroughs"/><published>2025-01-30T14:30:00+00:00</published><updated>2025-01-30T14:30:00+00:00</updated><id>https://aesareen.github.io/blog/2025/project1</id><content type="html" xml:base="https://aesareen.github.io/blog/2025/project1/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>One of the defining features of housing throughout the 2010s was the rise of short-term homestay platforms such as Airbnb, Booking.com, and VRBO. Giving people the opportunity to temporarily rent out spaces in their house to travelers has spurred an industry in excess of $15 billion, with that figure only expected to nearly quadruple in the next decade <a href="https://www.businessresearchinsights.com/market-reports/homestay-platform-market-104124#:~:text=The%20global%20Homestay%20Platform%20Market%20size%20was,growing%20at%20a%20CAGR%20of%20about%2018.5%.&amp;text=The%20global%20COVID%2D19%20pandemic%20has%20been%20unprecedented,across%20all%20regions%20compared%20to%20pre%2Dpandemic%20levels">[1]</a>.</p> <p>With so many financial resources at stake, governments have taken proactive measures to maintain the stability of their housing markets and prevent the price gouging frequently associated with an influx of short-term homestays at the expense of viable, long-term housing for residents. No city has perhaps enacted for aggressive measures to achieve these ends than New York City, whose Local Law 18 requires all short-term renters (short-term here defined to any stay 30 days or less) to be registered with the city—prohibiting transactions from renters that do not comply—and any visits that fall below this threshold are required to have the host remain as an occupant alongside the visitor through the duration of their stay. The repercussions of such legislative actions have been profound, and there is already a wealth of research that demonstrates the effects of such laws have fundamentally altered the duration of stay makeup across the city and have funneled money away from individual-run homestay services to hotels run by massive conglomerations (the ethicality of this switch is up to the reader) <a href="https://www.airdna.co/blog/nycs-short-term-rental-crackdown">[2]</a>, <a href="https://skift.com/2024/09/01/banned-in-nyc-airbnb-1-year-later">[3]</a>.</p> <p>However, an in-depth look into a city removed more than a year from these changes has been much less prevalent. The characteristics of New York’s boroughs—The Bronx, Brooklyn, Manhattan, Queens, and Staten Island—and the vastly different socioeconomic, racial, and cultural values that are intrinsic to each open the question on <strong>who are bearing the cost of these changes the most and who remain largely unaffected?</strong> Furthermore, <strong>what is the current homestay market in each of the boroughs</strong>; what similarities tie the industry together and what differences factionalize it? To further clarify, the question that I hope to find out are the geographic spatially of Airbnb listings but also how different types of listings are distributed across the city. These are the questions, among others, that this project seeks to answer.</p> <h3 id="where-did-this-data-even-come-from">Where did this data even come from?</h3> <p>There is an awesome site called <a href="https://insideairbnb.com/">Inside AirBnb</a> that has Airbnb data for a wide variety of cities across the globe; this is where I accessed any data from February 2024 - November 2024. The 2023 dataset is unfortunately limited behind a paid data request, but luckily someone made it available on <a href="https://www.kaggle.com/datasets/godofoutcasts/new-york-city-airbnb-2023-public-data/data">Kaggle</a>, along with the <a href="https://www.kaggle.com/datasets/eddzzh/airbnb-nyccsv">2020 dataset</a> (yay!).</p> <p>The features of this dataset is extensive, and an entire <a href="https://docs.google.com/spreadsheets/d/1b_dvmyhb_kAJhUmv81rAxl4KcXn0Pymz/edit?gid=1967362979#gid=1967362979">data dictionary</a> delves into each attribute. However, both datasets include:</p> <ul> <li>The price of the AirBnb when the data was taken</li> <li>The neighborhood the AirBnb is located in</li> <li>The latitude and longtitude (approximate) of the hosting site</li> <li>The room type that is being offered</li> <li>The minimum and maximum nights a host can rent out</li> <li>The number of days the property is available for throughout the year</li> </ul> <p>The 2024 data has quite a few more features, such as the host acceptance rate, whether they are a superhost (own a variety of properties across the area), and detailed information about the property about the number of beds within the building and bathrooms. Some of this data can be a bit intrusive, such a host profile picture and description, however, I did not use this information within my analysis.</p> <p>If you’d prefer to download this notebook, just press <a href="https://github.com/aesareen/3162-portfolio/blob/main/assets/jupyter/project_1.ipynb">here</a>.</p> <h1 id="step-one-pre-processing-our-data">Step One: Pre-Processing our data!</h1> <p>Before we can do any sort of modeling, we have to load in our dependencies. Just for reference, here all the packages I utilized:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="kn">import</span> <span class="n">plotly.offline</span> <span class="k">as</span> <span class="n">pyo</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">pandas.api.types</span> <span class="kn">import</span> <span class="n">is_numeric_dtype</span>
<span class="kn">from</span> <span class="n">great_tables</span> <span class="kn">import</span> <span class="n">GT</span><span class="p">,</span> <span class="n">md</span><span class="p">,</span> <span class="n">html</span><span class="p">,</span> <span class="n">system_fonts</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">loc</span>
</code></pre></div></div> <p>Now we can load in our dataframes (in case you’re interested, you can find the files <a href="https://github.com/aesareen/3162-portfolio/blob/90a4d4d01069e2fb622d1cf82f0a6ae75bf4d63e/assets/jupyter/datasets/NYC-Airbnb-2023.csv">here for July 2023</a> and <a href="https://github.com/aesareen/3162-portfolio/blob/90a4d4d01069e2fb622d1cf82f0a6ae75bf4d63e/assets/jupyter/datasets/new_york_listings.csv">here for November 2024</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nov_listings</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./datasets/new_york_listings.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">jul_23_listings</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./datasets/NYC-Airbnb-2023.csv</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>We can take a peek at each table:</p> <h3 id="november-2024-data-nov_listings">November 2024 Data: <code class="language-plaintext highlighter-rouge">nov_listings</code></h3> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>id</th> <th>listing_url</th> <th>scrape_id</th> <th>last_scraped</th> <th>source</th> <th>name</th> <th>description</th> <th>neighborhood_overview</th> <th>picture_url</th> <th>host_id</th> <th>...</th> <th>review_scores_communication</th> <th>review_scores_location</th> <th>review_scores_value</th> <th>license</th> <th>instant_bookable</th> <th>calculated_host_listings_count</th> <th>calculated_host_listings_count_entire_homes</th> <th>calculated_host_listings_count_private_rooms</th> <th>calculated_host_listings_count_shared_rooms</th> <th>reviews_per_month</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>2595</td> <td>https://www.airbnb.com/rooms/2595</td> <td>20241104040953</td> <td>2024-11-04</td> <td>city scrape</td> <td>Skylit Midtown Castle Sanctuary</td> <td>Beautiful, spacious skylit studio in the heart...</td> <td>Centrally located in the heart of Manhattan ju...</td> <td>https://a0.muscache.com/pictures/miso/Hosting-...</td> <td>2845</td> <td>...</td> <td>4.8</td> <td>4.81</td> <td>4.40</td> <td>NaN</td> <td>f</td> <td>3</td> <td>3</td> <td>0</td> <td>0</td> <td>0.27</td> </tr> <tr> <th>1</th> <td>6848</td> <td>https://www.airbnb.com/rooms/6848</td> <td>20241104040953</td> <td>2024-11-04</td> <td>city scrape</td> <td>Only 2 stops to Manhattan studio</td> <td>Comfortable studio apartment with super comfor...</td> <td>NaN</td> <td>https://a0.muscache.com/pictures/e4f031a7-f146...</td> <td>15991</td> <td>...</td> <td>4.8</td> <td>4.69</td> <td>4.58</td> <td>NaN</td> <td>f</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>1.04</td> </tr> <tr> <th>2</th> <td>6872</td> <td>https://www.airbnb.com/rooms/6872</td> <td>20241104040953</td> <td>2024-11-04</td> <td>city scrape</td> <td>Uptown Sanctuary w/ Private Bath (Month to Month)</td> <td>This charming distancing-friendly month-to-mon...</td> <td>This sweet Harlem sanctuary is a 10-20 minute ...</td> <td>https://a0.muscache.com/pictures/miso/Hosting-...</td> <td>16104</td> <td>...</td> <td>5.0</td> <td>5.00</td> <td>5.00</td> <td>NaN</td> <td>f</td> <td>2</td> <td>0</td> <td>2</td> <td>0</td> <td>0.03</td> </tr> </tbody> </table> <p>3 rows × 75 columns</p> </div> <h3 id="july-2023-data-jul_23_listings">July 2023 Data: <code class="language-plaintext highlighter-rouge">jul_23_listings</code></h3> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>id</th> <th>name</th> <th>host_id</th> <th>host_name</th> <th>neighbourhood_group</th> <th>neighbourhood</th> <th>latitude</th> <th>longitude</th> <th>room_type</th> <th>price</th> <th>minimum_nights</th> <th>number_of_reviews</th> <th>last_review</th> <th>reviews_per_month</th> <th>calculated_host_listings_count</th> <th>availability_365</th> <th>number_of_reviews_ltm</th> <th>license</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>2595</td> <td>Skylit Midtown Castle</td> <td>2845</td> <td>Jennifer</td> <td>Manhattan</td> <td>Midtown</td> <td>40.75356</td> <td>-73.98559</td> <td>Entire home/apt</td> <td>150</td> <td>30</td> <td>49</td> <td>2022-06-21</td> <td>0.30</td> <td>3</td> <td>314</td> <td>1</td> <td>NaN</td> </tr> <tr> <th>1</th> <td>5121</td> <td>BlissArtsSpace!</td> <td>7356</td> <td>Garon</td> <td>Brooklyn</td> <td>Bedford-Stuyvesant</td> <td>40.68535</td> <td>-73.95512</td> <td>Private room</td> <td>60</td> <td>30</td> <td>50</td> <td>2019-12-02</td> <td>0.30</td> <td>2</td> <td>365</td> <td>0</td> <td>NaN</td> </tr> <tr> <th>2</th> <td>5203</td> <td>Cozy Clean Guest Room - Family Apt</td> <td>7490</td> <td>MaryEllen</td> <td>Manhattan</td> <td>Upper West Side</td> <td>40.80380</td> <td>-73.96751</td> <td>Private room</td> <td>75</td> <td>2</td> <td>118</td> <td>2017-07-21</td> <td>0.72</td> <td>1</td> <td>0</td> <td>0</td> <td>NaN</td> </tr> </tbody> </table> <p>3 rows × 16 columns</p> </div> <p>Now with everything loaded, we can begin pre-processing our data. I start by removing some of the weird whitespace and capitalization that might be present throughout the file, along with removing the dollar sign from the price column (I do this for both datasets, but for brevity, I only show the code of <code class="language-plaintext highlighter-rouge">nov_listings</code>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nov_listings</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vectorize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">lower</span><span class="p">())(</span><span class="n">nov_listings</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">nov_listings</span><span class="p">[</span><span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">nov_listings</span><span class="p">[</span><span class="sh">"</span><span class="s">price</span><span class="sh">"</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">$</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span><span class="sh">''</span><span class="p">)</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></div> <p>Then we can drop the columns we will definitely know we won’t use throughout the visualization and analysis process.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nov_listings</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">picture_url</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">host_url</span><span class="sh">'</span><span class="p">,</span>
             <span class="sh">'</span><span class="s">neighbourhood</span><span class="sh">'</span><span class="p">,</span> <span class="c1">#Not really the neighborhood 
</span>             <span class="sh">'</span><span class="s">host_thumbnail_url</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">host_picture_url</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">host_has_profile_pic</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">host_identity_verified</span><span class="sh">'</span><span class="p">,</span>
             <span class="sh">'</span><span class="s">license</span><span class="sh">'</span><span class="p">,</span>
             <span class="p">],</span>
    <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div> <p>Now we can begin moving some of the missing values if we need it. I start with a basic print statement to just see how bad it really is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"""</span><span class="s">The number of NaN values per column in nov_listings: </span><span class="se">\n</span><span class="s">
</span><span class="si">{</span><span class="n">nov_listings</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="si">:</span><span class="mi">11</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="s">
    </span><span class="sh">"""</span>
    <span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"""</span><span class="s">
    </span><span class="sh">'</span><span class="s">The number of NaN values per column in jul_23_listings: </span><span class="se">\n</span><span class="s">
      </span><span class="si">{</span><span class="n">jul_23_listings</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="si">}</span><span class="s">
    </span><span class="sh">"""</span>
    <span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   The number of NaN values per column in nov_listings: 

    neighborhood_overview    16974
host_about               16224
host_response_time       15001
host_response_rate       15001
host_acceptance_rate     14983
last_review              11560
first_review             11560
host_location             7999
host_neighbourhood        7503
has_availability          5367
description               1044
dtype: int64'

    
    'The number of NaN values per column in jul_23_listings: 

      last_review                       10304
reviews_per_month                 10304
name                                 12
host_name                             5
neighbourhood_group                   0
neighbourhood                         0
id                                    0
host_id                               0
longitude                             0
latitude                              0
room_type                             0
price                                 0
number_of_reviews                     0
minimum_nights                        0
calculated_host_listings_count        0
availability_365                      0
dtype: int64
</code></pre></div></div> <p>As useful as raw values may be, they don’t do a lot in terms of telling us which columns we should target in large datasets, especially those with a large number of rows. So, I created a quick table that shows us the percentage of each column that is missing (and used the <code class="language-plaintext highlighter-rouge">great_tables</code> module to make it look nice because why not?).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/november_2024_nan_table-480.webp 480w,/assets/img/november_2024_nan_table-800.webp 800w,/assets/img/november_2024_nan_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/november_2024_nan_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/july_2023_nan_table-480.webp 480w,/assets/img/july_2023_nan_table-800.webp 800w,/assets/img/july_2023_nan_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/july_2023_nan_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This creates an interesting dilemma; we can definitely drop <code class="language-plaintext highlighter-rouge">calendar_updated</code>, but what about the columns that have a noticeable proportion of their values missing? We can fill them in based upon the median, that is pretty easy, but I wanted to take a different approach given that I am taking a geography-centered point of view for this project: fill them based upon the median for that column within their borough. I think this can create a more accurate view without getting so specific that we are filling them based upon similar values in their neighborhood (which might have only a handful of values).</p> <p>To do that, I created a function that takes in a DataFrame, a column to find the median for, and a column to group the DataFrame by. The function then groups by the specified column, finds the median for that column, and then fills in all the missing values just as we discussed above. I then apply that to every column within all numeric columns that have at least 30% of their values missing.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">missing_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="p">(</span><span class="n">nov_listings</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">nov_listings</span><span class="p">.</span><span class="n">index</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">).</span><span class="nf">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="p">.</span><span class="mi">3</span> <span class="ow">and</span> <span class="nf">is_numeric_dtype</span><span class="p">(</span><span class="n">nov_listings</span><span class="p">[</span><span class="n">name</span><span class="p">])]</span>

<span class="k">def</span> <span class="nf">fill_na_with_group_means</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">group_col</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">neighbourhood_group_cleansed</span><span class="sh">'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s"> Returns a dictionary with the median for the grouped column that can be used to fill NaN values

    Args:
        df (pd.DataFrame): dataframe to utilize
        col (str): column to take the median of 
        group_col (str, optional): column to group by Defaults to </span><span class="sh">'</span><span class="s">neighbourhood_group_cleansed</span><span class="sh">'</span><span class="s">.

    Returns:
        pd.Series: series with the indexes as the grouped_by indexes and the values as the medians of each group for the specified column
    </span><span class="sh">"""</span>
    <span class="c1"># print(df.groupby(group_col)[col].transform('median'))
</span>    <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="n">group_col</span><span class="p">)[</span><span class="n">col</span><span class="p">].</span><span class="nf">transform</span><span class="p">(</span><span class="sh">'</span><span class="s">median</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># Do it for every missing column
</span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">missing_columns</span><span class="p">:</span>
    <span class="n">nov_listings</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="nf">fill_na_with_group_means</span><span class="p">(</span><span class="n">nov_listings</span><span class="p">,</span> <span class="n">col</span><span class="p">)</span>
</code></pre></div></div> <h1 id="step-two-visualizations">Step Two: Visualizations</h1> <p>Much of the code behind the visualizations are quite verbose, so I won’t include them in this post, but I will walk through my thought process for including each one.</p> <p>First, one of the major consequences of Local Law 18 was that it many thought that it significantly decrease the number of Airbnbs across the city, and based upon the visualization below, that certainly looks like the case.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/listings_change-480.webp 480w,/assets/img/listings_change-800.webp 800w,/assets/img/listings_change-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/listings_change.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> While the city as a whole suffered from Airbnb decreases, the Bronx and Queens suffered the biggest causalities while the financially wealthy Manhattan withstood the worst of the legislation. </div> <p>To gain a better sense of how this spread looks through the city, you can explore the interactive maps below, with July on the left and November on the right (generated very easily through <code class="language-plaintext highlighter-rouge">plotly</code>!).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div class="l-page"> <iframe src="/assets/plotly/jul_2023_airbnb_map.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed white;"></iframe> </div> </div> <div class="col-sm mt-3 mt-md-0"> <div class="l-page"> <iframe src="/assets/plotly/nov_2024_airbnb_map.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed white;"></iframe> </div> </div> </div> <p>That’s some pretty cool insight, and helps us answer one of our initial questions, <strong>what is the current homestay market in each borough</strong>. However, the number of boroughs doesn’t simply tell the entire story. How about their average prices? Let’s explore that!</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/avg_price_change-480.webp 480w,/assets/img/avg_price_change-800.webp 800w,/assets/img/avg_price_change-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/avg_price_change.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>That’s really interesting! We would believe that if listings have decreased, then the demand for homestays would have rapidly increased, thus driving up the prices. However, almost each borough experienced drops in their price, outside of the Bronx—which we know from our previous visualization experienced the worse of the listings drop.</p> <p>So that begs the question, why? Amid decreasing supply, why has the price dropped (which goes against the very basic economic principles I know)? Sure, we can maybe cite some external factors, such as a decrease in homestay demand or the shifting of consumers to hotels, but the latter seems unlikely given hotel prices actually skyrocketed following the implementation of the law <a href="https://shorttermrentalz.com/news/airbnb-new-york-city-urge-reversal-local-law-18/">[4]</a>.</p> <p>To be honest, I don’t know for sure, I am just a guy trying to complete his project for a class. But, I can make one last visualization that can maybe help us dissect the root cause behind this rather perplexing phenomenon. I used September 2020 Airbnb data (which I only utilized one column, so there wasn’t much data pre-processing really needed).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sep_2020_host_age-480.webp 480w,/assets/img/sep_2020_host_age-800.webp 800w,/assets/img/sep_2020_host_age-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/sep_2020_host_age.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nov_host_age-480.webp 480w,/assets/img/nov_host_age-800.webp 800w,/assets/img/nov_host_age-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nov_host_age.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So, we can see over the course of 4 years, the ages of each Airbnb in New York City drastically changed. In September 2020, the ages are skewed right, with a notable percentage of the houses being less than 5 years old. However, in 2024, we get a distribution that is much more symmetric (or even slightly left-skewed). So, the age compositions of Airbnbs over this time frame got much older. Why does that matter? Well a massive part of Local Law 18 was trying to prevent superhosts from snatching up much of the housing market and converting them into Airbnbs. We can maybe hypothesize when Local Law 18 was passed, these superhosts realized the commitment to maintain their properties is far too costly, thus leading them to abandon their enterprise. Thus, the options available were limited to those that actually lived in the city, which typically have more modest abodes—explaining the trend we saw in the previous chart!</p> <h1 id="conclusion">Conclusion</h1> <p>Regardless of the consequences we saw Local Law 18 cause across New York City, homestays are here to “stay” (please feel free to laugh); not just in New York but across the world. Thus, learning how these pieces of legislation are influencing one of the world’s largest metropolitan areas and provide an innumerable amount of guidance to countless other urban developments.</p>]]></content><author><name></name></author><category term="dtsc-3162"/><category term="viz"/><category term="pandas"/><category term="plotly"/><category term="pre-processing"/><summary type="html"><![CDATA[The biggest city in the world effectively banned short-term homestays. How did each borough react?]]></summary></entry></feed>