<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://aesareen.github.io/3162-portfolio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aesareen.github.io/3162-portfolio/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-11T07:08:31+00:00</updated><id>https://aesareen.github.io/3162-portfolio/feed.xml</id><title type="html">Arnav Sareen - Portfolio</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Project Two - Educationally Influenced: Predicting the Academic Success Rates of Students in Cyprus</title><link href="https://aesareen.github.io/3162-portfolio/blog/2025/project2/" rel="alternate" type="text/html" title="Project Two - Educationally Influenced: Predicting the Academic Success Rates of Students in Cyprus"/><published>2025-02-24T14:30:00+00:00</published><updated>2025-02-24T14:30:00+00:00</updated><id>https://aesareen.github.io/3162-portfolio/blog/2025/project2</id><content type="html" xml:base="https://aesareen.github.io/3162-portfolio/blog/2025/project2/"><![CDATA[<h1 id="i-introduction">I. Introduction</h1> <p>As someone who always wanted to go into teaching, I have always been fascinated by the diverse makeups that classrooms bring together, there is perhaps not such a more heterogenous space in society that is so commonly available. The magic of teaching for me is that every single student carries their own dreams, aspirations, and motivations—and crucially from a pedagogical perspective, their own background knowledge. It is miraculously up the lecturer at hand to convey knowledge at a carefully-sculpted rate, depth, and breadth that sufficiently engages all students without leaving those behind that are clearly struggling with the ideas or stultifying those that clearly have sufficiently grasped the material and are ready for a greater challenge (or perhaps, most frustratingly for a teacher, students who simply do not care). In an ideal scenario, teachers could analyze the background and study habits of each student, develop a personalized plan that either curbs potential barricades to academic success or encourages characteristics that underpin in, and reap the success of a spirited, confident, and well-educated classroom.</p> <p>Where such a ideality was previously limited to the imaginations of educators, like many things in the modern-day, it can increasingly become a reality with the rise of machine learning and artificial intelligence systems. Ethical considerations at bay (because they certainly are quite a few), the ability to input an entire student’s demographics, personality, and interests into an algorithm and immediately how to best get that student to learn and critically, enjoy that learning, is maybe one of the most altruistic and revolutionary innovations brought upon this digital revolution.</p> <p>Thus, the following project delves into a fundamental question: <strong>Which features of a student are most correlated with academic success and how can we utilize these features to predict which grades they will achieve in their studies?</strong>. Forecasting such an outcome takes more than just analyzing a student’s study hours and class attendance; it demands at through look at their socioeconomic status, their familial circumstances, and their actions before, during, and after class. Only with such a holistic view can we even begin giving justice to this socially-essential inquiry.</p> <h3 id="where-did-this-data-even-come-from">Where did this data even come from?</h3> <p>Courtesy of the amazing <a href="https://www.archive.ics.uci.edu/">UC Irvine Machine Learning Repository</a> is a <a href="https://archive.ics.uci.edu/dataset/856/higher+education+students+performance+evaluation">dataset</a> released in 2023 about Engineering and Educational Science students attending Near East University in Cyprus. The best part of this dataset is that was utilized to support findings in a paper named <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-35249-3.pdf"><em>Student Performance Classification Using Artificial Intelligence Techniques</em></a>, so while doing my analysis, I can compare the results to actual researchers that completed a very similar task (and see if I can beat them, probably not though).</p> <p>There are not an incredible amount of samples in the dataset (only 145), but quite a few features for each student that include:</p> <ul> <li>Parental Education &amp; Other Familial Information</li> <li>Study Habits (amount of hours, amount of scientific &amp; non-scientific literature read, how often a student took notes in class)</li> <li>Attendance to various academic-related events and also classes</li> <li>Preparation before particular exams</li> <li>The student’s academic performance (both in GPA and grade)</li> </ul> <p><strong>If you’d prefer to download this notebook, just press <a href="https://github.com/aesareen/3162-portfolio/blob/main/assets/jupyter/project_2.ipynb">here</a>.</strong></p> <h1 id="ii-pre-processing-and-visualizing-the-data">II. Pre-Processing and Visualizing the Data</h1> <p>We can load in our dataframe like always do to get started:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">edu_df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./datasets/cyprus_education_dataset.csv</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Let’s take a peek at our first ten rows:</p> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>STUDENT ID</th> <th>1</th> <th>2</th> <th>3</th> <th>...</th> <th>29</th> <th>30</th> <th>COURSE ID</th> <th>GRADE</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>STUDENT1</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <th>1</th> <td>STUDENT2</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>2</td> <td>3</td> <td>1</td> <td>1</td> </tr> <tr> <th>2</th> <td>STUDENT3</td> <td>2</td> <td>2</td> <td>2</td> <td>...</td> <td>2</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>3</th> <td>STUDENT4</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>3</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>...</th> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <th>141</th> <td>STUDENT142</td> <td>1</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>5</td> </tr> <tr> <th>142</th> <td>STUDENT143</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>4</td> <td>3</td> <td>9</td> <td>1</td> </tr> <tr> <th>143</th> <td>STUDENT144</td> <td>2</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>4</td> </tr> <tr> <th>144</th> <td>STUDENT145</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>5</td> <td>4</td> <td>9</td> <td>3</td> </tr> </tbody> </table> <p>145 rows × 33 columns (total)</p> </div> <p>We can see that all of the column names and values are numerical, which is super helpful perhaps for a machine learning model but not so helpful for us mere human non-models. So, my first pre-processing step was simply renaming all the columns to the values they actually are so you (and I) can figure out what they are a little bit more.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The columns are just numbers, so I am replacing them with their actual values
</span><span class="n">col_names</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Sex</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">School Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Scholarship Percentage</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Additional Work</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Regular Art/Sports</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Has Partner</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Total Salary</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Transportation Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Accommodation Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">Mother</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Father</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Number of Sisters / Brothers</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Parental Status</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Mother</span><span class="sh">'</span><span class="s">s Occupation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Father</span><span class="sh">'</span><span class="s">s Occupation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Weekly Study Hours</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Reading frequency (non-scientific books/journals)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Reading frequency (scientific books/journals)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Attendance to department seminars / conferences</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Impact of projects / activities on your success</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Attendance to Classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Preparation to midterm exam 1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Preparation to midterm exams 2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Taking notes in classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Listening in classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Discussion improves my interest and success in the course</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Flip-classroom</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Cumulative GPA last semester</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Expected GPA at graduation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Course ID</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Output Grade</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Student ID</span><span class="sh">"</span><span class="p">]</span>
<span class="n">edu_df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">col_names</span>
</code></pre></div></div> <p>Now we have:</p> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>Age</th> <th>Sex</th> <th>School Type</th> <th>Scholarship Percentage</th> <th>...</th> <th>Expected GPA at graduation</th> <th>Course ID</th> <th>Output Grade</th> <th>Student ID</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>STUDENT1</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <th>1</th> <td>STUDENT2</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>2</td> <td>3</td> <td>1</td> <td>1</td> </tr> <tr> <th>2</th> <td>STUDENT3</td> <td>2</td> <td>2</td> <td>2</td> <td>...</td> <td>2</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>3</th> <td>STUDENT4</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>3</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>...</th> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <th>141</th> <td>STUDENT142</td> <td>1</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>5</td> </tr> <tr> <th>142</th> <td>STUDENT143</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>4</td> <td>3</td> <td>9</td> <td>1</td> </tr> <tr> <th>143</th> <td>STUDENT144</td> <td>2</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>4</td> </tr> <tr> <th>144</th> <td>STUDENT145</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>5</td> <td>4</td> <td>9</td> <td>3</td> </tr> </tbody> </table> <p>145 rows × 33 columns (total)</p> </div> <p>Much better! Our values are still a bit vague (what is a 3 Scholarship Percentage), but we can modify each column as necessary for our visualization step. Now we can move on to seeing the first step actually pre-processing our data: verifying if there any null values.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The number of NaN values per column in our dataset: </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">edu_df</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The number of NaN values per column in our dataset: 
 Age                       0
Sex                       0
School Type               0
Scholarship Percentage    0
Additional Work           0
Regular Art/Sports        0
Has Partner               0
Total Salary              0
Transportation Medium     0
Accommodation Type        0
dtype: int64
</code></pre></div></div> <p>Ah! Perfect! Super unrealistic but UCI archive did actually tell us there are no missing values within this dataset, making it super easy to work with. That is likely the result of this being a very small and manually-collected collection process, so many missing values were likely handled long ago (also the fact that the original researchers would deal with them as part of their own ML work before releasing it to the public!).</p> <p>However, we do not have zero pre-processing to do (ah, I wish). There is still quite a bit work we need to do if you want to make sense of all of this data and ensure that we are identifying the most pertinent features and the best model to identify trends within those features. If we are are trying to predict student outcomes, I think a good first step is to see what outcomes we are dealing with; in other words, model the distribution of grades and GPA that this particular dataset contains.</p> <p>However, in order to do that, we must do some interesting conversions as the grading system is currently numerically encoded and those numbers convert to the specific grading system used at NEU University in Cyprus, which must be further converted to ECTS grades and finally US-scale grades. To accomplish this, I used a resource provided by NEU to convert their grading system to ECTS and then utilized the most logical grade from the ECTS system based upon the equivalent US grade # Let’s change the grades so they are a lit bit more interpretable by us <a href="https://muhendislik.neu.edu.tr/wp-content/uploads/sites/146/2022/07/27/PGE_Exam-Regulations-Assessment-and-Grading.pdf?ver=01278343a7a9d5d38fc21f2487da1610">[1]</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grades_mapping</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="sh">'</span><span class="s">C-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="sh">'</span><span class="s">C+</span><span class="sh">'</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span> <span class="sh">'</span><span class="s">B-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">6</span><span class="p">:</span> <span class="sh">'</span><span class="s">B+</span><span class="sh">'</span><span class="p">,</span> <span class="mi">7</span><span class="p">:</span> <span class="sh">'</span><span class="s">A-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="mi">9</span><span class="p">:</span> <span class="sh">'</span><span class="s">A+</span><span class="sh">'</span><span class="p">}</span>

<span class="c1"># Convert the numerical Cyprus grading system grades to American Letter Grades
</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span><span class="n">grades_mapping</span><span class="p">)</span>

<span class="c1"># Make these letter grades ordinal
</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span>
    <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">],</span> 
    <span class="n">categories</span><span class="o">=</span><span class="n">grades_mapping</span><span class="p">.</span><span class="nf">values</span><span class="p">(),</span> 
    <span class="n">ordered</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <p>From there, I used my favorite visualization package, <code class="language-plaintext highlighter-rouge">plotly</code> to make two graphs: One of the grade distribution of all the students within our training dataset and one of the expected GPA distribution of all the students.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/grade_dist_of_students-480.webp 480w,/3162-portfolio/assets/img/grade_dist_of_students-800.webp 800w,/3162-portfolio/assets/img/grade_dist_of_students-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/grade_dist_of_students.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/gpa_dist_of_students-480.webp 480w,/3162-portfolio/assets/img/gpa_dist_of_students-800.webp 800w,/3162-portfolio/assets/img/gpa_dist_of_students-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/gpa_dist_of_students.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, that might look a bit concerning given that most students do not have a very good grade, however, I view it as a potential benefit. It shows that there are a very select amount of students that are high-performing in this given survey and analyzing their patterns, situation, and demographics especially can provide a lot of insight into the background of successful students.</p> <p>Now, we need to get an idea of which features are particularly pertinent to student success. I have a few initial ideas (such as parent education, how often they attend classes, and how often they listen in classes), but I have no idea which factors high-performing students are doing the most. So I created a series of seaborn <em>swarmplots</em> to help me with this; swarmplots are the best pick here over a traditional scatterplot as much of the data we have overlaps with one another and we need to add a bit of jitter to fully see the data (which is especially helpful given we don’t have an overwhelming number of observations here, which swarmplots struggle with). We can apply some more pre-processing before using the swarmplot to make the axis titles make a bit more sense (previously, they were solely numerical values and I have no idea what a 3 in Mother’s Education means), and then make a subplot of each relevant feature against our target—a student’s grade—to visualize which features might be worth really analyzing!</p> <div class="row mt-3"> <div class="col-lg mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/education_feats_swarmplot-480.webp 480w,/3162-portfolio/assets/img/education_feats_swarmplot-800.webp 800w,/3162-portfolio/assets/img/education_feats_swarmplot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/education_feats_swarmplot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="iii-model-selection">III. Model Selection</h1> <p>Given that our data is not very linear at all (we can see that with the massive amount of overlap between different data points), utilizing a model that does well this type of data is key. With that in mind, I identified the following models as likely the candidates to perform well for a classification task:</p> <ul> <li><strong>Support Vector Machine with a non-linear kernel function</strong>: Support Vector Machines are best when we are not working with linearly separable data, as we are here; it is able to project our data using a <em>kernel function</em> and find a hyperplane that separates our classes in feature space. They are not the most interpretable machine learning model out there, but they are powerful and versatile to a wide domain tasks (hopefully including ours!)</li> <li><strong>Random Forest</strong>: Random Forests are an incredibly popular and powerful ensemble model that conglomerates multiple weak decision trees together to create a model that is overall much more accurate and robust. These slew of decision trees mean they can usually provide high accuracy and also provide some awesome insight into the most vital features of our dataset, but unlike their little siblings the Decision Tree, they are not nearly are interpretable or computationally inexpensive, and can also be suspectable to bias to overrepresented classes.</li> <li><strong>Gradient Boosting Machine</strong>: Gradient Boosting is yet another ensemble algorithm like Random Forest, but it takes a step further by building upon previous models sequentially and correcting the errors of each predecessor. It shares many of the same benefits of Random Forests but with the added benefit of really handling weird datasets well, such as data that is missing or needs to be robustly pre-processed before using it. However, it can overfit and also takes quite a bit of resources to hyperparameter tune (I also don’t understand it as well as the others).</li> </ul> <p>With those models identified, let’s go to implementing them!</p> <h1 id="iv-model-implementation">IV. Model Implementation</h1> <h2 id="ivi-splitting-our-data">IV.I: Splitting our Data</h2> <p>Before implementing any models, we have to first divide our dataset into a series of training and test datasets. To do this, we can utilize the nifty <code class="language-plaintext highlighter-rouge">train_test_split</code> function from Scikit-Learn’s <code class="language-plaintext highlighter-rouge">model_selection</code> module. Before I split my data, I also dropped features that I didn’t feel like the model should use for classification (such as gender and age) and those that overlap with our main criteria for student success (output grades), such as expected GPA at graduation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Sex</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">School Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Student ID</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Parental Status</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Has Partner</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Expected GPA at graduation</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cumulative GPA last semester</span><span class="sh">'</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">()</span>
</code></pre></div></div> <p>Also, given the huge skew we saw in the grade distribution during our data visualization, I opted to utilize the stratify parameter with <code class="language-plaintext highlighter-rouge">train_test_split</code> to ensure that classes were equally represented in both our training and testing dataset, hopefully giving our models the best chance to not overfit and make good predictions on the test dataset!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Opting for a slightly bigger test size here given our smaller dataset along with stratifying given the heavy skew in grades that we saw with the previous visualization
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <h2 id="ivii-evaluation-metrics">IV.II: Evaluation Metrics</h2> <p>To see our model performed, we need a way to quantitatively score it. For this, I utilize a series of built-in functions from Scikit-Learn to calculate accuracy, F1-Score, and Confusion Matrices. Accuracy is the default calculation when doing something like this, and is always a helpful metric to have, but in an imbalanced dataset like this, can be highly misleading. If our model does overfit and assigns almost every student a D grade (<em>foreshadowing</em>), then our accuracy might be OK but the few students that do have higher grades will be completely overshadowed! For that reason, I also decided to include a weighted F1-Score to ensure that both Precision and Recall are included within our evaluation, and we aren’t applauding a model that in reality is pretty terrible. Finally, our confusion matrix can provide some great insight into how the model is classifying various how it should be classifying by showing us the predicted class versus the actual class, so if it does make a mistake, we can see which class it is from. Again, many of the metrics are already provided by Scikit-Learn, but I did make wrappers for each so I could just generalize them a bit easier:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Any</span>

<span class="k">def</span> <span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates the accuracy for a given model on a test set

    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model

    Returns:
        float: Model accuracy
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates the f1-score for a given model on a test set

    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model

    Returns:
        float: Model f1-score
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]),</span>  <span class="n">average</span> <span class="o">=</span> <span class="sh">'</span><span class="s">weighted</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GT</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates and visualizes the confusion matrix for a given model on a test set
    
    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model
        
    Returns:
        
    </span><span class="sh">"""</span>
    <span class="c1"># Get predictions and compute confusion matrix
</span>    <span class="n">cm</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="nf">from_estimator</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">cm</span>
</code></pre></div></div> <h2 id="iviii-naive-implementation">IV.III: Naive Implementation</h2> <p>Finally onto making and implementing our models! As any good Machine Learning Engineer would do (right?), after splitting my data and creating my evaluation metrics, I just throw my data to my model with essentially no other tuning or customization. The results, were um, not <em>great</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">svc</span> <span class="o">=</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">kernel</span> <span class="o">=</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="mi">11</span><span class="p">)</span>

        <span class="n">svc</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">svc_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">rf</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">7</span><span class="p">)</span>
        <span class="n">rf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">rf_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">gb</span> <span class="o">=</span> <span class="nc">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">150</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="p">.</span><span class="mi">15</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">)</span>
        <span class="n">gb</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">gb</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">gb_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.32756132756132755</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.32756132756132755</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.42273533204384267</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_svc_cm-480.webp 480w,/3162-portfolio/assets/img/edu_svc_cm-800.webp 800w,/3162-portfolio/assets/img/edu_svc_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_svc_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_rf_cm-480.webp 480w,/3162-portfolio/assets/img/edu_rf_cm-800.webp 800w,/3162-portfolio/assets/img/edu_rf_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_rf_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_gb_cm-480.webp 480w,/3162-portfolio/assets/img/edu_gb_cm-800.webp 800w,/3162-portfolio/assets/img/edu_gb_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_gb_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>Taking a look at the confusion matrices, we can see that all three models do what I feared: categorizing almost every student as having a D grade, regardless of what the features indicated. They need some serious refinement if they want to actually become usable for anything practical. We can start by reducing the number of features that each utilizes.</p> <h2 id="iviv-feature-engineering">IV.IV: Feature Engineering</h2> <p>While I love this dataset and all of the information it provides about students, it’s clear that not all of it is pertinent to accurately predicting student performance and in many cases, might be hurting it. Thus, we need to see which features models are utilizing and focus on those instead of muddying the water with unnecessary noise. Luckily, random forests have a class attribute that can provide us that exact information:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_importances</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">rf</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">most_important_features</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="n">feature_importances</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">head</span><span class="p">(</span><span class="mi">7</span><span class="p">).</span><span class="n">index</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The most important features are: </span><span class="si">{</span><span class="n">most_important_features</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">The</span> <span class="n">most</span> <span class="n">important</span> <span class="n">features</span> <span class="n">are</span><span class="p">:</span> <span class="nc">Index</span><span class="p">([</span><span class="sh">'</span><span class="s">Impact of projects / activities on your success</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Additional Work</span><span class="sh">'</span><span class="p">,</span>
       <span class="sh">'</span><span class="s">Number of Sisters / Brothers</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Mother</span><span class="sh">'</span><span class="n">s</span> <span class="n">Education</span><span class="sh">'</span><span class="s">,
       </span><span class="sh">'</span><span class="n">Reading</span> <span class="nf">frequency </span><span class="p">(</span><span class="n">scientific</span> <span class="n">books</span><span class="o">/</span><span class="n">journals</span><span class="p">)</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="n">Weekly</span> <span class="n">Study</span> <span class="n">Hours</span><span class="sh">'</span><span class="s">,
       </span><span class="sh">'</span><span class="n">Father</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">'</span><span class="p">],</span>
      <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Just looking at these features, a lot of this actually makes a ton of sense. These are engineering students that were surveyed, and that means things like projects and outside activities can provide so much valuable insight and information on the academic development of the participant. The amount of additional work they put outside of classes is vital, and we do see parent’s education does seem to play a role (Mother more than father indicates again the vital role of maternal focus on education in adolescence). The number of sisters and brothers may seem odd, but this is perhaps attributed to having an adequate support system that can aid a student throughout their studies or even having siblings in the same major that provide excellent study aids and peers. Nevertheless, let’s see if focusing on these features can aid our model’s performance. We modify our training and test datasets, and run essentially the exact same code above to get the following:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># SVM
</span>      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4594594594594595</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.29474757776644567</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="c1"># Random Forest
</span>        <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.5675675675675675</span>
        <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.5183982683982684</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># Gradient Boosting
</span>      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4864864864864865</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4370368150855956</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_svc_rev_cm-480.webp 480w,/3162-portfolio/assets/img/edu_svc_rev_cm-800.webp 800w,/3162-portfolio/assets/img/edu_svc_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_svc_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_rf_rev_cm-480.webp 480w,/3162-portfolio/assets/img/edu_rf_rev_cm-800.webp 800w,/3162-portfolio/assets/img/edu_rf_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_rf_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_gb_rev_cm-480.webp 480w,/3162-portfolio/assets/img/edu_gb_rev_cm-800.webp 800w,/3162-portfolio/assets/img/edu_gb_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_gb_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model with re-engineering features, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>We can see for Random Forest, just focusing on those features provided quite a noticeable accuracy bump, while for Gradient Boosting it minimally improved performance and actually reduced performance for the SVM (oops). Regardless, all of our models are still doing quite poor, meaning we need to try one more thing: Hyperparameter Tuning.</p> <h2 id="ivv-hyperparameter-tuning">IV.V: Hyperparameter Tuning</h2> <p>Hyperparameters can be incredibly crucial to the performance of every single ML model, so ensuring that they are properly tuned and have the right values is a must. Luckily, Sci-Kit Learn provides a <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> class that can conduct this entire process for us; all we have to do is define bounds for each hyperparameter and allow Scikit-Learn to take care of the rest. This <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> is better than the <code class="language-plaintext highlighter-rouge">GridSearchCV</code> as it does not search the entirety of hyperspace but randomly queries and tests values for a specified number of iterations, reducing the overall time needed to tune our models. Below is the code for conducting a hyperparameter search for a SVM, but the process is largely the same for Random Forests and Gradient Boosting, just with different hyperparameters and values of course.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hyperparameter Tuning for Support Vector 
</span>
<span class="n">svc_param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">kernel</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">degree</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">scale</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">svc_grid_search</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="nc">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">11</span><span class="p">),</span>
    <span class="n">param_distributions</span><span class="o">=</span><span class="n">svc_param_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">11</span>
<span class="p">)</span>


<span class="n">svc_grid_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span>
<span class="n">svc_rev_hyp</span> <span class="o">=</span> <span class="n">svc_grid_search</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The best estimator parameters were: </span><span class="si">{</span><span class="n">svc_grid_search</span><span class="p">.</span><span class="n">best_params_</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">svc_rev_hyp_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span>
</code></pre></div></div> <p>And finally, we were able to get some solid results with this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># SVM
</span>      <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">kernel</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">degree</span><span class="sh">'</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">}</span>
      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8648648648648649</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8517859965228386</span>  
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="c1"># Random Forest
</span>        <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span> <span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">log_loss</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">class_weight</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">balanced_subsample</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bootstrap</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.972972972972973</span>
        <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.9752661752661753</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># Gradient Boosting
</span>      <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">subsample</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span> <span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">friedman_mse</span><span class="sh">'</span><span class="p">}</span>
      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.43243243243243246</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.37537537537537535</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_svc_rev_hyp_cm-480.webp 480w,/3162-portfolio/assets/img/edu_svc_rev_hyp_cm-800.webp 800w,/3162-portfolio/assets/img/edu_svc_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_svc_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_rf_rev_hyp_cm-480.webp 480w,/3162-portfolio/assets/img/edu_rf_rev_hyp_cm-800.webp 800w,/3162-portfolio/assets/img/edu_rf_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_rf_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_gb_rev_hyp_cm-480.webp 480w,/3162-portfolio/assets/img/edu_gb_rev_hyp_cm-800.webp 800w,/3162-portfolio/assets/img/edu_gb_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_gb_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model with re-engineering features and hyperparameter tuning, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>Now, let’s address the elephant in the room before proceeding: yes, I still can’t figure out the Gradient Boosting Trees. Somehow the accuracy got worst throughout the refinement process, and I chalk that up to my own ignorance and novice to the concept, not the model itself (I’ve heard wonderful things about that). However, the accuracy jump for SVMs and Random Forest is incredible, with the latter really doing exceptional across all of our metrics, with only misclassifying one student that got a D as a B- (we just have a nice, overly optimistic model). I knew hypermeters were important, but I’d be lying if I told you I thought the performance would increase this much. We have actually have something that is usable for a potential school setting!</p> <p>Just to summarize, here all of the results of our models and implementations in a nice table (shout out again to <code class="language-plaintext highlighter-rouge">great_tables</code>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/edu_results_table-480.webp 480w,/3162-portfolio/assets/img/edu_results_table-800.webp 800w,/3162-portfolio/assets/img/edu_results_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/edu_results_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="v-conclusion-and-impact">V. Conclusion and Impact</h1> <p>Just to remind you, we set out to answer two main questions: <strong>Which features of a student are most correlated with academic success and how can we utilize these features to predict which grades they will achieve in their studies?</strong>. We definitely answered both, with our process highlighting the importance of practical hands-on experience outside the classroom, parental education level, and also the need for emotional factors like a support system that can provide comfort and aid. We also found that by focusing on these features, we can accurately predict student grades with an accuracy upwards of <strong>97%</strong>, albeit also sometimes as low as 43%. As educators begin to try to implement holistic educational curriculums, focusing on factors like these can help identify inequities before they become systemic an ensure that students are receiving the help needed based upon their own personal situation and lifestyle. Even if these models will forever always be inherently flawed and not a comprehensive predictor of academic success, they can provide a good baseline to identify where students are expected to perform and how they either exceed those expectations——indicating effective instruction and collegiate support—or faltered below them—indicating a need for more robust interventions and overhaul of existing systems.</p> <h1 id="vi-references">VI. References</h1> <p><a href="https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/#model-evaluation-metrics">GeeksForGeeks: Comprehensive Guide to Classification Models in Scikit-Learn</a></p> <p><a href="https://www.geeksforgeeks.org/how-to-tune-hyperparameters-in-gradient-boosting-algorithm/#">GeeksForGeeks: How to Tune Hyperparameters in Gradient Boosting Algorithm </a></p> <p>Literally Every Page in the Scikit-Learn Documentation for every model or function I used</p>]]></content><author><name></name></author><category term="dtsc-3162"/><category term="viz"/><category term="pandas"/><category term="linear-regression"/><category term="svm"/><category term="logistic-regression"/><summary type="html"><![CDATA[Trying to answer the old age question for teachers: What are the things that determine high-performing students and how can we see which students are more likely to be academically successful?]]></summary></entry><entry><title type="html">Project One - Cities Rediscovering Themselves: The Aftermath of Local Law 18 in New York City’s Airbnb Market Across the Boroughs</title><link href="https://aesareen.github.io/3162-portfolio/blog/2025/project1/" rel="alternate" type="text/html" title="Project One - Cities Rediscovering Themselves: The Aftermath of Local Law 18 in New York City’s Airbnb Market Across the Boroughs"/><published>2025-01-30T14:30:00+00:00</published><updated>2025-01-30T14:30:00+00:00</updated><id>https://aesareen.github.io/3162-portfolio/blog/2025/project1</id><content type="html" xml:base="https://aesareen.github.io/3162-portfolio/blog/2025/project1/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>One of the defining features of housing throughout the 2010s was the rise of short-term homestay platforms such as Airbnb, Booking.com, and VRBO. Giving people the opportunity to temporarily rent out spaces in their house to travelers has spurred an industry in excess of $15 billion, with that figure only expected to nearly quadruple in the next decade <a href="https://www.businessresearchinsights.com/market-reports/homestay-platform-market-104124#:~:text=The%20global%20Homestay%20Platform%20Market%20size%20was,growing%20at%20a%20CAGR%20of%20about%2018.5%.&amp;text=The%20global%20COVID%2D19%20pandemic%20has%20been%20unprecedented,across%20all%20regions%20compared%20to%20pre%2Dpandemic%20levels">[1]</a>.</p> <p>With so many financial resources at stake, governments have taken proactive measures to maintain the stability of their housing markets and prevent the price gouging frequently associated with an influx of short-term homestays at the expense of viable, long-term housing for residents. No city has perhaps enacted for aggressive measures to achieve these ends than New York City, whose Local Law 18 requires all short-term renters (short-term here defined to any stay 30 days or less) to be registered with the city—prohibiting transactions from renters that do not comply—and any visits that fall below this threshold are required to have the host remain as an occupant alongside the visitor through the duration of their stay. The repercussions of such legislative actions have been profound, and there is already a wealth of research that demonstrates the effects of such laws have fundamentally altered the duration of stay makeup across the city and have funneled money away from individual-run homestay services to hotels run by massive conglomerations (the ethicality of this switch is up to the reader) <a href="https://www.airdna.co/blog/nycs-short-term-rental-crackdown">[2]</a>, <a href="https://skift.com/2024/09/01/banned-in-nyc-airbnb-1-year-later">[3]</a>.</p> <p>However, an in-depth look into a city removed more than a year from these changes has been much less prevalent. The characteristics of New York’s boroughs—The Bronx, Brooklyn, Manhattan, Queens, and Staten Island—and the vastly different socioeconomic, racial, and cultural values that are intrinsic to each open the question on <strong>who are bearing the cost of these changes the most and who remain largely unaffected?</strong> Furthermore, <strong>what is the current homestay market in each of the boroughs</strong>; what similarities tie the industry together and what differences factionalize it? To further clarify, the question that I hope to find out are the geographic spatially of Airbnb listings but also how different types of listings are distributed across the city. These are the questions, among others, that this project seeks to answer.</p> <h3 id="where-did-this-data-even-come-from">Where did this data even come from?</h3> <p>There is an awesome site called <a href="https://insideairbnb.com/">Inside AirBnb</a> that has Airbnb data for a wide variety of cities across the globe; this is where I accessed any data from February 2024 - November 2024. The 2023 dataset is unfortunately limited behind a paid data request, but luckily someone made it available on <a href="https://www.kaggle.com/datasets/godofoutcasts/new-york-city-airbnb-2023-public-data/data">Kaggle</a>, along with the <a href="https://www.kaggle.com/datasets/eddzzh/airbnb-nyccsv">2020 dataset</a> (yay!).</p> <p>The features of this dataset is extensive, and an entire <a href="https://docs.google.com/spreadsheets/d/1b_dvmyhb_kAJhUmv81rAxl4KcXn0Pymz/edit?gid=1967362979#gid=1967362979">data dictionary</a> delves into each attribute. However, both datasets include:</p> <ul> <li>The price of the AirBnb when the data was taken</li> <li>The neighborhood the AirBnb is located in</li> <li>The latitude and longtitude (approximate) of the hosting site</li> <li>The room type that is being offered</li> <li>The minimum and maximum nights a host can rent out</li> <li>The number of days the property is available for throughout the year</li> </ul> <p>The 2024 data has quite a few more features, such as the host acceptance rate, whether they are a superhost (own a variety of properties across the area), and detailed information about the property about the number of beds within the building and bathrooms. Some of this data can be a bit intrusive, such a host profile picture and description, however, I did not use this information within my analysis.</p> <p>If you’d prefer to download this notebook, just press <a href="https://github.com/aesareen/3162-portfolio/blob/main/assets/jupyter/project_1.ipynb">here</a>.</p> <h1 id="step-one-pre-processing-our-data">Step One: Pre-Processing our data!</h1> <p>Before we can do any sort of modeling, we have to load in our dependencies. Just for reference, here all the packages I utilized:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="kn">import</span> <span class="n">plotly.offline</span> <span class="k">as</span> <span class="n">pyo</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">pandas.api.types</span> <span class="kn">import</span> <span class="n">is_numeric_dtype</span>
<span class="kn">from</span> <span class="n">great_tables</span> <span class="kn">import</span> <span class="n">GT</span><span class="p">,</span> <span class="n">md</span><span class="p">,</span> <span class="n">html</span><span class="p">,</span> <span class="n">system_fonts</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">loc</span>
</code></pre></div></div> <p>Now we can load in our dataframes (in case you’re interested, you can find the files <a href="https://github.com/aesareen/3162-portfolio/blob/90a4d4d01069e2fb622d1cf82f0a6ae75bf4d63e/assets/jupyter/datasets/NYC-Airbnb-2023.csv">here for July 2023</a> and <a href="https://github.com/aesareen/3162-portfolio/blob/90a4d4d01069e2fb622d1cf82f0a6ae75bf4d63e/assets/jupyter/datasets/new_york_listings.csv">here for November 2024</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nov_listings</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./datasets/new_york_listings.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">jul_23_listings</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./datasets/NYC-Airbnb-2023.csv</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>We can take a peek at each table:</p> <h3 id="november-2024-data-nov_listings">November 2024 Data: <code class="language-plaintext highlighter-rouge">nov_listings</code></h3> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>id</th> <th>listing_url</th> <th>scrape_id</th> <th>last_scraped</th> <th>source</th> <th>name</th> <th>description</th> <th>neighborhood_overview</th> <th>picture_url</th> <th>host_id</th> <th>...</th> <th>review_scores_communication</th> <th>review_scores_location</th> <th>review_scores_value</th> <th>license</th> <th>instant_bookable</th> <th>calculated_host_listings_count</th> <th>calculated_host_listings_count_entire_homes</th> <th>calculated_host_listings_count_private_rooms</th> <th>calculated_host_listings_count_shared_rooms</th> <th>reviews_per_month</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>2595</td> <td>https://www.airbnb.com/rooms/2595</td> <td>20241104040953</td> <td>2024-11-04</td> <td>city scrape</td> <td>Skylit Midtown Castle Sanctuary</td> <td>Beautiful, spacious skylit studio in the heart...</td> <td>Centrally located in the heart of Manhattan ju...</td> <td>https://a0.muscache.com/pictures/miso/Hosting-...</td> <td>2845</td> <td>...</td> <td>4.8</td> <td>4.81</td> <td>4.40</td> <td>NaN</td> <td>f</td> <td>3</td> <td>3</td> <td>0</td> <td>0</td> <td>0.27</td> </tr> <tr> <th>1</th> <td>6848</td> <td>https://www.airbnb.com/rooms/6848</td> <td>20241104040953</td> <td>2024-11-04</td> <td>city scrape</td> <td>Only 2 stops to Manhattan studio</td> <td>Comfortable studio apartment with super comfor...</td> <td>NaN</td> <td>https://a0.muscache.com/pictures/e4f031a7-f146...</td> <td>15991</td> <td>...</td> <td>4.8</td> <td>4.69</td> <td>4.58</td> <td>NaN</td> <td>f</td> <td>1</td> <td>1</td> <td>0</td> <td>0</td> <td>1.04</td> </tr> <tr> <th>2</th> <td>6872</td> <td>https://www.airbnb.com/rooms/6872</td> <td>20241104040953</td> <td>2024-11-04</td> <td>city scrape</td> <td>Uptown Sanctuary w/ Private Bath (Month to Month)</td> <td>This charming distancing-friendly month-to-mon...</td> <td>This sweet Harlem sanctuary is a 10-20 minute ...</td> <td>https://a0.muscache.com/pictures/miso/Hosting-...</td> <td>16104</td> <td>...</td> <td>5.0</td> <td>5.00</td> <td>5.00</td> <td>NaN</td> <td>f</td> <td>2</td> <td>0</td> <td>2</td> <td>0</td> <td>0.03</td> </tr> </tbody> </table> <p>3 rows × 75 columns</p> </div> <h3 id="july-2023-data-jul_23_listings">July 2023 Data: <code class="language-plaintext highlighter-rouge">jul_23_listings</code></h3> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>id</th> <th>name</th> <th>host_id</th> <th>host_name</th> <th>neighbourhood_group</th> <th>neighbourhood</th> <th>latitude</th> <th>longitude</th> <th>room_type</th> <th>price</th> <th>minimum_nights</th> <th>number_of_reviews</th> <th>last_review</th> <th>reviews_per_month</th> <th>calculated_host_listings_count</th> <th>availability_365</th> <th>number_of_reviews_ltm</th> <th>license</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>2595</td> <td>Skylit Midtown Castle</td> <td>2845</td> <td>Jennifer</td> <td>Manhattan</td> <td>Midtown</td> <td>40.75356</td> <td>-73.98559</td> <td>Entire home/apt</td> <td>150</td> <td>30</td> <td>49</td> <td>2022-06-21</td> <td>0.30</td> <td>3</td> <td>314</td> <td>1</td> <td>NaN</td> </tr> <tr> <th>1</th> <td>5121</td> <td>BlissArtsSpace!</td> <td>7356</td> <td>Garon</td> <td>Brooklyn</td> <td>Bedford-Stuyvesant</td> <td>40.68535</td> <td>-73.95512</td> <td>Private room</td> <td>60</td> <td>30</td> <td>50</td> <td>2019-12-02</td> <td>0.30</td> <td>2</td> <td>365</td> <td>0</td> <td>NaN</td> </tr> <tr> <th>2</th> <td>5203</td> <td>Cozy Clean Guest Room - Family Apt</td> <td>7490</td> <td>MaryEllen</td> <td>Manhattan</td> <td>Upper West Side</td> <td>40.80380</td> <td>-73.96751</td> <td>Private room</td> <td>75</td> <td>2</td> <td>118</td> <td>2017-07-21</td> <td>0.72</td> <td>1</td> <td>0</td> <td>0</td> <td>NaN</td> </tr> </tbody> </table> <p>3 rows × 16 columns</p> </div> <p>Now with everything loaded, we can begin pre-processing our data. I start by removing some of the weird whitespace and capitalization that might be present throughout the file, along with removing the dollar sign from the price column (I do this for both datasets, but for brevity, I only show the code of <code class="language-plaintext highlighter-rouge">nov_listings</code>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nov_listings</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vectorize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">lower</span><span class="p">())(</span><span class="n">nov_listings</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">nov_listings</span><span class="p">[</span><span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">nov_listings</span><span class="p">[</span><span class="sh">"</span><span class="s">price</span><span class="sh">"</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">$</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span><span class="sh">''</span><span class="p">)</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></div> <p>Then we can drop the columns we will definitely know we won’t use throughout the visualization and analysis process.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nov_listings</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">picture_url</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">host_url</span><span class="sh">'</span><span class="p">,</span>
             <span class="sh">'</span><span class="s">neighbourhood</span><span class="sh">'</span><span class="p">,</span> <span class="c1">#Not really the neighborhood 
</span>             <span class="sh">'</span><span class="s">host_thumbnail_url</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">host_picture_url</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">host_has_profile_pic</span><span class="sh">'</span><span class="p">,</span> 
             <span class="sh">'</span><span class="s">host_identity_verified</span><span class="sh">'</span><span class="p">,</span>
             <span class="sh">'</span><span class="s">license</span><span class="sh">'</span><span class="p">,</span>
             <span class="p">],</span>
    <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div> <p>Now we can begin moving some of the missing values if we need it. I start with a basic print statement to just see how bad it really is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"""</span><span class="s">The number of NaN values per column in nov_listings: </span><span class="se">\n</span><span class="s">
</span><span class="si">{</span><span class="n">nov_listings</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="si">:</span><span class="mi">11</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="s">
    </span><span class="sh">"""</span>
    <span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"""</span><span class="s">
    </span><span class="sh">'</span><span class="s">The number of NaN values per column in jul_23_listings: </span><span class="se">\n</span><span class="s">
      </span><span class="si">{</span><span class="n">jul_23_listings</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="si">}</span><span class="s">
    </span><span class="sh">"""</span>
    <span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   The number of NaN values per column in nov_listings: 

    neighborhood_overview    16974
host_about               16224
host_response_time       15001
host_response_rate       15001
host_acceptance_rate     14983
last_review              11560
first_review             11560
host_location             7999
host_neighbourhood        7503
has_availability          5367
description               1044
dtype: int64'

    
    'The number of NaN values per column in jul_23_listings: 

      last_review                       10304
reviews_per_month                 10304
name                                 12
host_name                             5
neighbourhood_group                   0
neighbourhood                         0
id                                    0
host_id                               0
longitude                             0
latitude                              0
room_type                             0
price                                 0
number_of_reviews                     0
minimum_nights                        0
calculated_host_listings_count        0
availability_365                      0
dtype: int64
</code></pre></div></div> <p>As useful as raw values may be, they don’t do a lot in terms of telling us which columns we should target in large datasets, especially those with a large number of rows. So, I created a quick table that shows us the percentage of each column that is missing (and used the <code class="language-plaintext highlighter-rouge">great_tables</code> module to make it look nice because why not?).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/november_2024_nan_table-480.webp 480w,/3162-portfolio/assets/img/november_2024_nan_table-800.webp 800w,/3162-portfolio/assets/img/november_2024_nan_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/november_2024_nan_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/july_2023_nan_table-480.webp 480w,/3162-portfolio/assets/img/july_2023_nan_table-800.webp 800w,/3162-portfolio/assets/img/july_2023_nan_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/july_2023_nan_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This creates an interesting dilemma; we can definitely drop <code class="language-plaintext highlighter-rouge">calendar_updated</code>, but what about the columns that have a noticeable proportion of their values missing? We can fill them in based upon the median, that is pretty easy, but I wanted to take a different approach given that I am taking a geography-centered point of view for this project: fill them based upon the median for that column within their borough. I think this can create a more accurate view without getting so specific that we are filling them based upon similar values in their neighborhood (which might have only a handful of values).</p> <p>To do that, I created a function that takes in a DataFrame, a column to find the median for, and a column to group the DataFrame by. The function then groups by the specified column, finds the median for that column, and then fills in all the missing values just as we discussed above. I then apply that to every column within all numeric columns that have at least 30% of their values missing.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">missing_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="p">(</span><span class="n">nov_listings</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">nov_listings</span><span class="p">.</span><span class="n">index</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">).</span><span class="nf">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="p">.</span><span class="mi">3</span> <span class="ow">and</span> <span class="nf">is_numeric_dtype</span><span class="p">(</span><span class="n">nov_listings</span><span class="p">[</span><span class="n">name</span><span class="p">])]</span>

<span class="k">def</span> <span class="nf">fill_na_with_group_means</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">group_col</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">neighbourhood_group_cleansed</span><span class="sh">'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s"> Returns a dictionary with the median for the grouped column that can be used to fill NaN values

    Args:
        df (pd.DataFrame): dataframe to utilize
        col (str): column to take the median of 
        group_col (str, optional): column to group by Defaults to </span><span class="sh">'</span><span class="s">neighbourhood_group_cleansed</span><span class="sh">'</span><span class="s">.

    Returns:
        pd.Series: series with the indexes as the grouped_by indexes and the values as the medians of each group for the specified column
    </span><span class="sh">"""</span>
    <span class="c1"># print(df.groupby(group_col)[col].transform('median'))
</span>    <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="n">group_col</span><span class="p">)[</span><span class="n">col</span><span class="p">].</span><span class="nf">transform</span><span class="p">(</span><span class="sh">'</span><span class="s">median</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># Do it for every missing column
</span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">missing_columns</span><span class="p">:</span>
    <span class="n">nov_listings</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="nf">fill_na_with_group_means</span><span class="p">(</span><span class="n">nov_listings</span><span class="p">,</span> <span class="n">col</span><span class="p">)</span>
</code></pre></div></div> <h1 id="step-two-visualizations">Step Two: Visualizations</h1> <p>Much of the code behind the visualizations are quite verbose, so I won’t include them in this post, but I will walk through my thought process for including each one.</p> <p>First, one of the major consequences of Local Law 18 was that it many thought that it significantly decrease the number of Airbnbs across the city, and based upon the visualization below, that certainly looks like the case.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/listings_change-480.webp 480w,/3162-portfolio/assets/img/listings_change-800.webp 800w,/3162-portfolio/assets/img/listings_change-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/listings_change.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> While the city as a whole suffered from Airbnb decreases, the Bronx and Queens suffered the biggest causalities while the financially wealthy Manhattan withstood the worst of the legislation. </div> <p>To gain a better sense of how this spread looks through the city, you can explore the interactive maps below, with July on the left and November on the right (generated very easily through <code class="language-plaintext highlighter-rouge">plotly</code>!).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div class="l-page"> <iframe src="/3162-portfolio/assets/plotly/jul_2023_airbnb_map.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed white;"></iframe> </div> </div> <div class="col-sm mt-3 mt-md-0"> <div class="l-page"> <iframe src="/3162-portfolio/assets/plotly/nov_2024_airbnb_map.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed white;"></iframe> </div> </div> </div> <p>That’s some pretty cool insight, and helps us answer one of our initial questions, <strong>what is the current homestay market in each borough</strong>. However, the number of boroughs doesn’t simply tell the entire story. How about their average prices? Let’s explore that!</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/avg_price_change-480.webp 480w,/3162-portfolio/assets/img/avg_price_change-800.webp 800w,/3162-portfolio/assets/img/avg_price_change-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/avg_price_change.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>That’s really interesting! We would believe that if listings have decreased, then the demand for homestays would have rapidly increased, thus driving up the prices. However, almost each borough experienced drops in their price, outside of the Bronx—which we know from our previous visualization experienced the worse of the listings drop.</p> <p>So that begs the question, why? Amid decreasing supply, why has the price dropped (which goes against the very basic economic principles I know)? Sure, we can maybe cite some external factors, such as a decrease in homestay demand or the shifting of consumers to hotels, but the latter seems unlikely given hotel prices actually skyrocketed following the implementation of the law <a href="https://shorttermrentalz.com/news/airbnb-new-york-city-urge-reversal-local-law-18/">[4]</a>.</p> <p>To be honest, I don’t know for sure, I am just a guy trying to complete his project for a class. But, I can make one last visualization that can maybe help us dissect the root cause behind this rather perplexing phenomenon. I used September 2020 Airbnb data (which I only utilized one column, so there wasn’t much data pre-processing really needed).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/sep_2020_host_age-480.webp 480w,/3162-portfolio/assets/img/sep_2020_host_age-800.webp 800w,/3162-portfolio/assets/img/sep_2020_host_age-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/sep_2020_host_age.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/nov_host_age-480.webp 480w,/3162-portfolio/assets/img/nov_host_age-800.webp 800w,/3162-portfolio/assets/img/nov_host_age-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/3162-portfolio/assets/img/nov_host_age.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So, we can see over the course of 4 years, the ages of each Airbnb in New York City drastically changed. In September 2020, the ages are skewed right, with a notable percentage of the houses being less than 5 years old. However, in 2024, we get a distribution that is much more symmetric (or even slightly left-skewed). So, the age compositions of Airbnbs over this time frame got much older. Why does that matter? Well a massive part of Local Law 18 was trying to prevent superhosts from snatching up much of the housing market and converting them into Airbnbs. We can maybe hypothesize when Local Law 18 was passed, these superhosts realized the commitment to maintain their properties is far too costly, thus leading them to abandon their enterprise. Thus, the options available were limited to those that actually lived in the city, which typically have more modest abodes—explaining the trend we saw in the previous chart!</p> <h1 id="conclusion">Conclusion</h1> <p>Regardless of the consequences we saw Local Law 18 cause across New York City, homestays are here to “stay” (please feel free to laugh); not just in New York but across the world. Thus, learning how these pieces of legislation are influencing one of the world’s largest metropolitan areas and provide an innumerable amount of guidance to countless other urban developments.</p>]]></content><author><name></name></author><category term="dtsc-3162"/><category term="viz"/><category term="pandas"/><category term="plotly"/><category term="pre-processing"/><summary type="html"><![CDATA[The biggest city in the world effectively banned short-term homestays. How did each borough react?]]></summary></entry></feed>