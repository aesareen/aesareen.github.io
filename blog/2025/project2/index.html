<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Project Two - Educationally Influenced: Predicting the Academic Success Rates of Students in Cyprus | Arnav Sareen - Portfolio </title> <meta name="author" content="Arnav Sareen"> <meta name="description" content="Trying to answer the old age question for teachers: What are the things that determine high-performing students and how can we see which students are more likely to be academically successful?"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aesareen.github.io/blog/2025/project2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Arnav Sareen - Portfolio </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Project Two - Educationally Influenced: Predicting the Academic Success Rates of Students in Cyprus</h1> <p class="post-meta"> Created in February 24, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/viz"> <i class="fa-solid fa-hashtag fa-sm"></i> viz</a>   <a href="/blog/tag/pandas"> <i class="fa-solid fa-hashtag fa-sm"></i> pandas</a>   <a href="/blog/tag/linear-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> linear-regression</a>   <a href="/blog/tag/svm"> <i class="fa-solid fa-hashtag fa-sm"></i> svm</a>   <a href="/blog/tag/logistic-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> logistic-regression</a>   ·   <a href="/blog/category/dtsc-3162"> <i class="fa-solid fa-tag fa-sm"></i> dtsc-3162</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"> <a href="#i-introduction">I. Introduction</a> <ul> <li class="toc-entry toc-h3"><a href="#where-did-this-data-even-come-from">Where did this data even come from?</a></li> </ul> </li> <li class="toc-entry toc-h1"><a href="#ii-pre-processing-and-visualizing-the-data">II. Pre-Processing and Visualizing the Data</a></li> <li class="toc-entry toc-h1"><a href="#iii-model-selection">III. Model Selection</a></li> <li class="toc-entry toc-h1"> <a href="#iv-model-implementation">IV. Model Implementation</a> <ul> <li class="toc-entry toc-h2"><a href="#ivi-splitting-our-data">IV.I: Splitting our Data</a></li> <li class="toc-entry toc-h2"><a href="#ivii-evaluation-metrics">IV.II: Evaluation Metrics</a></li> <li class="toc-entry toc-h2"><a href="#iviii-naive-implementation">IV.III: Naive Implementation</a></li> <li class="toc-entry toc-h2"><a href="#iviv-feature-engineering">IV.IV: Feature Engineering</a></li> <li class="toc-entry toc-h2"><a href="#ivv-hyperparameter-tuning">IV.V: Hyperparameter Tuning</a></li> </ul> </li> <li class="toc-entry toc-h1"><a href="#v-conclusion-and-impact">V. Conclusion and Impact</a></li> <li class="toc-entry toc-h1"><a href="#vi-references">VI. References</a></li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="i-introduction">I. Introduction</h1> <p>As someone who always wanted to go into teaching, I have always been fascinated by the diverse makeups that classrooms bring together, there is perhaps not such a more heterogenous space in society that is so commonly available. The magic of teaching for me is that every single student carries their own dreams, aspirations, and motivations—and crucially from a pedagogical perspective, their own background knowledge. It is miraculously up the lecturer at hand to convey knowledge at a carefully-sculpted rate, depth, and breadth that sufficiently engages all students without leaving those behind that are clearly struggling with the ideas or stultifying those that clearly have sufficiently grasped the material and are ready for a greater challenge (or perhaps, most frustratingly for a teacher, students who simply do not care). In an ideal scenario, teachers could analyze the background and study habits of each student, develop a personalized plan that either curbs potential barricades to academic success or encourages characteristics that underpin in, and reap the success of a spirited, confident, and well-educated classroom.</p> <p>Where such a ideality was previously limited to the imaginations of educators, like many things in the modern-day, it can increasingly become a reality with the rise of machine learning and artificial intelligence systems. Ethical considerations at bay (because they certainly are quite a few), the ability to input an entire student’s demographics, personality, and interests into an algorithm and immediately how to best get that student to learn and critically, enjoy that learning, is maybe one of the most altruistic and revolutionary innovations brought upon this digital revolution.</p> <p>Thus, the following project delves into a fundamental question: <strong>Which features of a student are most correlated with academic success and how can we utilize these features to predict which grades they will achieve in their studies?</strong>. Forecasting such an outcome takes more than just analyzing a student’s study hours and class attendance; it demands at through look at their socioeconomic status, their familial circumstances, and their actions before, during, and after class. Only with such a holistic view can we even begin giving justice to this socially-essential inquiry.</p> <h3 id="where-did-this-data-even-come-from">Where did this data even come from?</h3> <p>Courtesy of the amazing <a href="https://www.archive.ics.uci.edu/" rel="external nofollow noopener" target="_blank">UC Irvine Machine Learning Repository</a> is a <a href="https://archive.ics.uci.edu/dataset/856/higher+education+students+performance+evaluation" rel="external nofollow noopener" target="_blank">dataset</a> released in 2023 about Engineering and Educational Science students attending Near East University in Cyprus. The best part of this dataset is that was utilized to support findings in a paper named <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-35249-3.pdf" rel="external nofollow noopener" target="_blank"><em>Student Performance Classification Using Artificial Intelligence Techniques</em></a>, so while doing my analysis, I can compare the results to actual researchers that completed a very similar task (and see if I can beat them, probably not though).</p> <p>There are not an incredible amount of samples in the dataset (only 145), but quite a few features for each student that include:</p> <ul> <li>Parental Education &amp; Other Familial Information</li> <li>Study Habits (amount of hours, amount of scientific &amp; non-scientific literature read, how often a student took notes in class)</li> <li>Attendance to various academic-related events and also classes</li> <li>Preparation before particular exams</li> <li>The student’s academic performance (both in GPA and grade)</li> </ul> <p><strong>If you’d prefer to download this notebook, just press <a href="https://github.com/aesareen/3162-portfolio/blob/main/assets/jupyter/project_2.ipynb" rel="external nofollow noopener" target="_blank">here</a>.</strong></p> <h1 id="ii-pre-processing-and-visualizing-the-data">II. Pre-Processing and Visualizing the Data</h1> <p>We can load in our dataframe like always do to get started:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">edu_df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./datasets/cyprus_education_dataset.csv</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Let’s take a peek at our first ten rows:</p> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>STUDENT ID</th> <th>1</th> <th>2</th> <th>3</th> <th>...</th> <th>29</th> <th>30</th> <th>COURSE ID</th> <th>GRADE</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>STUDENT1</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <th>1</th> <td>STUDENT2</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>2</td> <td>3</td> <td>1</td> <td>1</td> </tr> <tr> <th>2</th> <td>STUDENT3</td> <td>2</td> <td>2</td> <td>2</td> <td>...</td> <td>2</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>3</th> <td>STUDENT4</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>3</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>...</th> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <th>141</th> <td>STUDENT142</td> <td>1</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>5</td> </tr> <tr> <th>142</th> <td>STUDENT143</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>4</td> <td>3</td> <td>9</td> <td>1</td> </tr> <tr> <th>143</th> <td>STUDENT144</td> <td>2</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>4</td> </tr> <tr> <th>144</th> <td>STUDENT145</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>5</td> <td>4</td> <td>9</td> <td>3</td> </tr> </tbody> </table> <p>145 rows × 33 columns (total)</p> </div> <p>We can see that all of the column names and values are numerical, which is super helpful perhaps for a machine learning model but not so helpful for us mere human non-models. So, my first pre-processing step was simply renaming all the columns to the values they actually are so you (and I) can figure out what they are a little bit more.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The columns are just numbers, so I am replacing them with their actual values
</span><span class="n">col_names</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Sex</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">School Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Scholarship Percentage</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Additional Work</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Regular Art/Sports</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Has Partner</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Total Salary</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Transportation Medium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Accommodation Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">Mother</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Father</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Number of Sisters / Brothers</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Parental Status</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Mother</span><span class="sh">'</span><span class="s">s Occupation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Father</span><span class="sh">'</span><span class="s">s Occupation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Weekly Study Hours</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Reading frequency (non-scientific books/journals)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Reading frequency (scientific books/journals)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Attendance to department seminars / conferences</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Impact of projects / activities on your success</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Attendance to Classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Preparation to midterm exam 1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Preparation to midterm exams 2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Taking notes in classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Listening in classes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Discussion improves my interest and success in the course</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Flip-classroom</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Cumulative GPA last semester</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Expected GPA at graduation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Course ID</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Output Grade</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Student ID</span><span class="sh">"</span><span class="p">]</span>
<span class="n">edu_df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">col_names</span>
</code></pre></div></div> <p>Now we have:</p> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>Age</th> <th>Sex</th> <th>School Type</th> <th>Scholarship Percentage</th> <th>...</th> <th>Expected GPA at graduation</th> <th>Course ID</th> <th>Output Grade</th> <th>Student ID</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>STUDENT1</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <th>1</th> <td>STUDENT2</td> <td>2</td> <td>2</td> <td>3</td> <td>...</td> <td>2</td> <td>3</td> <td>1</td> <td>1</td> </tr> <tr> <th>2</th> <td>STUDENT3</td> <td>2</td> <td>2</td> <td>2</td> <td>...</td> <td>2</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>3</th> <td>STUDENT4</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>3</td> <td>2</td> <td>1</td> <td>1</td> </tr> <tr> <th>...</th> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <th>141</th> <td>STUDENT142</td> <td>1</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>5</td> </tr> <tr> <th>142</th> <td>STUDENT143</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>4</td> <td>3</td> <td>9</td> <td>1</td> </tr> <tr> <th>143</th> <td>STUDENT144</td> <td>2</td> <td>1</td> <td>2</td> <td>...</td> <td>5</td> <td>3</td> <td>9</td> <td>4</td> </tr> <tr> <th>144</th> <td>STUDENT145</td> <td>1</td> <td>1</td> <td>1</td> <td>...</td> <td>5</td> <td>4</td> <td>9</td> <td>3</td> </tr> </tbody> </table> <p>145 rows × 33 columns (total)</p> </div> <p>Much better! Our values are still a bit vague (what is a 3 Scholarship Percentage), but we can modify each column as necessary for our visualization step. Now we can move on to seeing the first step actually pre-processing our data: verifying if there any null values.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The number of NaN values per column in our dataset: </span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="n">edu_df</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The number of NaN values per column in our dataset: 
 Age                       0
Sex                       0
School Type               0
Scholarship Percentage    0
Additional Work           0
Regular Art/Sports        0
Has Partner               0
Total Salary              0
Transportation Medium     0
Accommodation Type        0
dtype: int64
</code></pre></div></div> <p>Ah! Perfect! Super unrealistic but UCI archive did actually tell us there are no missing values within this dataset, making it super easy to work with. That is likely the result of this being a very small and manually-collected collection process, so many missing values were likely handled long ago (also the fact that the original researchers would deal with them as part of their own ML work before releasing it to the public!).</p> <p>However, we do not have zero pre-processing to do (ah, I wish). There is still quite a bit work we need to do if you want to make sense of all of this data and ensure that we are identifying the most pertinent features and the best model to identify trends within those features. If we are are trying to predict student outcomes, I think a good first step is to see what outcomes we are dealing with; in other words, model the distribution of grades and GPA that this particular dataset contains.</p> <p>However, in order to do that, we must do some interesting conversions as the grading system is currently numerically encoded and those numbers convert to the specific grading system used at NEU University in Cyprus, which must be further converted to ECTS grades and finally US-scale grades. To accomplish this, I used a resource provided by NEU to convert their grading system to ECTS and then utilized the most logical grade from the ECTS system based upon the equivalent US grade # Let’s change the grades so they are a lit bit more interpretable by us <a href="https://muhendislik.neu.edu.tr/wp-content/uploads/sites/146/2022/07/27/PGE_Exam-Regulations-Assessment-and-Grading.pdf?ver=01278343a7a9d5d38fc21f2487da1610" rel="external nofollow noopener" target="_blank">[1]</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grades_mapping</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="sh">'</span><span class="s">C-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="sh">'</span><span class="s">C+</span><span class="sh">'</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span> <span class="sh">'</span><span class="s">B-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">6</span><span class="p">:</span> <span class="sh">'</span><span class="s">B+</span><span class="sh">'</span><span class="p">,</span> <span class="mi">7</span><span class="p">:</span> <span class="sh">'</span><span class="s">A-</span><span class="sh">'</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="mi">9</span><span class="p">:</span> <span class="sh">'</span><span class="s">A+</span><span class="sh">'</span><span class="p">}</span>

<span class="c1"># Convert the numerical Cyprus grading system grades to American Letter Grades
</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span><span class="n">grades_mapping</span><span class="p">)</span>

<span class="c1"># Make these letter grades ordinal
</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span>
    <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">],</span> 
    <span class="n">categories</span><span class="o">=</span><span class="n">grades_mapping</span><span class="p">.</span><span class="nf">values</span><span class="p">(),</span> 
    <span class="n">ordered</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <p>From there, I used my favorite visualization package, <code class="language-plaintext highlighter-rouge">plotly</code> to make two graphs: One of the grade distribution of all the students within our training dataset and one of the expected GPA distribution of all the students.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grade_dist_of_students-480.webp 480w,/assets/img/grade_dist_of_students-800.webp 800w,/assets/img/grade_dist_of_students-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/grade_dist_of_students.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpa_dist_of_students-480.webp 480w,/assets/img/gpa_dist_of_students-800.webp 800w,/assets/img/gpa_dist_of_students-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/gpa_dist_of_students.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Now, that might look a bit concerning given that most students do not have a very good grade, however, I view it as a potential benefit. It shows that there are a very select amount of students that are high-performing in this given survey and analyzing their patterns, situation, and demographics especially can provide a lot of insight into the background of successful students.</p> <p>Now, we need to get an idea of which features are particularly pertinent to student success. I have a few initial ideas (such as parent education, how often they attend classes, and how often they listen in classes), but I have no idea which factors high-performing students are doing the most. So I created a series of seaborn <em>swarmplots</em> to help me with this; swarmplots are the best pick here over a traditional scatterplot as much of the data we have overlaps with one another and we need to add a bit of jitter to fully see the data (which is especially helpful given we don’t have an overwhelming number of observations here, which swarmplots struggle with). We can apply some more pre-processing before using the swarmplot to make the axis titles make a bit more sense (previously, they were solely numerical values and I have no idea what a 3 in Mother’s Education means), and then make a subplot of each relevant feature against our target—a student’s grade—to visualize which features might be worth really analyzing!</p> <div class="row mt-3"> <div class="col-lg mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/education_feats_swarmplot-480.webp 480w,/assets/img/education_feats_swarmplot-800.webp 800w,/assets/img/education_feats_swarmplot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/education_feats_swarmplot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="iii-model-selection">III. Model Selection</h1> <p>Given that our data is not very linear at all (we can see that with the massive amount of overlap between different data points), utilizing a model that does well this type of data is key. With that in mind, I identified the following models as likely the candidates to perform well for a classification task:</p> <ul> <li> <strong>Support Vector Machine with a non-linear kernel function</strong>: Support Vector Machines are best when we are not working with linearly separable data, as we are here; it is able to project our data using a <em>kernel function</em> and find a hyperplane that separates our classes in feature space. They are not the most interpretable machine learning model out there, but they are powerful and versatile to a wide domain tasks (hopefully including ours!)</li> <li> <strong>Random Forest</strong>: Random Forests are an incredibly popular and powerful ensemble model that conglomerates multiple weak decision trees together to create a model that is overall much more accurate and robust. These slew of decision trees mean they can usually provide high accuracy and also provide some awesome insight into the most vital features of our dataset, but unlike their little siblings the Decision Tree, they are not nearly are interpretable or computationally inexpensive, and can also be suspectable to bias to overrepresented classes.</li> <li> <strong>Gradient Boosting Machine</strong>: Gradient Boosting is yet another ensemble algorithm like Random Forest, but it takes a step further by building upon previous models sequentially and correcting the errors of each predecessor. It shares many of the same benefits of Random Forests but with the added benefit of really handling weird datasets well, such as data that is missing or needs to be robustly pre-processed before using it. However, it can overfit and also takes quite a bit of resources to hyperparameter tune (I also don’t understand it as well as the others).</li> </ul> <p>With those models identified, let’s go to implementing them!</p> <h1 id="iv-model-implementation">IV. Model Implementation</h1> <h2 id="ivi-splitting-our-data">IV.I: Splitting our Data</h2> <p>Before implementing any models, we have to first divide our dataset into a series of training and test datasets. To do this, we can utilize the nifty <code class="language-plaintext highlighter-rouge">train_test_split</code> function from Scikit-Learn’s <code class="language-plaintext highlighter-rouge">model_selection</code> module. Before I split my data, I also dropped features that I didn’t feel like the model should use for classification (such as gender and age) and those that overlap with our main criteria for student success (output grades), such as expected GPA at graduation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Sex</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">School Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Student ID</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Parental Status</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Has Partner</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Expected GPA at graduation</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cumulative GPA last semester</span><span class="sh">'</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">].</span><span class="nf">to_numpy</span><span class="p">()</span>
</code></pre></div></div> <p>Also, given the huge skew we saw in the grade distribution during our data visualization, I opted to utilize the stratify parameter with <code class="language-plaintext highlighter-rouge">train_test_split</code> to ensure that classes were equally represented in both our training and testing dataset, hopefully giving our models the best chance to not overfit and make good predictions on the test dataset!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Opting for a slightly bigger test size here given our smaller dataset along with stratifying given the heavy skew in grades that we saw with the previous visualization
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <h2 id="ivii-evaluation-metrics">IV.II: Evaluation Metrics</h2> <p>To see our model performed, we need a way to quantitatively score it. For this, I utilize a series of built-in functions from Scikit-Learn to calculate accuracy, F1-Score, and Confusion Matrices. Accuracy is the default calculation when doing something like this, and is always a helpful metric to have, but in an imbalanced dataset like this, can be highly misleading. If our model does overfit and assigns almost every student a D grade (<em>foreshadowing</em>), then our accuracy might be OK but the few students that do have higher grades will be completely overshadowed! For that reason, I also decided to include a weighted F1-Score to ensure that both Precision and Recall are included within our evaluation, and we aren’t applauding a model that in reality is pretty terrible. Finally, our confusion matrix can provide some great insight into how the model is classifying various how it should be classifying by showing us the predicted class versus the actual class, so if it does make a mistake, we can see which class it is from. Again, many of the metrics are already provided by Scikit-Learn, but I did make wrappers for each so I could just generalize them a bit easier:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Any</span>

<span class="k">def</span> <span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates the accuracy for a given model on a test set

    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model

    Returns:
        float: Model accuracy
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates the f1-score for a given model on a test set

    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model

    Returns:
        float: Model f1-score
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">edu_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Output Grade</span><span class="sh">'</span><span class="p">]),</span>  <span class="n">average</span> <span class="o">=</span> <span class="sh">'</span><span class="s">weighted</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">X_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_test</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GT</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Calculates and visualizes the confusion matrix for a given model on a test set
    
    Args:
        model (Any): Provided Scikit Learn Model to test with
        X_test (np.ndarray): Test Input for Model
        y_test (np.ndarray): Test Target for the model
        
    Returns:
        
    </span><span class="sh">"""</span>
    <span class="c1"># Get predictions and compute confusion matrix
</span>    <span class="n">cm</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="nf">from_estimator</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">cm</span>
</code></pre></div></div> <h2 id="iviii-naive-implementation">IV.III: Naive Implementation</h2> <p>Finally onto making and implementing our models! As any good Machine Learning Engineer would do (right?), after splitting my data and creating my evaluation metrics, I just throw my data to my model with essentially no other tuning or customization. The results, were um, not <em>great</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">svc</span> <span class="o">=</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">kernel</span> <span class="o">=</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="mi">11</span><span class="p">)</span>

        <span class="n">svc</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">svc_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">rf</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">7</span><span class="p">)</span>
        <span class="n">rf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">rf_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="n">gb</span> <span class="o">=</span> <span class="nc">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">150</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="p">.</span><span class="mi">15</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">11</span><span class="p">)</span>
        <span class="n">gb</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">gb</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">gb_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.32756132756132755</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.32756132756132755</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4772727272727273</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.42273533204384267</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_svc_cm-480.webp 480w,/assets/img/edu_svc_cm-800.webp 800w,/assets/img/edu_svc_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_svc_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_rf_cm-480.webp 480w,/assets/img/edu_rf_cm-800.webp 800w,/assets/img/edu_rf_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_rf_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_gb_cm-480.webp 480w,/assets/img/edu_gb_cm-800.webp 800w,/assets/img/edu_gb_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_gb_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>Taking a look at the confusion matrices, we can see that all three models do what I feared: categorizing almost every student as having a D grade, regardless of what the features indicated. They need some serious refinement if they want to actually become usable for anything practical. We can start by reducing the number of features that each utilizes.</p> <h2 id="iviv-feature-engineering">IV.IV: Feature Engineering</h2> <p>While I love this dataset and all of the information it provides about students, it’s clear that not all of it is pertinent to accurately predicting student performance and in many cases, might be hurting it. Thus, we need to see which features models are utilizing and focus on those instead of muddying the water with unnecessary noise. Luckily, random forests have a class attribute that can provide us that exact information:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_importances</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">rf</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">most_important_features</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="n">feature_importances</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">head</span><span class="p">(</span><span class="mi">7</span><span class="p">).</span><span class="n">index</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The most important features are: </span><span class="si">{</span><span class="n">most_important_features</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">The</span> <span class="n">most</span> <span class="n">important</span> <span class="n">features</span> <span class="n">are</span><span class="p">:</span> <span class="nc">Index</span><span class="p">([</span><span class="sh">'</span><span class="s">Impact of projects / activities on your success</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Additional Work</span><span class="sh">'</span><span class="p">,</span>
       <span class="sh">'</span><span class="s">Number of Sisters / Brothers</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Mother</span><span class="sh">'</span><span class="n">s</span> <span class="n">Education</span><span class="sh">'</span><span class="s">,
       </span><span class="sh">'</span><span class="n">Reading</span> <span class="nf">frequency </span><span class="p">(</span><span class="n">scientific</span> <span class="n">books</span><span class="o">/</span><span class="n">journals</span><span class="p">)</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="n">Weekly</span> <span class="n">Study</span> <span class="n">Hours</span><span class="sh">'</span><span class="s">,
       </span><span class="sh">'</span><span class="n">Father</span><span class="sh">'</span><span class="s">s Education</span><span class="sh">'</span><span class="p">],</span>
      <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>Just looking at these features, a lot of this actually makes a ton of sense. These are engineering students that were surveyed, and that means things like projects and outside activities can provide so much valuable insight and information on the academic development of the participant. The amount of additional work they put outside of classes is vital, and we do see parent’s education does seem to play a role (Mother more than father indicates again the vital role of maternal focus on education in adolescence). The number of sisters and brothers may seem odd, but this is perhaps attributed to having an adequate support system that can aid a student throughout their studies or even having siblings in the same major that provide excellent study aids and peers. Nevertheless, let’s see if focusing on these features can aid our model’s performance. We modify our training and test datasets, and run essentially the exact same code above to get the following:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># SVM
</span>      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4594594594594595</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.29474757776644567</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="c1"># Random Forest
</span>        <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.5675675675675675</span>
        <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.5183982683982684</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># Gradient Boosting
</span>      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4864864864864865</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.4370368150855956</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_svc_rev_cm-480.webp 480w,/assets/img/edu_svc_rev_cm-800.webp 800w,/assets/img/edu_svc_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_svc_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_rf_rev_cm-480.webp 480w,/assets/img/edu_rf_rev_cm-800.webp 800w,/assets/img/edu_rf_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_rf_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_gb_rev_cm-480.webp 480w,/assets/img/edu_gb_rev_cm-800.webp 800w,/assets/img/edu_gb_rev_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_gb_rev_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model with re-engineering features, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>We can see for Random Forest, just focusing on those features provided quite a noticeable accuracy bump, while for Gradient Boosting it minimally improved performance and actually reduced performance for the SVM (oops). Regardless, all of our models are still doing quite poor, meaning we need to try one more thing: Hyperparameter Tuning.</p> <h2 id="ivv-hyperparameter-tuning">IV.V: Hyperparameter Tuning</h2> <p>Hyperparameters can be incredibly crucial to the performance of every single ML model, so ensuring that they are properly tuned and have the right values is a must. Luckily, Sci-Kit Learn provides a <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> class that can conduct this entire process for us; all we have to do is define bounds for each hyperparameter and allow Scikit-Learn to take care of the rest. This <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> is better than the <code class="language-plaintext highlighter-rouge">GridSearchCV</code> as it does not search the entirety of hyperspace but randomly queries and tests values for a specified number of iterations, reducing the overall time needed to tune our models. Below is the code for conducting a hyperparameter search for a SVM, but the process is largely the same for Random Forests and Gradient Boosting, just with different hyperparameters and values of course.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hyperparameter Tuning for Support Vector 
</span>
<span class="n">svc_param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">kernel</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">poly</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">degree</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">scale</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">svc_grid_search</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="nc">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">11</span><span class="p">),</span>
    <span class="n">param_distributions</span><span class="o">=</span><span class="n">svc_param_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">11</span>
<span class="p">)</span>


<span class="n">svc_grid_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span>
<span class="n">svc_rev_hyp</span> <span class="o">=</span> <span class="n">svc_grid_search</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The best estimator parameters were: </span><span class="si">{</span><span class="n">svc_grid_search</span><span class="p">.</span><span class="n">best_params_</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Accuracy Score: </span><span class="si">{</span><span class="nf">accuracy_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">F1 Score: </span><span class="si">{</span><span class="nf">f1_score_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">svc_rev_hyp_cm</span> <span class="o">=</span> <span class="nf">confusion_matrix_calc</span><span class="p">(</span><span class="n">svc_rev_hyp</span><span class="p">,</span> <span class="n">X_test_rev</span><span class="p">,</span> <span class="n">y_test_rev</span><span class="p">)</span>
</code></pre></div></div> <p>And finally, we were able to get some solid results with this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># SVM
</span>      <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">kernel</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">degree</span><span class="sh">'</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">}</span>
      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8648648648648649</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.8517859965228386</span>  
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">        <span class="c1"># Random Forest
</span>        <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span> <span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">log_loss</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">class_weight</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">balanced_subsample</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bootstrap</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.972972972972973</span>
        <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.9752661752661753</span>
      </code></pre></figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c1"># Gradient Boosting
</span>      <span class="n">The</span> <span class="n">best</span> <span class="n">estimator</span> <span class="n">parameters</span> <span class="n">were</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">subsample</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span> <span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">friedman_mse</span><span class="sh">'</span><span class="p">}</span>
      <span class="n">Accuracy</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.43243243243243246</span>
      <span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.37537537537537535</span>
      </code></pre></figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_svc_rev_hyp_cm-480.webp 480w,/assets/img/edu_svc_rev_hyp_cm-800.webp 800w,/assets/img/edu_svc_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_svc_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_rf_rev_hyp_cm-480.webp 480w,/assets/img/edu_rf_rev_hyp_cm-800.webp 800w,/assets/img/edu_rf_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_rf_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_gb_rev_hyp_cm-480.webp 480w,/assets/img/edu_gb_rev_hyp_cm-800.webp 800w,/assets/img/edu_gb_rev_hyp_cm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_gb_rev_hyp_cm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The confusion matrices for each model with re-engineering features and hyperparameter tuning, from left to right: SVM, Random Forest, and Gradient Boosting </div> <p>Now, let’s address the elephant in the room before proceeding: yes, I still can’t figure out the Gradient Boosting Trees. Somehow the accuracy got worst throughout the refinement process, and I chalk that up to my own ignorance and novice to the concept, not the model itself (I’ve heard wonderful things about that). However, the accuracy jump for SVMs and Random Forest is incredible, with the latter really doing exceptional across all of our metrics, with only misclassifying one student that got a D as a B- (we just have a nice, overly optimistic model). I knew hypermeters were important, but I’d be lying if I told you I thought the performance would increase this much. We have actually have something that is usable for a potential school setting!</p> <p>Just to summarize, here all of the results of our models and implementations in a nice table (shout out again to <code class="language-plaintext highlighter-rouge">great_tables</code>).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/edu_results_table-480.webp 480w,/assets/img/edu_results_table-800.webp 800w,/assets/img/edu_results_table-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/edu_results_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="v-conclusion-and-impact">V. Conclusion and Impact</h1> <p>Just to remind you, we set out to answer two main questions: <strong>Which features of a student are most correlated with academic success and how can we utilize these features to predict which grades they will achieve in their studies?</strong>. We definitely answered both, with our process highlighting the importance of practical hands-on experience outside the classroom, parental education level, and also the need for emotional factors like a support system that can provide comfort and aid. We also found that by focusing on these features, we can accurately predict student grades with an accuracy upwards of <strong>97%</strong>, albeit also sometimes as low as 43%. As educators begin to try to implement holistic educational curriculums, focusing on factors like these can help identify inequities before they become systemic an ensure that students are receiving the help needed based upon their own personal situation and lifestyle. Even if these models will forever always be inherently flawed and not a comprehensive predictor of academic success, they can provide a good baseline to identify where students are expected to perform and how they either exceed those expectations——indicating effective instruction and collegiate support—or faltered below them—indicating a need for more robust interventions and overhaul of existing systems.</p> <h1 id="vi-references">VI. References</h1> <p><a href="https://www.geeksforgeeks.org/comprehensive-guide-to-classification-models-in-scikit-learn/#model-evaluation-metrics" rel="external nofollow noopener" target="_blank">GeeksForGeeks: Comprehensive Guide to Classification Models in Scikit-Learn</a></p> <p><a href="https://www.geeksforgeeks.org/how-to-tune-hyperparameters-in-gradient-boosting-algorithm/#" rel="external nofollow noopener" target="_blank">GeeksForGeeks: How to Tune Hyperparameters in Gradient Boosting Algorithm </a></p> <p>Literally Every Page in the Scikit-Learn Documentation for every model or function I used</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Arnav Sareen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>