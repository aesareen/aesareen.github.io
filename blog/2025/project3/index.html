<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Project Three - Catching the Bag: Forecasting the NFL Wide Receiver Contracts Based Upon Historical Statistics | Arnav Sareen - Portfolio </title> <meta name="author" content="Arnav Sareen"> <meta name="description" content="Amid the massive spending in NFL free agency, can we truly answer whether teams are doling out good contracts? Is there a way to qualitatively predict what a player should be paid based upon their output and production?"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/3162-portfolio/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/3162-portfolio/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/3162-portfolio/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/3162-portfolio/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/3162-portfolio/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aesareen.github.io/3162-portfolio/blog/2025/project3/"> <script src="/3162-portfolio/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/3162-portfolio/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/3162-portfolio/"> Arnav Sareen - Portfolio </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/3162-portfolio/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/3162-portfolio/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/3162-portfolio/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/3162-portfolio/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Project Three - Catching the Bag: Forecasting the NFL Wide Receiver Contracts Based Upon Historical Statistics</h1> <p class="post-meta"> Created in March 25, 2025 </p> <p class="post-tags"> <a href="/3162-portfolio/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/3162-portfolio/blog/category/dtsc-3162"> <i class="fa-solid fa-tag fa-sm"></i> dtsc-3162</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"> <a href="#i-introduction">I. Introduction</a> <ul> <li class="toc-entry toc-h2"><a href="#iii-where-did-this-data-even-come-from">I.II: Where did this data even come from?</a></li> </ul> </li> <li class="toc-entry toc-h1"><a href="#ii-how-does-regression-work">II. How Does Regression Work?</a></li> <li class="toc-entry toc-h1"><a href="#iii-data-understanding-and-pre-processing">III. Data Understanding and Pre-Processing</a></li> <li class="toc-entry toc-h1"><a href="#iv-experiment-one-univariate-linear-regression">IV. Experiment One: Univariate Linear Regression</a></li> <li class="toc-entry toc-h1"><a href="#v-experiment-two-multivariate-regression-models">V. Experiment Two: Multivariate Regression Models</a></li> <li class="toc-entry toc-h1"><a href="#vi-experiment-three-hyperparameter-tuning">VI. Experiment Three: Hyperparameter Tuning</a></li> <li class="toc-entry toc-h1"><a href="#vii-conclusion-and-impact">VII. Conclusion and Impact</a></li> <li class="toc-entry toc-h1"><a href="#viii-references">VIII. References</a></li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="i-introduction">I. Introduction</h1> <p>Amidst the donut-loving, chilly streets of New England persists one of the region’s most beloved and historic pastimes: sports. Having been born and spent the first twelve years of my life living in Massachusetts, I was practically indoctrinated in a fierce and persistent love for Boston sports teams—the Bruins, Red Sox, and Celtics all remain casual fandoms in my current life, but my absolute adoration for the New England Patriots and football has only blossomed in my years since departing the Bay State (perhaps due to the lack of a similarly successful equivalent in the Carolinas…).</p> <p>With this very crucial backstory out of the way (trust me), we can finally launch into the main theme of this project: predicting the contracts of NFL players. In case you’re unfamiliar, most sports operate on a contractual basis, where players are signed to a team for a specific number of years for up to a specified amount of money. In the NFL, when these players’ contracts expire and are not renewed by the original team (a rationale that ranges from the player simply not being good enough anymore to financial constraints of the team), the player enters a period known as <strong>free agency</strong>. Now, any team (with some occasional, nuanced exceptions that are beyond the scope of this project), are able to offer a contract to the player in hopes that they bring their talents to a new franchise. As you can imagine, free agency period—which typically coincides with the start of the NFL New Year in mid-March—is a deeply exciting and turbulent time as generations of lives change overnight through the formulation and signing of unfathomably lucrative contracts. For instance, in this most recent 2025 free agency period alone, just looking at players who are <em>wide receivers</em>, we can observe the following contracts:</p> <ul> <li>Ja’Marr Chase (<em>Cincinnati Bengals</em>): $<strong>161,000,000</strong> over 4 years, $<strong>40,250,000</strong> average per year</li> <li>D.K. Metcalf (<em>Pittsburgh Steelers</em>): $<strong>131,999,529</strong> over 4 years, $<strong>32,999,882</strong> average per year</li> <li>Tee Higgins (<em>Cincinnati Bengals</em>): $<strong>115,000,000</strong> over 4 years, $<strong>28,750,000</strong> average per year</li> <li>Chris Godwin (<em>Tampa Bay Buccaneers</em>): $<strong>66,000,000</strong> over 3 years, $<strong>22,000,000</strong> average per year</li> <li>Devante Adams (<em>Los Angeles Rams</em>): $<strong>44,000,000</strong> over 2 years, $<strong>22,000,000</strong> average per year</li> </ul> <p>Literally while working on this project, my beloved New England Patriots made a long-overdue splash for wide receiver Stefon Diggs, signing him to a massive 3 year, <strong>$63,500,000</strong> deal for an average of <strong>$21,166,667</strong> per year. With this obscene amount of money being thrown around, I think it is normal for both invested fans and even casual observers to ask: <strong>How much money should these players be made? What about their production on the field translates to their pay off the field?</strong>. To do this, we must launch in the world of regression and delve into the various models and techniques out there to predict continuous values. For simplicity, my project focuses on a very specific position in the NFL, the aforementioned wide receivers. However, the techniques and strategies explained below are equally applicable to any other positions (with different features and inputs of course).</p> <h2 id="iii-where-did-this-data-even-come-from">I.II: Where did this data even come from?</h2> <p>Now, despite how popular the NFL is and how big of a deal NFL free agency has become, there is actually <em>no easily accessible API or package for NFL salary data</em>. That means I had to utilize every site administrator’s worst nightmare: <strong>web scraping</strong>. Luckily, there is an awesome site called <a href="https://www.overthecap.com" rel="external nofollow noopener" target="_blank">overthecap.com</a> that provides an immense amount of financial data and information about NFL players, updated daily. There is a <a href="https://overthecap.com/contract-history/" rel="external nofollow noopener" target="_blank">very specific page</a> that focuses upon contract history and has data spanning all the way back to 1985 and robust tracking since the early 2010s. Hence, some basic using <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="external nofollow noopener" target="_blank">BeautifulSoup</a> parsing, I was able to quickly scrape OverTheCap and organize my data into a DataFrame that contains information like:</p> <ul> <li>Player Name</li> <li>Contract Sign Date</li> <li>Contract Value, Average Per Year (APY), and Length</li> <li>Whether the contract is still active</li> </ul> <p>This financial data is coupled with statistical data courtesy of the <code class="language-plaintext highlighter-rouge">nfl_data_py</code> package, which is the Python Wrapper of the amazing <a href="https://nflverse.nflverse.com/" rel="external nofollow noopener" target="_blank">nflverse</a> project that has made a huge amount of NFL player data and advanced statistics available for free. For our use case where we are focusing on receivers, this includes vital stats like:</p> <ul> <li>A Player’s Height, Weight, and Age</li> <li>A Player’s Receiving Yards, Touchdowns, Receptions and Targets Per Season</li> <li>A number of advanced stats like <a href="https://www.the33rdteam.com/epa-explained/" rel="external nofollow noopener" target="_blank">Receiving EPA</a>, <a href="https://www.addmorefunds.com/articles/nfl/air-yards/" rel="external nofollow noopener" target="_blank">Weighted Opportunity Rating</a>, and <a href="https://www.pff.com/news/fantasy-football-predicting-breakout-rookie-wide-receivers-using-pff-grades-and-dominator-rating" rel="external nofollow noopener" target="_blank">Dominator Rating</a> </li> </ul> <p>These metrics will prove incredibly helpful in aiding our prediction, though as we will soon see, sometimes the most simple of calculations is all we need to make largely accurate predictions.</p> <p><strong>If you’d prefer to download this notebook, just press <a href="https://github.com/aesareen/3162-portfolio/blob/main/assets/jupyter/project_3.ipynb" rel="external nofollow noopener" target="_blank">here</a>.</strong></p> <h1 id="ii-how-does-regression-work">II. How Does Regression Work?</h1> <p>I use three main regression models throughout this notebook, but they all stem from the grandfather of all regression models: <strong>Linear Regression</strong>.</p> <p>Statistically, calculating linear regression for a single variable (a <strong>univariate</strong> function) relies upon a relatively simple equation to find the slope of our line, $\beta$, and the intercept, $\alpha$.</p> \[\beta = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{(x_i - \bar{x})^2}\] \[\alpha = \hat{y} - \beta\hat{x}\] <p>This <strong>analytical method</strong> works great for single variable functions, but doesn’t address the fact that many real-world problems involves a single variable depending on a variety of predictor variables, along with this approach doesn’t quite scale super-well computationally with the massive amounts of data that the modern world interacts with. Hence, the need for the <strong>closed-form</strong> solution, also known as <strong>Ordinary Least Squares</strong>. This utilizes an equation with a data matrix that stores all of our features and inputs for us, and uses some nifty matrix operations and matrix calculus to calculate the optimal weights for us with the following equation:</p> \[\hat{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\] <p>$X$ here is our <strong>data matrix</strong>, which is simply a matrix-representation of our data, for instance, if I had data that looked like this:</p> <meta charset="UTF-8"> <title>Centered Table</title> <style>table{margin:50px auto;border-collapse:collapse;text-align:center}th,td{border:1px solid #333;padding:8px 12px}</style> <table> <thead> <tr> <th>Receiving Yards</th> <th>Touchdowns</th> <th>Age</th> </tr> </thead> <tbody> <tr> <td>1300</td> <td>7</td> <td>22</td> </tr> <tr> <td>386</td> <td>2</td> <td>29</td> </tr> </tbody> </table> <p>In a data matrix, that would look like:</p> \[\begin{pmatrix}1 &amp; 1300 &amp; 7 &amp; 22\\\ 1 &amp; 386 &amp; 2 &amp; 29\end{pmatrix}\] <p>You can see we add an additional column of 1’s to represent our intercept or <strong>bias</strong> (of course, this can be any constant value, but 1 is traditional to start out with).</p> <p>This OLS solution is awesome, and is in fact the method that scikit-learn uses underneath the hood with the <code class="language-plaintext highlighter-rouge">LinearRegression</code> class <a href="https://stackoverflow.com/a/34470001" rel="external nofollow noopener" target="_blank">[1]</a> but faces one main drawback: calculating $(\mathbf{X}^\top \mathbf{X})^{-1}$ can be a very expensive operation with very large datasets. Hence, there is one last approach, <strong>Least Mean Squares</strong> or also known as <strong>Linear Regression with Gradient Descent</strong> that adopts a more iterative optimization approach to deal with larger datasets that may take too long to run with OLS or work with data matrices that might not even fit into the memory of our machine to do the inversion.</p> \[\mathbf{w}_{new} = \mathbf{w}_{old} - \alpha \nabla J(\mathbf{w}_{old})\] <p>With this method, we are initializing our weights to random values and leveraging a hyperparameter called <strong>learning rate</strong>, $\alpha$ or $\eta$ (depending on where you look), to iteratively tweak our weights. The key behind any gradient descent process is a <strong>loss function</strong>, which in this case is $J(\mathbf{w}_{old})$. We can have a variety of loss functions, but the idea is that they represent the difference between the our true labels and the values we are currently predicting. We want to minimize this of course, hence why we take the <strong>gradient</strong> of it and subtract it from our current weights. In the case of Linear Regression, our loss function is really nice, <strong>the mean squared error</strong>:</p> \[J(\mathbf{w}_{old}) = \frac{1}{n}\sum{(x_{i}^\top \mathbf{w}_{old} - y_{i})^{2}} = \frac{1}{n}\sum{(\hat{y} - y_{i})^{2}}\] <p>It looks really complex, but it is really just finding the difference our our predicted value and true value and averaging that difference over the entire dataset. I utilize a univariate Linear Regression Model below and it performs not great, but given how simple it is, much better than I anticipated!</p> <p>The other models I use are summarized below, with a brief description for each:</p> <ul> <li> <strong>Decision Trees</strong>: While decision trees are awesome for classification, we can also utilize them for regression while working with numerical values. They also work to minimize a statistic like MSE and split based on that instead of Gini Index or entropy; the leaf nodes are predicting numerical values instead of the predicted class.</li> <li> <strong>Random Forests</strong>: Super similar idea to Decision Trees and their versatility, but they take it further by being an ensemble model that combines many weak decision trees together to usually create more powerful, robust predictions that are less prone to overfitting compared to a normal decision tree.</li> <li> <strong>XGBoost</strong>: XGBoost is yet another ensemble model that is very efficient and effective, and among one of the most popular models for both classification and regression. To be honest, I don’t quite fully understand the nuances of gradient boosting quite yet but I know that it involves the “multiple-models” approach of random forests with the minimizing loss function through gradients that we discussed earlier with least mean squares. There is a really easy to use Python package that is super compatible with scikit-learn, so I decided to give it a go!</li> </ul> <h1 id="iii-data-understanding-and-pre-processing">III. Data Understanding and Pre-Processing</h1> <p>Given that I created the financial dataset myself, much of the pre-processing and formatting was already done when I collected the data. To get a better idea of why I decided to just focus on the wide receivers instead of creating a more general model to all predictions, we can quickly visualize the average salary values by position:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/nfl_average_pay_per_year-480.webp 480w,/3162-portfolio/assets/img/nfl_average_pay_per_year-800.webp 800w,/3162-portfolio/assets/img/nfl_average_pay_per_year-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/3162-portfolio/assets/img/nfl_average_pay_per_year.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This chart can provide some really helpful insight into why a general model wouldn’t perform great: there is so much variation in the pay per position. Yes, we could obviously use pay as a feature and that would help, but in my mind, creating a tailored model on a per-position basis can allow so much more fine-tuning and adapting that you can only get on a granular scale. Receivers to me met the middle ground of being a valued position while not being so far down that their contracts are not particularly interesting (see other positions not even listed here like center, guard, or punter).</p> <p>With that solidified, we can move onto cleaning and processing the analytical data. To do that, I first had to use the pandas <code class="language-plaintext highlighter-rouge">.join()</code> methods to combine multiple stats datasets together and then had to do some name alterations to ensure that the names in this dataframe were consistent with the ones in the salary one:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a dictionary to hold the player name corrections
</span><span class="n">name_corrections</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">DK Metcalf</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">D.K. Metcalf</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Devonta Smith</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">DeVonta Smith</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Michael Pittman</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Michael Pittman, Jr.</span><span class="sh">"</span>
<span class="p">}</span>

<span class="c1"># Apply the corrections to the nfl_data DataFrame
</span><span class="k">for</span> <span class="n">old_name</span><span class="p">,</span> <span class="n">new_name</span> <span class="ow">in</span> <span class="n">name_corrections</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">nfl_data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">nfl_data</span><span class="p">[</span><span class="sh">'</span><span class="s">player_name</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="n">old_name</span><span class="p">,</span> <span class="sh">'</span><span class="s">player_name</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_name</span>
</code></pre></div></div> <p>With that out of the way, I decided to take a look at the top salaries overall in the NFL and because I had access to headshots via <code class="language-plaintext highlighter-rouge">nfl_data_py</code>, I decided to have some fun and make some nice tables:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/highest_paid_players_all-480.webp 480w,/3162-portfolio/assets/img/highest_paid_players_all-800.webp 800w,/3162-portfolio/assets/img/highest_paid_players_all-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/3162-portfolio/assets/img/highest_paid_players_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/highest_paid_wrs-480.webp 480w,/3162-portfolio/assets/img/highest_paid_wrs-800.webp 800w,/3162-portfolio/assets/img/highest_paid_wrs-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/3162-portfolio/assets/img/highest_paid_wrs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>This illustrates yet another reason why I think going with wide receivers is much better than focusing on the most flashy position, quarterback. If you look at the highest paid players list (which is exclusively QBs), some of the players there are <em>really</em> good and are among the best in their position. But some of the players there <em>kinda suck</em> and don’t really have the statistical feats of their peers to make an algorithmic approach predicated historical stats really feasible (tldr; Trevor Lawrence and Tua Tagovailoa are over-paid and would likely mess up my model). On the other hand, all of the wide receivers paid within the top ten are essentially the best-of-the-best only and all of them have strong cases to be in that list (some cases are stronger than others though).</p> <p>The <code class="language-plaintext highlighter-rouge">nfl_data_py</code> dataframe gives a great deal of information, and most of it isn’t particularly relevant, so I parsed it down to nine relevant features that I believe are most critical to this prediction (all of these are per season): player’s age, receptions, targets, receiving touchdowns, receiving yards, receiving air yards, receiving yards culminated after a catch, receiving EPA, target share, and weighted dominator score. To tackle our first experiment with using a simple linear regression model, I plotted each against the our target, which I decided to be average pay per year instead of contract value due to it being a bit more normalized against long, potentially misleading contracts.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/nfl_feat_scatter-480.webp 480w,/3162-portfolio/assets/img/nfl_feat_scatter-800.webp 800w,/3162-portfolio/assets/img/nfl_feat_scatter-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/3162-portfolio/assets/img/nfl_feat_scatter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="iv-experiment-one-univariate-linear-regression">IV. Experiment One: Univariate Linear Regression</h1> <p>Having already gone in-depth about Linear Regression and now identified the trends of our relevant features, we can finally begin some actual modeling. There is a slight problem, none of our features really look strictly linear, and in fact, most of them have a pretty similar trend line if we plotted a single one for each feature individually. To make it simple, I picked the most most straightforward feature, receiving yards, for my univariate linear regression model. As with any model, we first begin by splitting our data into relevant datasets:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">basic_X_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">basic_X_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">basic_y_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> 
<span class="n">basic_y_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> 

<span class="c1"># We have to use reshape here to turn our row vectors into column vectors
</span><span class="n">basic_X_trn</span><span class="p">,</span> <span class="n">basic_X_tst</span><span class="p">,</span> <span class="n">basic_y_trn</span><span class="p">,</span> <span class="n">basic_y_tst</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">combined_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">receiving_yards</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">combined_stats</span><span class="p">[</span><span class="sh">'</span><span class="s">APY ($)</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">test_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">33</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <p>As with any scikit-learn model, creating and fitting our model is literally just two lines, which is awesome:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">basic_model</span><span class="p">:</span> <span class="n">LinearRegression</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">basic_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">basic_X_trn</span><span class="p">,</span> <span class="n">basic_y_trn</span><span class="p">)</span>
</code></pre></div></div> <p>Given that we are doing quite a bit of repeated evaluation throughout this project that is essentially the same sequence over and over again, I decided to create a helper function <code class="language-plaintext highlighter-rouge">evaluate_model</code> that helps speed up this process a bit. It simply takes in the model to evaluate, the relevant training &amp; test datasets, and then also a list of metrics that we want to evaluate the model on, calculating each metrics on both the training &amp; test datasets:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Callable</span>

<span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">X_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">X_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">metrics</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="nb">float</span><span class="p">]],</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">Calculate the metric scores for a model

    Args:
        model (BaseEstimator): scikit learn model to use for evaluation
        X_trn (np.ndarray): training input for the model
        X_tst (np.ndarray): testing input 
        y_trn (np.ndarray): training output
        y_tst (np.ndarray): testing output
        metrics (list[Callable[[np.ndarray, np.ndarray], float]]): metrics to evaluate the model on; from sklearn.metrics
        model_name (str, optional): name of the model to use in print statements. Defaults to </span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="s">.

    Returns:
        tuple[dict[str, float], np.ndarray, np.ndarray]: metrics of the model, train predictions of model, and test predictions
    </span><span class="sh">"""</span>
    <span class="n">metric_scores</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="c1"># Calculate the training metrics
</span>    <span class="n">train_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_trn</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
        <span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">metric</span><span class="p">.</span><span class="n">__name__</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">).</span><span class="nf">title</span><span class="p">()</span>
        <span class="n">metric_score</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nf">metric</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">train_preds</span><span class="p">)</span>
        <span class="n">metric_scores</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s">_train</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric_score</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The </span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s"> on the training data of our </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s"> is </span><span class="si">{</span><span class="n">metric_score</span><span class="si">:</span><span class="p">,.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># Calculate the test metrics
</span>    <span class="n">test_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_tst</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
        <span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">metric</span><span class="p">.</span><span class="n">__name__</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">).</span><span class="nf">title</span><span class="p">()</span>
        <span class="n">metric_score</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nf">metric</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">test_preds</span><span class="p">)</span>
        <span class="n">metric_scores</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s">_test</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric_score</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The </span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s"> on the testing data of our </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s"> is </span><span class="si">{</span><span class="n">metric_score</span><span class="si">:</span><span class="p">,.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">metric_scores</span><span class="p">,</span> <span class="n">train_preds</span><span class="p">,</span> <span class="n">test_preds</span>
</code></pre></div></div> <p>We can then use this function to do much of the evaluation for us:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">basic_eval</span><span class="p">,</span> <span class="n">basic_train_preds</span><span class="p">,</span> <span class="n">basic_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">basic_model</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">basic_X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">basic_X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">basic_y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">basic_y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Basic Linear Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Output
The Root Mean Squared Error on the training data of our Basic Linear Regression Model is 2,983,494.0606
The R2 Score on the training data of our Basic Linear Regression Model is 0.6813
The Root Mean Squared Error on the testing data of our Basic Linear Regression Model is 2,798,006.3796
The R2 Score on the testing data of our Basic Linear Regression Model is 0.4157
</code></pre></div></div> <p>That’s not particularly great, let’s quickly visualize our line to see how close it is to the actual points (I created another helper function called <code class="language-plaintext highlighter-rouge">make_plot</code> but forgot that my future models will use more than one feature, making it pretty difficult to plot.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">basic_plot</span> <span class="o">=</span> <span class="nf">make_plot</span><span class="p">(</span><span class="n">basic_X_tst</span><span class="p">,</span> <span class="n">basic_y_tst</span><span class="p">,</span> <span class="n">basic_test_preds</span><span class="p">,</span> <span class="sh">"</span><span class="s">Basic Linear Regression: Just Using Receiving Yards</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/basic_lin_reg-480.webp 480w,/3162-portfolio/assets/img/basic_lin_reg-800.webp 800w,/3162-portfolio/assets/img/basic_lin_reg-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/3162-portfolio/assets/img/basic_lin_reg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>So based on this woeful model performance and the previous scatterplot we made, we can clearly see that relying upon a model that uses just a single feature is not going to be nearly enough to succeed. Thus, let’s move on to the next part: multivariate regression.</p> <h1 id="v-experiment-two-multivariate-regression-models">V. Experiment Two: Multivariate Regression Models</h1> <p>Before we delve into using more sophisticated models, it’s clear that we are going to need to buff up our training and test data to include the relevant features that we identified earlier.</p> <p>Wide Receivers have a big of flexibility in the NFL, and some players who are actually listed as a Wide Receiver never play the position at all but instead serve as integral pieces of a team’s special teams (see Patriots legend <a href="https://en.wikipedia.org/wiki/Matthew_Slater" rel="external nofollow noopener" target="_blank">Matthew Slater</a>). Players that fill into this bucket typically have their weighted dominator score be 0, so we will do a little bit of data pre-processing here and simply drop those rows (it’s only a small handful of players affected).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's create a new training set that relies on a few more features
</span><span class="n">X_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">X_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">y_trn</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> 
<span class="n">y_tst</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span> 

<span class="c1"># Some receivers just play special teams and don't necessarily really play as a traditional Wide Receiver, so if there w8dom value is NaN here, we can assume they fall into that category
</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">combined_stats</span><span class="p">[</span><span class="n">rel_features</span><span class="p">].</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">w8dom</span><span class="sh">'</span><span class="p">).</span><span class="n">values</span><span class="p">,</span> <span class="n">combined_stats</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">w8dom</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">APY ($)</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">test_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">33</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <p>With these dataset, we can follow a very simple sequence of method and steps to our linear regression model for our next model: A Decision Tree.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Testing out a Decision Tree
</span><span class="n">tree_model</span><span class="p">:</span> <span class="n">DecisionTreeRegressor</span> <span class="o">=</span> <span class="nc">DecisionTreeRegressor</span><span class="p">()</span>
<span class="n">tree_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">)</span>

<span class="n">tree_eval</span><span class="p">:</span> <span class="nb">dict</span>
<span class="n">tree_train_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">tree_test_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>

<span class="n">tree_eval</span><span class="p">,</span> <span class="n">tree_train_preds</span><span class="p">,</span> <span class="n">tree_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">tree_model</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Decision Tree Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Root Mean Squared Error on the training data of our Decision Tree Regression Model is 291,430.6363
The R2 Score on the training data of our Decision Tree Regression Model is 0.9970
The Root Mean Squared Error on the testing data of our Decision Tree Regression Model is 2,288,790.0169
The R2 Score on the testing data of our Decision Tree Regression Model is 0.6443
</code></pre></div></div> <p>We are already seeing much better performance! But, look at the discrepancy in the training metrics and the testing metrics; that is a massive difference and a clear sign of <strong>overfitting</strong>. Seeing this, I realized that I needed to figure out why this was happening and potential steps to mitigate it. First off, was there a particular feature that the Decision Tree was overfitting to especially?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># What feature is our model overfitting to?
</span><span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">tree_model</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">combined_stats</span><span class="p">[</span><span class="n">rel_features</span><span class="p">].</span><span class="n">columns</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>receiving_yards                0.806376
tgt_sh                         0.060913
targets                        0.038668
w8dom                          0.029500
receiving_epa                  0.017403
receiving_air_yards            0.016052
age                            0.013541
receptions                     0.007803
receiving_yards_after_catch    0.005176
receiving_tds                  0.004569
dtype: float64
</code></pre></div></div> <p>Ah! So we see a huge part of our model’s predictions is just looking at the player’s receiving yards (which we saw above isn’t necessarily the best idea) and then using the other features as essentially afterthoughts to tune its predictions. Now, we have to ask ourselves <strong>is this a problem?</strong>. The model overfitting certainly is, but is just dropping receiving yards the real solution? Because, NFL teams certainly do look at receiving yards as a major indicator of how to structure a player’s contracts, and in many regards, <strong>receiving yards is the most important statistic for a WR</strong>. I did some experimentation, specifically some <strong>ablation tests</strong> where I dropped receiving yards and evaluate the performance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's see if we can remove receiving_yards to get similar accuracy
</span><span class="n">rel_features</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="sh">'</span><span class="s">receiving_yards</span><span class="sh">'</span><span class="p">)</span>
<span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">combined_stats</span><span class="p">[</span><span class="n">rel_features</span><span class="p">].</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">w8dom</span><span class="sh">'</span><span class="p">).</span><span class="n">values</span><span class="p">,</span> <span class="n">combined_stats</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">w8dom</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">APY ($)</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">test_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">33</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">tree_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">)</span>
<span class="n">tree_eval</span><span class="p">,</span> <span class="n">tree_train_preds</span><span class="p">,</span> <span class="n">tree_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">tree_model</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Decision Tree Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Root Mean Squared Error on the training data of our Decision Tree Regression Model is 291,430.6363
The R2 Score on the training data of our Decision Tree Regression Model is 0.9970
The Root Mean Squared Error on the testing data of our Decision Tree Regression Model is 2,176,075.7494
The R2 Score on the testing data of our Decision Tree Regression Model is 0.6785
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># View new feature importances
</span><span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">tree_model</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">combined_stats</span><span class="p">[</span><span class="n">rel_features</span><span class="p">].</span><span class="n">columns</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tgt_sh                         0.696595
targets                        0.114698
w8dom                          0.055644
receiving_air_yards            0.052225
receiving_epa                  0.035021
age                            0.016034
receiving_yards_after_catch    0.012006
receiving_tds                  0.009475
receptions                     0.008303
dtype: float64
</code></pre></div></div> <p>We find ourselves getting nearly identical performance with the model having a more balanced feature importance index. Hence, I decided to remove <code class="language-plaintext highlighter-rouge">receiving_yards</code> as a way to hopefully make our model more generalizable, interpretable, and less prone to overfitting. I utilize this new updated feature list for my next model, which also aims to combat overfitting by combine multiple decision trees together: random forests.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let's try to over-combat the overfitting with an ensemble model
</span><span class="n">rf</span><span class="p">:</span> <span class="n">RandomForestRegressor</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">()</span>
<span class="n">rf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>

<span class="n">rf_eval</span><span class="p">:</span> <span class="nb">dict</span> 
<span class="n">rf_train_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">rf_test_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>

<span class="n">rf_eval</span><span class="p">,</span> <span class="n">rf_train_preds</span><span class="p">,</span> <span class="n">rf_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">rf</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Random Forest Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Root Mean Squared Error on the training data of our Random Forest Regression Model is 797,022.3446
The R2 Score on the training data of our Random Forest Regression Model is 0.9779
The Root Mean Squared Error on the testing data of our Random Forest Regression Model is 1,852,110.1425
The R2 Score on the testing data of our Random Forest Regression Model is 0.7671
</code></pre></div></div> <p>We can see that we get an instant boost in out testing accuracy at minimal decrease in our trainining accuracy, and if we inspect the feature importances for the random forest model, we get a much more balanced picture:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tgt_sh                         0.385418
receiving_air_yards            0.301290
w8dom                          0.064857
targets                        0.062970
receiving_yards_after_catch    0.055652
receiving_epa                  0.051901
receptions                     0.051064
age                            0.015059
receiving_tds                  0.011789
dtype: float64
</code></pre></div></div> <p>We are striking a good medium of using target share and receiving air yards to predict now, however the weighted dominator factor, target, and receptions all have notable impacts on the final predicted score.</p> <p>Finally, we can move onto our last model: XGBoost. Even though this not directly from the scikit-learn library, it’s objects and methods are directly compatiable with the library and the code is essentially identical</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Another ensemble method, but maybe even more generalizable and powerful: XGBoost!
</span><span class="n">xgboost</span><span class="p">:</span> <span class="n">XGBRegressor</span> <span class="o">=</span> <span class="nc">XGBRegressor</span><span class="p">()</span>
<span class="n">xgboost</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">)</span>


<span class="n">xgboost_eval</span><span class="p">:</span> <span class="nb">dict</span> 
<span class="n">xgboost_train_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="n">xgboost_test_preds</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>

<span class="n">xgboost_eval</span><span class="p">,</span> <span class="n">xgboost_train_preds</span><span class="p">,</span> <span class="n">xgboost_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">XGBoost Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Root Mean Squared Error on the training data of our XGBoost Regression Model is 293,540.1383
The R2 Score on the training data of our XGBoost Regression Model is 0.9970
The Root Mean Squared Error on the testing data of our XGBoost Regression Model is 1,902,217.4789
The R2 Score on the testing data of our XGBoost Regression Model is 0.7543
</code></pre></div></div> <p>We can see that the latter two models do very similarly in performance, and all of them can be helpful in trying to acocomplish our ultimate goal: predicting these contracts. With these enhanced models out of the way, we can move onto the culminating experiement: <strong>hyperparameter tuning</strong>.</p> <h1 id="vi-experiment-three-hyperparameter-tuning">VI. Experiment Three: Hyperparameter Tuning</h1> <p>I utilized hyperparameter tuning last project, and none of my models gave me even useable results until I did. My accuracy for this project is a bit better this time around, so I am not expecting so drastic of an improvement, but at least noticable growth is RMSE and $R^2$ would be great.</p> <p>The process for all three models—decision trees, random forests, and XGBoost—is essentially identical, with the only difference being the actual parameters we are tuning of course. For reference, I show the code for the XGBoost tuning below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgboost_param_grid</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.10</span><span class="p">,</span><span class="mf">0.15</span><span class="p">,</span><span class="mf">0.20</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.30</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">min_child_weight</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span> <span class="p">],</span>
    <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span> <span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.4</span> <span class="p">],</span>
    <span class="sh">"</span><span class="s">colsample_bytree</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span><span class="mf">0.5</span> <span class="p">,</span><span class="mf">0.7</span> <span class="p">]</span>
<span class="p">}</span>

<span class="n">xgboost_random_search</span><span class="p">:</span> <span class="n">RandomizedSearchCV</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="n">param_distributions</span> <span class="o">=</span> <span class="n">xgboost_param_grid</span><span class="p">,</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">neg_root_mean_squared_error</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r2</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">refit</span> <span class="o">=</span> <span class="sh">"</span><span class="s">neg_root_mean_squared_error</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">11</span>
<span class="p">)</span>

<span class="n">xgboost_random_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_trn</span><span class="p">,</span> <span class="n">y_trn</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>
<span class="n">tuned_xgboost</span><span class="p">:</span> <span class="n">XGBRegressor</span> <span class="o">=</span> <span class="n">xgboost_random_search</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">The best estimator parameters were: </span><span class="si">{</span><span class="n">xgboost_random_search</span><span class="p">.</span><span class="n">best_params_</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">tuned_forest_eval</span><span class="p">,</span> <span class="n">tuned_forest_train_preds</span><span class="p">,</span> <span class="n">tuned_forest_test_preds</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">tuned_xgboost</span><span class="p">,</span> <span class="n">X_trn</span> <span class="o">=</span> <span class="n">X_trn</span><span class="p">,</span> <span class="n">X_tst</span> <span class="o">=</span> <span class="n">X_tst</span><span class="p">,</span> <span class="n">y_trn</span> <span class="o">=</span> <span class="n">y_trn</span><span class="p">,</span> <span class="n">y_tst</span> <span class="o">=</span> <span class="n">y_tst</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">],</span> <span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Tuned XGBoost Regression Model</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The results for all of the models, including the hyparameter tuned ones, can be found below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/3162-portfolio/assets/img/nfl_results_table-480.webp 480w,/3162-portfolio/assets/img/nfl_results_table-800.webp 800w,/3162-portfolio/assets/img/nfl_results_table-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/3162-portfolio/assets/img/nfl_results_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This table shows us that while hyperparameter tuning definietely helped our models here, it wasn’t the parameters themselves this time that needed to be radically changed to improve performance. Instead, our models might benefit from other measures like regularization to further improve performance and generalizability. Oh well, that’s for another project.</p> <h1 id="vii-conclusion-and-impact">VII. Conclusion and Impact</h1> <p>One of the biggest impacts of a project like this is that currently contract negotations demand an intimate knowledge of the current market for a player and how a player’s stats translate into tangible financial value. This can help provide players that are representing themselves or athletes that are a bit earlier into their career that cannot quite afford an expensive agent to get a rough estimate about how their production thus far translates to NFL dollar amounts. However, one of the drawbacks is that there are so many aspects that go into contract agreements that is deeply unquantifable, at least not readily; how much a player likes or dislikes a particular geogrpahic location, the increased tax rates in one state compared to another that can inflate the final APY, or the famous “hometown discount” that players like <a href="https://www.cbssports.com/nfl/news/agents-take-what-type-of-discount-will-tom-brady-give-the-patriots-on-his-contract-this-time/" rel="external nofollow noopener" target="_blank">Tom Brady took for years</a> so that the team could sign players that put them in a better position to win. These things are beyond the scope of nearly any machine learning model, and certainly mine.</p> <p>However, what we did see is that what matters in terms of differentiating the pay day for various NFL wide receivers is relatively simple, the stats that have been tracked since the conception of the game of football: receiving yards and how prominent a player is within their offense (the latter has not been numerically calculated for as long as the former, but both have historically been very easy to spot). Additionally, I learned that creating datasets is a lot more time intensive and laborious than it seems and I am inifintely grateful for the incredible work by millions of people on Kaggle, scientific labortories, and countless other entities that take the time to collect, normalize, and parse much of this data for us. Finally, while my previous project showed the immense power of hyperparameter tuning, this one showed that it certainly can’t be the only avenue to improve performance and should, as with many things in machine learning, be one of the many tools that we utilize in order to make our models more robust and usable in the everday world.</p> <h1 id="viii-references">VIII. References</h1> <p><a href="www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup Documentation</a> <br> <a href="https://machinelearningmastery.com/xgboost-for-regression/" rel="external nofollow noopener" target="_blank">XGBoost for Regression</a> <br> <a href="https://jayant017.medium.com/hyperparameter-tuning-in-xgboost-using-randomizedsearchcv-88fcb5b58a73" rel="external nofollow noopener" target="_blank">Hyperparameter Tuning for XGBoost</a> <br> Special shotout to the scikit-learn, great_tables, and seaborn documentation :)</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Arnav Sareen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/3162-portfolio/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/3162-portfolio/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/3162-portfolio/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/3162-portfolio/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/3162-portfolio/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/3162-portfolio/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/3162-portfolio/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/3162-portfolio/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/3162-portfolio/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/3162-portfolio/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/3162-portfolio/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/3162-portfolio/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/3162-portfolio/assets/js/search-data.js"></script> <script src="/3162-portfolio/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>